{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281d4463-2af2-42a5-9cbe-244ae5608fb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PageRank\n",
    "\n",
    "In this notebook, we will see a basic application of PageRank using pyspark. We will start by reviewing the theory. Suppose we have a graph, with nodes and edges, pretty much like the web. We want to determine the importance of each node. With the web analogy, this means ranking web pages according to their relevance. Of course, we cannot rely ask people directly to rank pages. We have to use the information already available. Let's see an example of a small web. \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/892/1*E1QqUL6eQpJsuxI9V5FE7g.png\" alt=\"alt text\" width=\"400\"/>\n",
    "\n",
    "Here you have $5$ web pages $\\{0,1,2,3,4\\}$. Each edge $i \\rightarrow j$ means that the web page $i$ is referring the page $j$. One first idea to determine the relevance of a web page can be the number of other web pages referring to it. For example, page $1$ is referred the most, so one could assume that it is the most relevant page. However, we do not account for the relevance of other pages. For example, if a very relevant page, $x$,  refers to another page $y$.  We could also safely say that $y$ is probably relevant although it may be referred to just a few times. Thus, to get the score of a page $i$, $r_i$, we can consider a score like this one:\n",
    "\n",
    "$$ r_i = \\sum_{j \\rightarrow i} \\frac{r_j}{d_j}$$\n",
    "\n",
    "This means that the relevance score of the page $i$, named $r_i$, is given as a weighted sum of the relevance scores of all the pages referring to $i$. Each page referring to $i$ is weighted according to the out-degree $d_j$ (the number of pages referred from $j$). Of course, without knowing the relevance of all pages referring to $i$, we cannot determine the relevance of $i$. Let's try to write what we know formally. \n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{4}\n",
    "    r_0 =& \\frac{r_4}{3}\\\\\n",
    "    r_1 =& \\frac{r_2}{2} + \\frac{r_4}{3} + r_3\\\\\n",
    "    r_2 =& \\frac{r_0}{3} + \\frac{r_4}{3}\\\\\n",
    "    r_3 =& \\frac{r_2}{2} + \\frac{r_0}{3}\\\\\n",
    "    r_4 =& \\frac{r_0}{3} + r_1\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "In matrix form:\n",
    "\n",
    "$$\n",
    "P =\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 & \\frac{1}{3}\\\\\n",
    "0 & 0 & \\frac{1}{2} & 1 & \\frac{1}{3}\\\\\n",
    "\\frac{1}{3} & 0 & 0 & 0 & \\frac{1}{3}\\\\\n",
    "\\frac{1}{3} & 0 & \\frac{1}{2} & 0 & 0\\\\\n",
    "\\frac{1}{3} & 1 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We have good news, this is a sparse matrix (it has a lot of $0$s). This means that we can leverage the sparsity of the matrix to store only what is really necessary. Notice that $P^T$ is row stochastic, meaning that the rows sum to $1$. This is because each column of $P$ represents a probability distribution. Let's have deeper look at the first column of the matrix:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This row tells us the probability of going on any web page of a person randomly browsing (called a random walker) the web page $2$. In this example, the random walker has probability $\\frac{1}{3}$ of ending on page $2$, probability $\\frac{1}{3}$ of ending on page $3$, and probability $\\frac{1}{3}$ of ending on page $4$. Remember, in practice, the element $P_{ij}$ represents the probability of going $j \\rightarrow i$. Let's suppose our random walker can start from any node, so its initial probability distribution is: \n",
    "\n",
    "$$\n",
    "\\pi = \\begin{bmatrix}\n",
    "\\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's ask ourselves, what is the probability of a random walker with initial distribution $\\pi$ of ending in any other page after one step? Actually, let's begin with a simple question what is the probability of ending the state $5$. \n",
    "\n",
    "- We have a probability of $\\frac{1}{5}$ of starting on $0$ and probability $\\frac{1}{3}$ of moving to $4$ ($\\frac{1}{5}\\frac{1}{3}=\\frac{1}{15}$).\n",
    "- We have a probability of $\\frac{1}{5}$ of starting on $1$ and probability $1$ of moving to $4$ ($\\frac{1}{5}1=\\frac{1}{5}$).\n",
    "- We have a probability of $\\frac{1}{5}$ of starting on $2$ and probability $0$ of moving to $4$ ($\\frac{1}{5}0=0$).\n",
    "- We have a probability of $\\frac{1}{5}$ of starting on $3$ and probability $0$ of moving to $4$ ($\\frac{1}{5}0=0$).\n",
    "- We have a probability of $\\frac{1}{5}$ of starting on $4$ and probability $0$ of moving to $4$ ($\\frac{1}{5}0=0$).\n",
    "\n",
    "So, we have a probability of ending in $4$ of $\\frac{1}{15} + \\frac{1}{5} + 0 + 0 + 0 = \\frac{4}{15}$. This amount to multiply the last row of $P$ with $\\pi$:\n",
    "\n",
    "$$\n",
    "P_4\\pi = \\begin{bmatrix}\n",
    "\\frac{1}{3} & 1 & 0 & 0 & 0\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{5} \\\\ \\frac{1}{5} \\\\ \\frac{1}{5} \\\\ \\frac{1}{5} \\\\ \\frac{1}{5}\\\\\n",
    "\\end{bmatrix} = \\frac{4}{15}\n",
    "$$\n",
    "\n",
    "In practice, we can obtain the probability of ending in any state after one step, $\\pi^{(1)}$, by multiplying $P$ and $\\pi$.\n",
    "\n",
    "$$\n",
    "P\\pi = \\pi^{(1)} =\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 & \\frac{1}{3}\\\\\n",
    "0 & 0 & \\frac{1}{2} & 1 & \\frac{1}{3}\\\\\n",
    "\\frac{1}{3} & 0 & 0 & 0 & \\frac{1}{3}\\\\\n",
    "\\frac{1}{3} & 0 & \\frac{1}{2} & 0 & 0\\\\\n",
    "\\frac{1}{3} & 1 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{5} \\\\ \\frac{1}{5} \\\\ \\frac{1}{5} \\\\ \\frac{1}{5} \\\\ \\frac{1}{5}\\\\\n",
    "\\end{bmatrix}= \n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{15} & \\frac{11}{30} & \\frac{2}{15} & \\frac{1}{6} & \\frac{4}{15}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can obtain the probability of a random walker finding himself on any webpage after two steps, $\\pi^{(2)}$, by multiplying $P$ and $\\pi^{(1)}$. Through an iterative process, we can obtain the probability of ending on any page after any finite number of steps. \n",
    "\n",
    "## PageRank on spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2269f75-66b5-4317-82d9-329ec4ec343d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ataxia:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4513f4abc0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load spark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"Python Spark SQL basic example\") \\\n",
    "                    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6fa4cf-129f-4dfe-ba01-c90a02db3af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's try an example \n",
    "dataset = spark.sparkContext.parallelize([(0,3), (0,2), (0,4), \n",
    "                                          (1,4),\n",
    "                                          (2,1), (2,3),\n",
    "                                          (3,1),\n",
    "                                          (4,0), (4,1), (4,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a1630b-7aa3-4fd0-9117-3863a1c331ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 3),\n",
       " (0, 2),\n",
       " (0, 4),\n",
       " (1, 4),\n",
       " (2, 1),\n",
       " (2, 3),\n",
       " (3, 1),\n",
       " (4, 0),\n",
       " (4, 1),\n",
       " (4, 2)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's peek the first entries\n",
    "dataset.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b7c7a44-e7b5-44cc-98ae-eff713c511c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "total_pages = max(dataset.max(lambda x:x[0])[0],dataset.max(lambda x:x[1])[1])\n",
    "print(total_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c48cc21a-874e-4b74-bb22-4c6320603ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the out-degree for each node\n",
    "id2degree = dataset.countByKey()\n",
    "id2degree[0],id2degree[1],id2degree[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc57471-21e2-4390-8437-cd21b8cd1bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 3, 0.3333333333333333),\n",
       " (0, 2, 0.3333333333333333),\n",
       " (0, 4, 0.3333333333333333),\n",
       " (1, 4, 1.0),\n",
       " (2, 1, 0.5),\n",
       " (2, 3, 0.5),\n",
       " (3, 1, 1.0),\n",
       " (4, 0, 0.3333333333333333),\n",
       " (4, 1, 0.3333333333333333),\n",
       " (4, 2, 0.3333333333333333)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = dataset.map(lambda x:(x[0],x[1],1/id2degree[x[0]]))\n",
    "P.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5386b75-435b-40c3-871c-b715d5f6fe4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2, 0.2, 0.2, 0.2, 0.2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "p = np.full((total_pages+1,), 1/(total_pages+1))\n",
    "p[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e71de1-26e9-457d-8bfc-78b3ad4c0a19",
   "metadata": {},
   "source": [
    "Now we need to implement the distributed version of the matrix multiplication. This part will be a little tricky. We will assume that the vector $p$ can fit in memory. \n",
    "- The matrix $P$ is represented as $(i,j,m_{ij})$.\n",
    "- The vector $p$ is represented as $(j, v_j)$\n",
    "\n",
    "The algorithm proceed as follows:\n",
    "- Firstly, we map each $(i,j,m_{ij}) \\rightarrow (i, m_{ij}v_j)$\n",
    "- Next, we reduce by key $(i, [m_{ij}v_j, \\dots, m_{it}v_t]) \\rightarrow (i, m_{ij}v_j + \\dots + m_{it}v_t)$\n",
    "\n",
    "$$Px = y$$\n",
    "$$y_i = \\sum_k m_{ik}x_k$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a99fd78f-8d1e-4f1f-a74d-a9c201aa02eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1.0), (1, 1.0), (2, 1.0), (3, 1.0), (4, 1.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns do sum to 1.\n",
    "PT = P.map(lambda x: (x[1],x[0],x[2]))\n",
    "PT  .map(lambda x: (x[1],x[2]))\\\n",
    "    .reduceByKey(lambda x,y: x+y)\\\n",
    "    .take(10)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6400a321-62df-4d73-993b-cf5d29fdd486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    new_p = PT.map(lambda x:(x[0],(x[2]*p[x[1]])))\\\n",
    "              .reduceByKey(lambda x,y: x+y)\\\n",
    "              .collect()\n",
    "    for idx,prb in new_p:\n",
    "        p[idx] = prb\n",
    "    \n",
    "    print(f\"iteration {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6882f9ba-88fa-4aff-b86b-07677967a50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 0.3357537807583531),\n",
       " (1, 0.2961593337736455),\n",
       " (2, 0.14710325323036794),\n",
       " (3, 0.11074425477146095),\n",
       " (0, 0.11023937746617213)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(p.argsort()[::-1], p[p.argsort()[::-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88110cbc-a43e-46bd-8fae-7441d9ecc6db",
   "metadata": {},
   "source": [
    "### On real data\n",
    "\n",
    "We will study a real dataset from https://www.cs.cornell.edu/courses/cs685/2002fa/. You can download the raw file at https://www.cs.cornell.edu/courses/cs685/2002fa/data/gr0.California. These web subgraphs were constructed by expanding a 200-page response set to a search engine query, as in the hub/authority algorithm. This data was collected some time back, so a number of the links will be broken. \n",
    "\n",
    "Before going any further, try to make the page rank algorithm yourself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e043b9bc-1f64-43f1-bc09-66145d330b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's download the dataset\n",
    "import wget, os\n",
    "if not os.path.isfile(\"dataset.txt\"):\n",
    "    wget.download(url = \"https://www.cs.cornell.edu/courses/cs685/2002fa/data/gr0.California\", out = \"dataset.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94a534e1-2231-4429-973e-5ab1c5b8c1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset --->\n",
      "['n 0 http://www.berkeley.edu/',\n",
      " 'n 1 http://www.caltech.edu/',\n",
      " 'n 2 http://www.realestatenet.com/',\n",
      " 'n 3 http://www.ucsb.edu/',\n",
      " 'n 4 http://www.washingtonpost.com/wp-srv/national/longterm/50states/ca.htm',\n",
      " 'n 5 http://www-ucpress.berkeley.edu/',\n",
      " 'n 6 http://www.ucr.edu/',\n",
      " 'n 7 http://www.tegnetcorporation.com/',\n",
      " 'n 8 http://www.research.digital.com/SRC/virtual-tourist/California.html',\n",
      " 'n 9 http://www.leginfo.ca.gov/calaw.html']\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "from pprint import pprint\n",
    "dataset = spark.sparkContext.textFile(name = \"dataset.txt\", minPartitions = 2)\n",
    "\n",
    "# let's have a peek a our dataset\n",
    "print(\"dataset --->\")\n",
    "pprint(dataset.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d16710b-4c33-44c0-832e-e9d7ff5c9d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firsts 10 nodes entries:  [(0, 'http://www.berkeley.edu/'), (1, 'http://www.caltech.edu/'), (2, 'http://www.realestatenet.com/'), (3, 'http://www.ucsb.edu/'), (4, 'http://www.washingtonpost.com/wp-srv/national/longterm/50states/ca.htm'), (5, 'http://www-ucpress.berkeley.edu/'), (6, 'http://www.ucr.edu/'), (7, 'http://www.tegnetcorporation.com/'), (8, 'http://www.research.digital.com/SRC/virtual-tourist/California.html'), (9, 'http://www.leginfo.ca.gov/calaw.html')]\n",
      "\n",
      "firsts 10 edges entries:  [(0, 449), (0, 450), (0, 451), (0, 452), (0, 453), (0, 454), (0, 455), (0, 456), (0, 432), (0, 457)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get nodes\n",
    "id2ref = dataset.filter(lambda x:x.startswith(\"n\"))\\\n",
    "                .map(lambda x:tuple(x.split(\" \")))\\\n",
    "                .map(lambda x:(int(x[1]),x[2]))\n",
    "print(\"firsts 10 nodes entries: \", id2ref.take(10),end=\"\\n\\n\")\n",
    "\n",
    "# get edges\n",
    "id2id = dataset.filter(lambda x:x.startswith(\"e\"))\\\n",
    "               .map(lambda x:tuple(x.split(\" \")))\\\n",
    "               .map(lambda x:(int(x[1]),int(x[2])))\n",
    "print(\"firsts 10 edges entries: \", id2id.take(10),end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ad12b0-29a0-4ae3-85b7-90ed079cf6fb",
   "metadata": {},
   "source": [
    "Next we need to fix an important problem. <br />\n",
    "There are nodes that do not appear among the edges. <br />\n",
    "This means that the stochastic matrix would have some full zero rows. <br />\n",
    "This is a big problems as by multiplying P by p, we cannot expect a distribution. <br />\n",
    "There are many ways to solve this problem, but one of the simplest is adding edges to an artificial nodes.<br />\n",
    "This artificial node---called hidden. <br />\n",
    "it will reach all other node and it will be reachable by all other nodes. <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5050977-a239-4a00-ba5e-795d69abbc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = max(id2id.keys().max(), id2id.values().max()) + 1\n",
    "id2hidden = spark.sparkContext.parallelize([(i,hidden) for i in range(hidden)])\n",
    "hidden2id = spark.sparkContext.parallelize([(hidden,i) for i in range(hidden)])\n",
    "id2id_filled  = id2id.union(id2hidden).union(hidden2id)\n",
    "id2ref_filled = id2ref.union(spark.sparkContext.parallelize([(hidden, \"None\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d903d0fa-2a3b-47c0-bc94-d0f08b928564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:=========================>                               (8 + 8) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree of node 0:18, degree of node 1:2, degree of node 2:4.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# compute the out-degree for each node\n",
    "id2degree = id2id_filled.countByKey()\n",
    "print(f\"degree of node 0:{id2degree[0]}, degree of node 1:{id2degree[1]}, degree of node 2:{id2degree[2]}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69ff8d0-a02b-46a5-86f3-4cb0692eaca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firsts 10 matrix entries: [(0, 449, 0.05555555555555555), (0, 450, 0.05555555555555555), (0, 451, 0.05555555555555555), (0, 452, 0.05555555555555555), (0, 453, 0.05555555555555555), (0, 454, 0.05555555555555555), (0, 455, 0.05555555555555555), (0, 456, 0.05555555555555555), (0, 432, 0.05555555555555555), (0, 457, 0.05555555555555555)]\n",
      "\n",
      "total nodes: 9665\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:=================================================>      (16 + 2) / 18]\r"
     ]
    }
   ],
   "source": [
    "# compute sparse transition matrix\n",
    "P = id2id_filled.map(lambda x:(x[0],x[1],1/id2degree[x[0]]))\n",
    "PT = P.map(lambda x: (x[1],x[0],x[2]))\n",
    "print(\"firsts 10 matrix entries:\", P.take(10), end=\"\\n\\n\")\n",
    "    \n",
    "# compute total number of nodes\n",
    "connected_nodes = id2id.keys().union(id2id.values()).distinct().count()\n",
    "total_nodes = id2ref_filled.map(lambda x:x[0]).count()\n",
    "print(f\"total nodes: {total_nodes}\\n\")\n",
    "\n",
    "# compute probabilities vector\n",
    "import numpy as np\n",
    "p = np.full((total_nodes,), 1/(total_nodes))\n",
    "\n",
    "# P*p for some iteration\n",
    "for i in range(10):\n",
    "    new_p = PT.map(lambda x:(x[0],(x[2]*p[x[1]])))\\\n",
    "              .reduceByKey(lambda x,y: x+y)\\\n",
    "              .collect()\n",
    "    for idx,prb in new_p:\n",
    "        p[idx] = prb\n",
    "\n",
    "print(p.sum())\n",
    "\n",
    "# print top pages\n",
    "for page in list(zip(p.argsort()[::-1], p[p.argsort()[::-1]]))[:10]:\n",
    "    print(f\"prb:{page[1]}, page:{id2ref_filled.lookup(page[0])}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PR-venv",
   "language": "python",
   "name": "pr-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
