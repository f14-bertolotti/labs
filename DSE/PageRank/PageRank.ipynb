{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281d4463-2af2-42a5-9cbe-244ae5608fb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PageRank\n",
    "\n",
    "In this notebook, we'll explore a basic application of PageRank using PySpark. Before diving into the implementation, let's review the theory behind PageRank.\n",
    "\n",
    "Suppose we have a graph with nodes and edges, similar to the structure of the web. Our goal is to determine the importance of each node in the graph. In the context of the web, this translates to ranking web pages based on their relevance. However, we can't simply ask people to directly rank pages. Instead, we must utilize the information already available to us. Let's consider an example of a small web graph to illustrate this concept.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/892/1*E1QqUL6eQpJsuxI9V5FE7g.png\" alt=\"alt text\" width=\"400\"/>\n",
    "\n",
    "Here we have 5 web pages, denoted as ${0, 1, 2, 3, 4}$. Each edge $i \\rightarrow j$ signifies that web page $i$ refers to web page $j$. One initial approach to determine the relevance of a web page could be to consider the number of other web pages referring to it. For instance, if a page receives many referrals, it may be assumed to be more relevant. However, this approach fails to account for the relevance of the referring pages themselves. For example, if a highly relevant page, say $x$, refers to another page $y$, we can reasonably infer that $y$ is also relevant, even if it's referred to by just a few pages. To calculate the score of a page $i$, denoted as $r_i$, we can consider a scoring method like this:\n",
    "\n",
    "$$ r_i = \\sum_{j \\rightarrow i} \\frac{r_j}{d_j}$$\n",
    "\n",
    "This implies that the relevance score of page $i$, denoted as $r_i$, is calculated as a weighted sum of the relevance scores of all pages referring to $i$. Each referring page $j$ is weighted according to its out-degree $d_j$, representing the number of pages referred from $j$.. However, without knowing the relevance of all pages referring to $i$, we cannot accurately determine the relevance of $i$. Let's attempt to formalize what we know for the previous graph:\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{4}\n",
    "    r_0 =& \\frac{r_4}{3}\\\\\n",
    "    r_1 =& \\frac{r_2}{2} + \\frac{r_4}{3} + r_3\\\\\n",
    "    r_2 =& \\frac{r_0}{3} + \\frac{r_4}{3}\\\\\n",
    "    r_3 =& \\frac{r_2}{2} + \\frac{r_0}{3}\\\\\n",
    "    r_4 =& \\frac{r_0}{3} + r_1\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "In matrix form:\n",
    "\n",
    "$$\n",
    "P =\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 & \\frac{1}{3}\\\\\n",
    "0 & 0 & \\frac{1}{2} & 1 & \\frac{1}{3}\\\\\n",
    "\\frac{1}{3} & 0 & 0 & 0 & \\frac{1}{3}\\\\\n",
    "\\frac{1}{3} & 0 & \\frac{1}{2} & 0 & 0\\\\\n",
    "\\frac{1}{3} & 1 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We have good news: this matrix is sparse, meaning it contains many zeros. This sparsity allows us to store only the non-zero elements, saving storage space. Additionally, the matrix's transpose, denoted by $P^T$, is row-stochastic. This means that each row in $P^T$ sums to 1 because each column in the original matrix, $P$, represents a probability distribution.\n",
    "\n",
    "This matrix, represents the transition probabilities between web pages in a simplified web graph. Each element, $P_{ij}$, signifies the probability of a random walker moving from web page $i$ to web page $j$ in a single step.\n",
    "\n",
    "Consider the first column, specifically the elements $P_{0,j}$ (where j represents any web page):\n",
    "\n",
    "* $P_{0,2}$: Probability of the random walker transitioning from page 0 to page 2 in one step.\n",
    "* $P_{0,3}$: Probability of the random walker transitioning from page 0 to page 3 in one step.\n",
    "* $P_{0,4}$: Probability of the random walker transitioning from page 0 to page 4 in one step.\n",
    "\n",
    "In the provided example, these probabilities are $\\frac{1}{3}$, $\\frac{1}{3}$, and $\\frac{1}{3}$, respectively. This indicates that if the random walker starts at page 0, it has an equal chance (one-third) of transitioning to any of the pages 2, 3, and 4 in the next step. It's important to remember that random walks follow the defined connections in the graph, and the walker doesn't \"jump\" to any arbitrary page but moves based on the transition probabilities between connected pages.\n",
    "\n",
    "\n",
    "Let's suppose our random walker can start from any node, so its initial probability distribution is: \n",
    "\n",
    "$$\n",
    "\\pi = \\begin{bmatrix}\n",
    "\\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's ask ourselves, what is the probability of a random walker with initial distribution $\\pi$ of ending in any other page after one step? Actually, let's begin with a simple question what is the probability of ending the state $5$. \n",
    "\n",
    "- We have a probability of $\\frac{1}{5}$ of starting on $0$ and probability $\\frac{1}{3}$ of moving to $4$ ($\\frac{1}{5}\\frac{1}{3}=\\frac{1}{15}$).\n",
    "- We have a probability of $\\frac{1}{5}$ of starting on $1$ and probability $1$ of moving to $4$ ($\\frac{1}{5}1=\\frac{1}{5}$).\n",
    "- We have a probability of $\\frac{1}{5}$ of starting on $2$ and probability $0$ of moving to $4$ ($\\frac{1}{5}0=0$).\n",
    "- We have a probability of $\\frac{1}{5}$ of starting on $3$ and probability $0$ of moving to $4$ ($\\frac{1}{5}0=0$).\n",
    "- We have a probability of $\\frac{1}{5}$ of starting on $4$ and probability $0$ of moving to $4$ ($\\frac{1}{5}0=0$).\n",
    "\n",
    "So, we have a probability of ending in $4$ of $\\frac{1}{15} + \\frac{1}{5} + 0 + 0 + 0 = \\frac{4}{15}$. This amount to multiply the last row of $P$ with $\\pi$:\n",
    "\n",
    "$$\n",
    "P_4\\pi = \\begin{bmatrix}\n",
    "\\frac{1}{3} & 1 & 0 & 0 & 0\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{5} \\\\ \\frac{1}{5} \\\\ \\frac{1}{5} \\\\ \\frac{1}{5} \\\\ \\frac{1}{5}\\\\\n",
    "\\end{bmatrix} = \\frac{4}{15}\n",
    "$$\n",
    "\n",
    "In practice, we can obtain the probability of ending in any state after one step, $\\pi^{(1)}$, by multiplying $P$ and $\\pi$.\n",
    "\n",
    "$$\n",
    "P\\pi = \\pi^{(1)} =\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 & \\frac{1}{3}\\\\\n",
    "0 & 0 & \\frac{1}{2} & 1 & \\frac{1}{3}\\\\\n",
    "\\frac{1}{3} & 0 & 0 & 0 & \\frac{1}{3}\\\\\n",
    "\\frac{1}{3} & 0 & \\frac{1}{2} & 0 & 0\\\\\n",
    "\\frac{1}{3} & 1 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{5} \\\\ \\frac{1}{5} \\\\ \\frac{1}{5} \\\\ \\frac{1}{5} \\\\ \\frac{1}{5}\\\\\n",
    "\\end{bmatrix}= \n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{15} & \\frac{11}{30} & \\frac{2}{15} & \\frac{1}{6} & \\frac{4}{15}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can obtain the probability of a random walker finding himself on any webpage after two steps, $\\pi^{(2)}$, by multiplying $P$ and $\\pi^{(1)}$. Through an iterative process, we can obtain the probability of ending on any page after any finite number of steps. \n",
    "\n",
    "______________________\n",
    "## PageRank on spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2269f75-66b5-4317-82d9-329ec4ec343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"Python Spark SQL basic example\") \\\n",
    "                    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6fa4cf-129f-4dfe-ba01-c90a02db3af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's try an example \n",
    "dataset = spark.sparkContext.parallelize([(0,3), (0,2), (0,4), \n",
    "                                          (1,4),\n",
    "                                          (2,1), (2,3),\n",
    "                                          (3,1),\n",
    "                                          (4,0), (4,1), (4,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1630b-7aa3-4fd0-9117-3863a1c331ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's peek the first entries\n",
    "dataset.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c7a44-e7b5-44cc-98ae-eff713c511c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pages = dataset.keys().distinct().count()\n",
    "total_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b055de-8f98-4c88-92c5-1175255b539b",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "* (⭐) process `dataset` to return an rdd with nodes associated to the directly reachable nodes (e.g. `(0,[2,3,4])`)\n",
    "* (⭐) process `dataset` to return an rdd with nodes associated to the number of directly reachable nodes (e.g. `(0,3)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48cc21a-874e-4b74-bb22-4c6320603ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the out-degree for each node\n",
    "id2degree = dataset.countByKey()\n",
    "id2degree[0],id2degree[1],id2degree[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc57471-21e2-4390-8437-cd21b8cd1bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we append edges the transition probabilities\n",
    "P = dataset.map(lambda x:(x[0],x[1],1/id2degree[x[0]]))\n",
    "P.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5386b75-435b-40c3-871c-b715d5f6fe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's set the intial probability distribution\n",
    "# Here, each node has equal probability\n",
    "import numpy as np\n",
    "p = np.array([1/total_pages for i in range(total_pages)])\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e71de1-26e9-457d-8bfc-78b3ad4c0a19",
   "metadata": {},
   "source": [
    "Now we need to implement the distributed version of the matrix multiplication. This part will be a little tricky. We will assume that the vector $p$ can fit in memory. \n",
    "- The matrix $P$ is represented as $(i,j,m_{ij})$.\n",
    "- The vector $\\pi$ is represented as $(j, v_j)$\n",
    "\n",
    "The algorithm proceed as follows:\n",
    "- Firstly, we map each $(i,j,m_{ij}) \\rightarrow (i, m_{ij}v_j)$\n",
    "- Next, we reduce by key $(i, [m_{ij}v_j, \\dots, m_{it}v_t]) \\rightarrow (i, m_{ij}v_j + \\dots + m_{it}v_t)$\n",
    "\n",
    "$$P\\pi = y$$\n",
    "$$y_i = \\sum_k m_{ik}\\pi_k$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99fd78f-8d1e-4f1f-a74d-a9c201aa02eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns do sum to 1.\n",
    "PT = P.map(lambda x: (x[1],x[0],x[2]))\n",
    "PT  .map(lambda x: (x[1],x[2]))\\\n",
    "    .reduceByKey(lambda x,y: x+y)\\\n",
    "    .take(10)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6400a321-62df-4d73-993b-cf5d29fdd486",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    # P x p\n",
    "    p_rdd = PT.map(lambda x:(x[0],(x[2]*p[x[1]])))\\\n",
    "           .reduceByKey(lambda x,y: x+y)\\\n",
    "           .collect()\n",
    "    # update p\n",
    "    for idx,prb in p_rdd:\n",
    "        p[idx] = prb\n",
    "     \n",
    "    print(f\"iteration {i}, current p: {p}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6882f9ba-88fa-4aff-b86b-07677967a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(p.argsort()[::-1], p[p.argsort()[::-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88110cbc-a43e-46bd-8fae-7441d9ecc6db",
   "metadata": {},
   "source": [
    "### On real data\n",
    "\n",
    "Let's delve into a real dataset sourced from https://www.cs.cornell.edu/courses/cs685/2002fa/. You can access the raw file directly from https://www.cs.cornell.edu/courses/cs685/2002fa/data/gr0.California. These web subgraphs were generated by expanding a 200-page response set to a search engine query, much like in the hub/authority algorithm. It's worth noting that this data was collected some time ago, so it's expected that some of the links may be broken.\n",
    "\n",
    "Before proceeding, let's attempt to implement the PageRank algorithm ourselves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e043b9bc-1f64-43f1-bc09-66145d330b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's download the dataset\n",
    "import wget, os\n",
    "if not os.path.isfile(\"dataset.txt\"):\n",
    "    wget.download(url = \"https://www.cs.cornell.edu/courses/cs685/2002fa/data/gr0.California\", out = \"dataset.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a534e1-2231-4429-973e-5ab1c5b8c1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "from pprint import pprint\n",
    "dataset = spark.sparkContext.textFile(name = \"dataset.txt\")\n",
    "\n",
    "# let's have a peek a our dataset\n",
    "print(\"dataset --->\")\n",
    "pprint(dataset.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16710b-4c33-44c0-832e-e9d7ff5c9d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nodes\n",
    "id2ref = dataset.filter(lambda x:x.startswith(\"n\"))\\\n",
    "                .map(lambda x:tuple(x.split(\" \")))\\\n",
    "                .map(lambda x:(int(x[1]),x[2]))\n",
    "print(\"firsts 10 nodes entries: \", id2ref.take(10),end=\"\\n\\n\")\n",
    "\n",
    "# get edges\n",
    "id2id = dataset.filter(lambda x:x.startswith(\"e\"))\\\n",
    "               .map(lambda x:tuple(x.split(\" \")))\\\n",
    "               .map(lambda x:(int(x[1]),int(x[2])))\n",
    "print(\"firsts 10 edges entries: \", id2id.take(10),end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ad12b0-29a0-4ae3-85b7-90ed079cf6fb",
   "metadata": {},
   "source": [
    "Next, we need to address a significant issue. \n",
    "\n",
    "Some nodes do not appear among the edges in our dataset. Consequently, the stochastic matrix would contain some rows entirely filled with zeros. This poses a problem because when multiplying the transition matrix $P$ by the probability vector $\\pi$, we cannot expect a valid probability distribution.\n",
    "\n",
    "There are several methods to tackle this issue, the simplest solution is simply to remove nodes that do not have edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5050977-a239-4a00-ba5e-795d69abbc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = max(id2id.keys().max(), id2id.values().max()) + 1\n",
    "id2hidden = spark.sparkContext.parallelize([(i,hidden) for i in range(hidden)])\n",
    "hidden2id = spark.sparkContext.parallelize([(hidden,i) for i in range(hidden)])\n",
    "id2id_filled  = id2id.union(id2hidden).union(hidden2id)\n",
    "id2ref_filled = id2ref.union(spark.sparkContext.parallelize([(hidden, \"None\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d903d0fa-2a3b-47c0-bc94-d0f08b928564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the out-degree for each node\n",
    "id2degree = id2id_filled.countByKey()\n",
    "print(f\"degree of node 0:{id2degree[0]}, degree of node 1:{id2degree[1]}, degree of node 2:{id2degree[2]}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69ff8d0-a02b-46a5-86f3-4cb0692eaca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute sparse transition matrix\n",
    "P = id2id_filled.map(lambda x:(x[0],x[1],1/id2degree[x[0]]))\n",
    "PT = P.map(lambda x: (x[1],x[0],x[2]))\n",
    "print(\"firsts 10 matrix entries:\", P.take(10), end=\"\\n\\n\")\n",
    "    \n",
    "# compute total number of nodes\n",
    "connected_nodes = id2id.keys().union(id2id.values()).distinct().count()\n",
    "total_nodes = id2ref_filled.map(lambda x:x[0]).count()\n",
    "print(f\"total nodes: {total_nodes}, connected_nodes: {connected_nodes}\\n\")\n",
    "\n",
    "# compute probabilities vector\n",
    "import numpy as np\n",
    "p = np.full((total_nodes,), 1/(total_nodes))\n",
    "\n",
    "# P*p for some iteration\n",
    "for i in range(10):\n",
    "    new_p = PT.map(lambda x:(x[0],(x[2]*p[x[1]])))\\\n",
    "              .reduceByKey(lambda x,y: x+y)\\\n",
    "              .collect()\n",
    "    for idx,prb in new_p:\n",
    "        p[idx] = prb\n",
    "\n",
    "print(p.sum())\n",
    "\n",
    "# print top pages\n",
    "for page in list(zip(p.argsort()[::-1], p[p.argsort()[::-1]]))[:10]:\n",
    "    print(f\"prb:{page[1]}, page:{id2ref_filled.lookup(page[0])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9997e89-efb4-4aa6-a053-cf0150efdd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ef899-1a38-48bf-bbb4-dfd262567623",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "* (⭐⭐⭐) modify this algorithm: remove the unreferenced nodes and recompute the random walk probabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
