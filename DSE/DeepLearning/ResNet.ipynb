{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "301bc4c5-828f-4522-a8f2-bb38ce91d12c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 17:03:06.592838: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-12 17:03:07.152141: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/lib/:/opt/cuda/targets/x86_64-linux/lib/:/opt/cuda/nvvm/libdevice/:/home/f14/Downloads/cudnn-linux-x86_64-8.8.1.3_cuda11-archive/lib/:/opt/cuda/nvvm/libdevice/\n",
      "2023-03-12 17:03:07.152196: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/lib/:/opt/cuda/targets/x86_64-linux/lib/:/opt/cuda/nvvm/libdevice/:/home/f14/Downloads/cudnn-linux-x86_64-8.8.1.3_cuda11-archive/lib/:/opt/cuda/nvvm/libdevice/\n",
      "2023-03-12 17:03:07.152202: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-12 17:03:07.784908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 17:03:07.789234: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-12 17:03:07.789336: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ccf94a1-fa4c-4c51-a252-8a69ae897c67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wget, zipfile, os, random\n",
    "\n",
    "# download dataset\n",
    "if not os.path.isfile(\"data.zip\"): \n",
    "    print(\"downloading...\")\n",
    "    wget.download(\"https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/3362/31148/bundle/archive.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1678299698&Signature=b6XuKNXvsSuBcv2SZYiTqcfr7Mc%2FjpSR%2BptWRsmjRTHr1vqLdnpC6k9YaLk%2BMRSmTeZcOXGFg21TGUyv9RR7TctRcWLW8WhIdibLM2BTLqc2YCM5ZXU9DkwYRypzgmXjZs4U%2B2n4AeOzO2w7CJFnxgjOeOtAamziNd5F07aiSap1A8b2PwtClwvsQA0kx1vp0HTtT99NsWpGFY310ZOMjRnpFK17ANh5UmYAaFeHm2RVyR%2FRRVcWbVTSWKhaEDLgppgdIREZ%2FGsXpybkFAqfWGqmHkDVEpty0E3BuyGGZ8DEkDphwjHlab824Waid0IpOeOgO6qAziK5fy22dCO0Pg%3D%3D&response-content-disposition=attachment%3B+filename%3Ddogs-vs-cats.zip\", \"data.zip\")\n",
    "\n",
    "# extract dataset\n",
    "if not (os.path.isdir(\"data\") and os.path.isdir(\"data/train\") and os.path.isdir(\"data/test1\")): \n",
    "    print(\"extracting...\")\n",
    "    with zipfile.ZipFile(\"data.zip\"      , 'r') as file: file.extractall(\"./data/\")\n",
    "    with zipfile.ZipFile(\"data/train.zip\", 'r') as file: file.extractall(\"./data/\")\n",
    "    with zipfile.ZipFile(\"data/test1.zip\", 'r') as file: file.extractall(\"./data/\")\n",
    "\n",
    "# get all train paths\n",
    "paths = list(map(lambda name:os.path.join(\"data/train\",name), os.listdir(\"data/train/\")))\n",
    "random.shuffle(paths)\n",
    "test_paths  = paths[:len(paths)//3]\n",
    "train_paths = paths[len(paths)//3+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e17a02e2-fb33-44b3-b0b6-afa8af28eedb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random, copy\n",
    "def batchfy(paths, batch_size):\n",
    "    #shuffle all paths\n",
    "    paths = copy.deepcopy(paths)\n",
    "    random.shuffle(paths)\n",
    "    \n",
    "    # batchfy\n",
    "    size = len(paths)\n",
    "    batches = size // batch_size\n",
    "    batches = [paths[i*batch_size:(i+1)*batch_size] for i in range(batches)]\n",
    "    \n",
    "    return batches\n",
    "\n",
    "\n",
    "def load_batch(paths):\n",
    "    tensor_images = tf.stack([tf.image.resize(tf.io.decode_image(tf.io.read_file(path),channels=3), (256,256))/256 for path in paths])\n",
    "    tensor_labels = tf.convert_to_tensor([[1,0] if path[11:14] == \"cat\" else [0,1] for path in paths], dtype=float)\n",
    "    return tensor_images, tensor_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cfd30be8-abdc-44d7-8e43-3ec18b003bdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class ConvBlock(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, kernel_size=(3,3), filters=16, strides=(2, 2)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv2a = tf.keras.layers.Conv2D(filters, (1, 1), strides=strides, )\n",
    "        self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2b = tf.keras.layers.Conv2D(filters, kernel_size, padding='same')\n",
    "        self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2c = tf.keras.layers.Conv2D(filters, (1, 1))\n",
    "        self.bn2c = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.convd = tf.keras.layers.Conv2D(filters, (1, 1), strides=strides)\n",
    "        self.bnd = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, z):\n",
    "        x = tf.nn.relu(self.bn2a(self.conv2a(z)))\n",
    "        x = tf.nn.relu(self.bn2b(self.conv2b(x)))\n",
    "        x = self.bn2c(self.conv2c(x))\n",
    "\n",
    "        z = self.convd(z)\n",
    "        z = self.bnd(z)\n",
    "\n",
    "        return tf.nn.relu(x + z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "676df341-178e-4481-b0d4-963a37828cc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyResNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, (7, 7), strides=(2, 2))\n",
    "        self.bn_conv1 = tf.keras.layers.BatchNormalization()\n",
    "        self.max_pool = tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2))\n",
    "\n",
    "        self.c0 = ConvBlock()\n",
    "        self.c1 = ConvBlock()\n",
    "        self.c2 = ConvBlock()\n",
    "        self.c3 = ConvBlock()\n",
    "        self.c4 = ConvBlock()\n",
    "        \n",
    "        # GAP\n",
    "        self.gap = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.lin = tf.keras.layers.Dense(2, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, x):\n",
    "      \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn_conv1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        \n",
    "        x = self.c0(x)\n",
    "        x = self.c1(x)\n",
    "        x = self.c2(x)\n",
    "        x = self.c3(x)\n",
    "        x = self.c4(x)\n",
    "\n",
    "        x = self.lin(self.gap(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c6653b65-da25-44da-aea2-d5dd5918ad08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch:0, batch:389/390, loss: 0.66907, acc: 0.53125, cma: 0.55950     \n",
      " epoch:1, batch:389/390, loss: 0.65776, acc: 0.59375, cma: 0.60076     \n",
      " epoch:2, batch:389/390, loss: 0.61214, acc: 0.62500, cma: 0.63353     \n",
      " epoch:3, batch:389/390, loss: 0.51184, acc: 0.78125, cma: 0.67588     \n",
      " epoch:4, batch:389/390, loss: 0.50566, acc: 0.76562, cma: 0.69820     \n",
      " epoch:5, batch:389/390, loss: 0.63617, acc: 0.64062, cma: 0.70889     \n",
      " epoch:6, batch:389/390, loss: 0.53048, acc: 0.71875, cma: 0.71807     \n",
      " epoch:7, batch:389/390, loss: 0.70511, acc: 0.60938, cma: 0.72432     \n",
      " epoch:8, batch:389/390, loss: 0.53825, acc: 0.70312, cma: 0.72901     \n",
      " epoch:9, batch:389/390, loss: 0.48911, acc: 0.81250, cma: 0.73914     \n",
      " epoch:10, batch:389/390, loss: 0.40406, acc: 0.84375, cma: 0.74359     \n",
      " epoch:11, batch:389/390, loss: 0.42890, acc: 0.82812, cma: 0.74587     \n",
      " epoch:12, batch:389/390, loss: 0.44459, acc: 0.87500, cma: 0.75224     \n",
      " epoch:13, batch:389/390, loss: 0.44304, acc: 0.81250, cma: 0.75509     \n",
      " epoch:14, batch:389/390, loss: 0.46990, acc: 0.78125, cma: 0.75721     \n",
      " epoch:15, batch:389/390, loss: 0.39924, acc: 0.82812, cma: 0.76418     \n",
      " epoch:16, batch:389/390, loss: 0.43087, acc: 0.78125, cma: 0.76951     \n",
      " epoch:17, batch:389/390, loss: 0.44085, acc: 0.78125, cma: 0.77412     \n",
      " epoch:18, batch:389/390, loss: 0.46232, acc: 0.75000, cma: 0.77744     \n",
      " epoch:19, batch:389/390, loss: 0.40081, acc: 0.78125, cma: 0.78109     \n",
      " epoch:20, batch:389/390, loss: 0.40029, acc: 0.85938, cma: 0.78269     \n",
      " epoch:21, batch:389/390, loss: 0.54030, acc: 0.68750, cma: 0.78550     \n",
      " epoch:22, batch:389/390, loss: 0.40006, acc: 0.81250, cma: 0.78690     \n",
      " epoch:23, batch:389/390, loss: 0.36337, acc: 0.84375, cma: 0.78982     \n",
      " epoch:24, batch:389/390, loss: 0.40131, acc: 0.85938, cma: 0.79479     \n",
      " epoch:25, batch:389/390, loss: 0.33810, acc: 0.78125, cma: 0.79760     \n",
      " epoch:26, batch:389/390, loss: 0.51084, acc: 0.75000, cma: 0.79888     \n",
      " epoch:27, batch:389/390, loss: 0.44579, acc: 0.78125, cma: 0.79940     \n",
      " epoch:28, batch:389/390, loss: 0.42054, acc: 0.81250, cma: 0.80020     \n",
      " epoch:29, batch:389/390, loss: 0.40053, acc: 0.82812, cma: 0.80260     \n",
      " epoch:30, batch:389/390, loss: 0.48252, acc: 0.76562, cma: 0.80857     \n",
      " epoch:31, batch:389/390, loss: 0.33666, acc: 0.84375, cma: 0.81030     \n",
      " epoch:32, batch:389/390, loss: 0.37422, acc: 0.87500, cma: 0.80653     \n",
      " epoch:33, batch:389/390, loss: 0.33136, acc: 0.85938, cma: 0.81218     \n",
      " epoch:34, batch:389/390, loss: 0.37881, acc: 0.82812, cma: 0.81458     \n",
      " epoch:35, batch:389/390, loss: 0.28203, acc: 0.89062, cma: 0.81567     \n",
      " epoch:36, batch:389/390, loss: 0.49398, acc: 0.73438, cma: 0.81310     \n",
      " epoch:37, batch:389/390, loss: 0.48427, acc: 0.75000, cma: 0.81963     \n",
      " epoch:38, batch:389/390, loss: 0.54926, acc: 0.70312, cma: 0.81695     \n",
      " epoch:39, batch:389/390, loss: 0.53388, acc: 0.76562, cma: 0.82212     \n",
      " epoch:40, batch:389/390, loss: 0.38835, acc: 0.81250, cma: 0.82103     \n",
      " epoch:41, batch:389/390, loss: 0.42916, acc: 0.84375, cma: 0.82604     \n",
      " epoch:42, batch:389/390, loss: 0.47062, acc: 0.75000, cma: 0.82588     \n",
      " epoch:43, batch:389/390, loss: 0.36514, acc: 0.84375, cma: 0.82376     \n",
      " epoch:44, batch:389/390, loss: 0.37104, acc: 0.85938, cma: 0.82556     \n",
      " epoch:45, batch:389/390, loss: 0.53490, acc: 0.75000, cma: 0.82889     \n",
      " epoch:46, batch:389/390, loss: 0.49576, acc: 0.73438, cma: 0.83225     \n",
      " epoch:47, batch:389/390, loss: 0.31614, acc: 0.87500, cma: 0.83317     \n",
      " epoch:48, batch:389/390, loss: 0.44043, acc: 0.79688, cma: 0.83510     \n",
      " epoch:49, batch:389/390, loss: 0.31185, acc: 0.84375, cma: 0.83682     \n",
      " epoch:50, batch:389/390, loss: 0.32061, acc: 0.89062, cma: 0.83450     \n",
      " epoch:51, batch:389/390, loss: 0.33790, acc: 0.81250, cma: 0.83554     \n",
      " epoch:52, batch:389/390, loss: 0.49820, acc: 0.71875, cma: 0.83750     \n",
      " epoch:53, batch:389/390, loss: 0.33029, acc: 0.87500, cma: 0.83910     \n",
      " epoch:54, batch:389/390, loss: 0.37573, acc: 0.82812, cma: 0.84131     \n",
      " epoch:55, batch:389/390, loss: 0.29809, acc: 0.87500, cma: 0.84407     \n",
      " epoch:56, batch:389/390, loss: 0.40407, acc: 0.79688, cma: 0.84599     \n",
      " epoch:57, batch:389/390, loss: 0.37850, acc: 0.84375, cma: 0.84331     \n",
      " epoch:58, batch:389/390, loss: 0.60220, acc: 0.67188, cma: 0.84475     \n",
      " epoch:59, batch:389/390, loss: 0.43150, acc: 0.79688, cma: 0.84575     \n",
      " epoch:60, batch:389/390, loss: 0.39316, acc: 0.84375, cma: 0.85004     \n",
      " epoch:61, batch:389/390, loss: 0.32194, acc: 0.85938, cma: 0.84872     \n",
      " epoch:62, batch:389/390, loss: 0.44969, acc: 0.85938, cma: 0.85040     \n",
      " epoch:63, batch:389/390, loss: 0.29040, acc: 0.87500, cma: 0.85124     \n",
      " epoch:64, batch:389/390, loss: 0.30020, acc: 0.87500, cma: 0.85148     \n",
      " epoch:65, batch:389/390, loss: 0.42878, acc: 0.81250, cma: 0.85120     \n",
      " epoch:66, batch:389/390, loss: 0.20669, acc: 0.96875, cma: 0.85252     \n",
      " epoch:67, batch:389/390, loss: 0.41870, acc: 0.84375, cma: 0.85345     \n",
      " epoch:68, batch:389/390, loss: 0.14436, acc: 0.95312, cma: 0.85300     \n",
      " epoch:69, batch:389/390, loss: 0.29175, acc: 0.90625, cma: 0.85877     \n",
      " epoch:70, batch:389/390, loss: 0.26296, acc: 0.85938, cma: 0.85497     \n",
      " epoch:71, batch:389/390, loss: 0.37636, acc: 0.85938, cma: 0.85769     \n",
      " epoch:72, batch:389/390, loss: 0.31782, acc: 0.81250, cma: 0.85945     \n",
      " epoch:73, batch:389/390, loss: 0.29451, acc: 0.87500, cma: 0.86098     \n",
      " epoch:74, batch:389/390, loss: 0.28172, acc: 0.84375, cma: 0.86198     \n",
      " epoch:75, batch:389/390, loss: 0.34972, acc: 0.85938, cma: 0.86278     \n",
      " epoch:76, batch:389/390, loss: 0.46697, acc: 0.78125, cma: 0.86266     \n",
      " epoch:77, batch:389/390, loss: 0.24838, acc: 0.92188, cma: 0.86374     \n",
      " epoch:78, batch:389/390, loss: 0.21198, acc: 0.90625, cma: 0.86611     \n",
      " epoch:79, batch:389/390, loss: 0.36089, acc: 0.87500, cma: 0.86663     \n",
      " epoch:80, batch:389/390, loss: 0.31012, acc: 0.84375, cma: 0.86727     \n",
      " epoch:81, batch:389/390, loss: 0.32961, acc: 0.89062, cma: 0.86739     \n",
      " epoch:82, batch:389/390, loss: 0.29855, acc: 0.82812, cma: 0.86883     \n",
      " epoch:83, batch:389/390, loss: 0.24874, acc: 0.90625, cma: 0.86983     \n",
      " epoch:84, batch:389/390, loss: 0.21008, acc: 0.93750, cma: 0.86879     \n",
      " epoch:85, batch:389/390, loss: 0.28917, acc: 0.87500, cma: 0.87248     \n",
      " epoch:86, batch:389/390, loss: 0.29740, acc: 0.84375, cma: 0.87167     \n",
      " epoch:87, batch:389/390, loss: 0.49794, acc: 0.75000, cma: 0.87364     \n",
      " epoch:88, batch:389/390, loss: 0.27878, acc: 0.89062, cma: 0.87356     \n",
      " epoch:89, batch:389/390, loss: 0.37704, acc: 0.87500, cma: 0.87352     \n",
      " epoch:90, batch:389/390, loss: 0.20609, acc: 0.90625, cma: 0.87720     \n",
      " epoch:91, batch:389/390, loss: 0.32068, acc: 0.85938, cma: 0.87696     \n",
      " epoch:92, batch:389/390, loss: 0.25298, acc: 0.89062, cma: 0.87817     \n",
      " epoch:93, batch:389/390, loss: 0.26613, acc: 0.89062, cma: 0.87973     \n",
      " epoch:94, batch:389/390, loss: 0.21489, acc: 0.93750, cma: 0.87821     \n",
      " epoch:95, batch:389/390, loss: 0.16970, acc: 0.93750, cma: 0.87917     \n",
      " epoch:96, batch:389/390, loss: 0.33312, acc: 0.87500, cma: 0.87917     \n",
      " epoch:97, batch:389/390, loss: 0.33726, acc: 0.85938, cma: 0.88061     \n",
      " epoch:98, batch:389/390, loss: 0.27630, acc: 0.90625, cma: 0.87945     \n",
      " epoch:99, batch:389/390, loss: 0.33512, acc: 0.85938, cma: 0.88353     \n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "model = MyResNet()\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(axis=-1)\n",
    "\n",
    "for e in range(epochs):\n",
    "    cma = 0\n",
    "    \n",
    "    # shuffle and batch the dataset\n",
    "    batched_paths = batchfy(paths, batch_size)\n",
    "    \n",
    "    for i, batch in enumerate(batched_paths):\n",
    "        X, Y = load_batch(batch)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            P = model(X)\n",
    "            L = loss(Y, P)\n",
    "        \n",
    "        A = tf.reduce_mean(tf.cast(tf.argmax(P,-1) == tf.argmax(Y,-1), float))\n",
    "        cma = cma + (A-cma) / (i+1)\n",
    "        G = tape.gradient(L, model.trainable_variables)\n",
    "        optim.apply_gradients(zip(G, model.trainable_variables))\n",
    "\n",
    "        print(f\"\\r epoch:{e}, batch:{i}/{len(batched_paths)}, loss: {L.numpy():.5f}, acc: {A.numpy():.5f}, cma: {cma:.5f}\", end=\" \"*5)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-venv",
   "language": "python",
   "name": "dl-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
