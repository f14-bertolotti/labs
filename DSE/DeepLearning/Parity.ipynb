{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b88175e7-f870-45aa-8a5c-ac3203ce263b",
   "metadata": {},
   "source": [
    "# The Parity Problem\n",
    "The problem is very simple and it can be solved just in few lines of code.<br />\n",
    "Of course, we are going to use Recursive Neural Network (RNN) to solve our problem.<br />\n",
    "You have a binary list, for example ```[1,1,0,1,0,1,0,1,0]```, and you need to check if the number of one is even or odd.<br />\n",
    "We can solve this problem just in a few lines for code.<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4651f378-f2fd-47e8-ab0e-b4d94fb3c281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def iseven(lst):\n",
    "    return sum(lst) % 2 == 0\n",
    "\n",
    "def isodd(lst):\n",
    "    return not iseven(lst)\n",
    "\n",
    "print(f\"is 101010 even: {iseven([1,0,1,0,1,0])}\")\n",
    "print(f\"is 101010  odd: { isodd([1,0,1,0,1,0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b902a8b0-fa45-443a-8abe-59b0d46cc2e4",
   "metadata": {},
   "source": [
    "Now we need to build a dataset full of this problems with their solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95857a85-65b1-4420-8d3a-7dfd2d91b798",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_size = 10000\n",
    "max_length = 10\n",
    "\n",
    "import random\n",
    "def gen_problem():\n",
    "    size = random.randint(1,max_length)\n",
    "    return [random.randint(0,1) for _ in range(size)] + [0] * ( max_length - size )\n",
    "\n",
    "def gen_dataset():\n",
    "    return [gen_problem() for _ in range(dataset_size)]\n",
    "\n",
    "dataset = gen_dataset()\n",
    "solutions = [1 if iseven(problem) else 0 for problem in dataset]\n",
    "for i in range(5):\n",
    "    print(f\"problem {i}: {dataset[i]}, sol: {solutions[i]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b04a7-7372-423f-879e-4e4b1ba3e360",
   "metadata": {},
   "source": [
    "Now, we have our own dataset. <br />\n",
    "We need to split the dataset into test, validation, and training. <br />\n",
    "- The training slice will be used to train out model.\n",
    "- The validation slice will be used to check how our model is doing during training.\n",
    "- The test slice will be used to check how our model is doing after training.\n",
    "\n",
    "It is important to have large training, validation, and test dataset so that <br />\n",
    "you have enough data to train and validate your model without the risk of overfitting.\n",
    "A good ratio of data is:\n",
    "- 70% for training.\n",
    "- 15% for validating.\n",
    "- 15% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e26294-ab4b-4f73-a579-1d8366811a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trs = int(dataset_size * 0.70)\n",
    "vls = int(dataset_size * 0.15)\n",
    "tss = int(dataset_size * 0.15)\n",
    "training_set   = (dataset[:trs], solutions[:trs])\n",
    "validation_set = (dataset[trs:trs+vls], solutions[trs:trs+vls])\n",
    "test_set       = (dataset[trs+vls:trs+vls+tss], solutions[trs+vls:trs+vls+tss])\n",
    "print(f\"training size: {len(training_set[0])}, validation size: {len(validation_set[0])}, test size: {len(test_set[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72c702b-93d8-4766-bf7c-e1032739bd73",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "Tensorflow uses its own data type to process data which are tensors.<br />\n",
    "Tensors are lists that need to be homogeneous in all dimension.<br />\n",
    "For example: \n",
    "- Tensor([1,1,1,1]) is a tensorm of dimension 1 and size 4.\n",
    "- Tensor([[1,1], [1,1], [1,1]]) is a tensor of dimension 2 and size (3, 2).\n",
    "- Tensor([[1,1,1], [1,1], [1,1]]) is not a valid tensor as it is not homogeneous\n",
    "\n",
    "Let's convert our datasets into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adb7bf0-c310-4c21-8cba-99a8b8ef8136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "training_set   = (tf.convert_to_tensor(  training_set[0], dtype=float), tf.convert_to_tensor(  training_set[1], dtype=float))\n",
    "validation_set = (tf.convert_to_tensor(validation_set[0], dtype=float), tf.convert_to_tensor(validation_set[1], dtype=float))\n",
    "test_set       = (tf.convert_to_tensor(      test_set[0], dtype=float), tf.convert_to_tensor(      test_set[1], dtype=float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ac1aa6-fd42-40c4-967a-6f098870e67b",
   "metadata": {},
   "source": [
    "### Network architecture\n",
    "Next, we need to define a neural network architecture. <br />\n",
    "We will use a RNN, but it will be implemented from scratch. <br />\n",
    "However, tensorflow already has implementation for many popular architecture. \n",
    "\n",
    "<img src=\"https://static.wixstatic.com/media/3eee0b_969c1d3e8d7943f0bd693d6151199f69~mv2.gif\"  width=\"600\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3166ed2f-529d-44cf-962b-ebbfb3957a41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, max_length, batch_size, size=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.tob = tf.keras.layers.Dense(size, activation=\"relu\")\n",
    "        self.x2h = tf.keras.layers.Dense(size, activation=\"relu\")\n",
    "        self.h2y = tf.keras.layers.Dense(size, activation=\"relu\")\n",
    "        self.h2h = tf.keras.layers.Dense(size, activation=\"relu\")\n",
    "        self.y2s = tf.keras.layers.Dense(1)\n",
    "        self.h0  = tf.zeros((1, size))\n",
    "        \n",
    "    def call(self, batch):\n",
    "        b = self.tob(batch)\n",
    "        \n",
    "        h = self.h0\n",
    "        for i in range(self.max_length):\n",
    "            x = tf.expand_dims(b[:,i],-1)\n",
    "            h = h + self.x2h(x)\n",
    "            y = self.h2y(h)\n",
    "            h = self.h2h(h)\n",
    "        \n",
    "        s = self.y2s(y)\n",
    "        \n",
    "        return tf.math.sigmoid(tf.reshape(s, (-1)))\n",
    "    \n",
    "batch_size = 50\n",
    "model = Model(max_length, batch_size, size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973cce2-0e34-4e7d-bcfa-756c4ceed4b8",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "Now that we have a model and the data we can finally start the training. <br />\n",
    "We feed the model with small batches of problems at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869b7f92-97cb-490c-a4ac-43d7d71c3834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "def MSE(preds, truths):\n",
    "    return tf.math.reduce_mean((preds - truths) ** 2)\n",
    "\n",
    "def acc(preds, truths):\n",
    "    return tf.math.reduce_mean(tf.cast(tf.math.round(preds) == truths, dtype=float))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# split the training set in batches\n",
    "training_batches = (tf.reshape(training_set[0],[trs//batch_size, batch_size, max_length]), tf.reshape(training_set[1],[trs//batch_size, batch_size]))\n",
    "      \n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    # Train \n",
    "    for i, (batchX, batchY) in enumerate(zip(*training_batches)):\n",
    "\n",
    "        # register computational graph\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model(batchX)\n",
    "            loss  = MSE(preds, batchY)\n",
    "\n",
    "        # compute gradients from graph wrt. model variables\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        # log results\n",
    "        print(f\"\\r epoch: {e}/{epochs}, batch: {i}/{trs//batch_size}, loss: {tf.get_static_value(loss):.5f}, acc: {tf.get_static_value(acc(preds, batchY)):.5f}\", end=\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ad6e2a-abf3-46c3-b93d-e6c41009f44a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercises\n",
    "- (⭐) validate the model every time the epoch ends\n",
    "- (⭐) test the model at the end of all epochs\n",
    "- (⭐⭐⭐) rewrite the model so that it works with max_length >= 50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
