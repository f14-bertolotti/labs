{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f901e84f-0d2e-4a43-97fa-77ecef88cf2d",
   "metadata": {},
   "source": [
    "### 3.3 Tensorflow: Text Sequence Modeling \n",
    "\n",
    "In this section, we will use several high-level python libraries to address a simple textual task. Given an incomplete sentece, we want to predict the next token. First, we will start with the dataset. We will use the [datasets] library from [huggingface]. [datasets] is a well organized collection of several datasets. We will use the [bookcorpus] dataset, which is a collection of several books. \n",
    "\n",
    "### 3.3.1 Download Bookcorpus\n",
    "\n",
    "[bookcorpus]: https://huggingface.co/datasets/bookcorpus\n",
    "[huggingface]:https://huggingface.co/\n",
    "[datasets]:https://huggingface.co/docs/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d504aeb3-16f0-478a-ab26-02004a89649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "dataset = datasets.load_dataset(\"bookcorpus\", split=\"train[:5%]\")\n",
    "print(f\"# samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133470a-c0ea-42bb-83c5-99cdf02aa414",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dbcba9-83a3-4620-bb34-b89c8dc7b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2ascii(string):\n",
    "    return [ord(char) for char in string.lower() if char.isascii()]\n",
    "\n",
    "tkn2id = {\"PAD\":256, \"SOS\":257, \"EOS\":258}\n",
    "\n",
    "def ascii2str(lst):\n",
    "    return \"\".join([chr(char) if char<256 else str(char-256) for char in lst])\n",
    "\n",
    "print(\"str2ascii(\\\"hello my fried\\\") ->\", str2ascii(\"hello my friend\"))\n",
    "print(\"ascii2str(str2ascii(\\\"hello my fried\\\")) ->\", ascii2str(str2ascii(\"hello my fried\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42636833-9c55-4fb8-8bf6-749ec18538cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "def collate(data):\n",
    "    batch = tf.convert_to_tensor([([tkn2id[\"SOS\"]] + str2ascii(x[\"text\"])+[tkn2id[\"EOS\"]]+[tkn2id[\"PAD\"]]*64)[:64] for x in data])\n",
    "    return {\"text\"  : batch[:,:-1],\n",
    "            \"targets\" : batch[:,+1:]}\n",
    "\n",
    "tfdataset = dataset.to_tf_dataset(columns=\"text\", shuffle=False, label_cols=\"targets\", batch_size=16, collate_fn=collate) \\\n",
    "                   .prefetch(10000) \\\n",
    "                   .shuffle(10000) \\\n",
    "                   .repeat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0305e76-c537-492d-bc7d-0c6e550dc6ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3.3 Embedding.\n",
    "\n",
    "Feeding integers from $0$ to $vocab\\_size$ is not practical. Meaning that it does not work. To overcome this issue, we map token ids to embedding. Each token is associated to its trainable vector of parameters. During training embeddings learn to represent the token. For example, the figure below shows a possible scenario during training. \n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1400/1*xD9n3KeWXuenMNL_BpYp6A.png\" alt=\"drawing\" width=\"600\"/></center>\n",
    "\n",
    "\n",
    "### 3.3.4 Recurrent Neural Networks\n",
    "\n",
    "Recurrent neural networks (RNNs) are a popular neural network architectures to process sequences. They are fed with one embedding. They process the embedding internally and output their state. At the next step, they are fed with another embedding. Again, they process the embedding internally with the inner state modified by the previous embeddings and so on. \n",
    "\n",
    "<center><img src=\"https://research.aimultiple.com/wp-content/uploads/2021/08/rnn-text.gif\" alt=\"drawing\" width=\"400\"/></center>\n",
    "\n",
    "There are many kinds of RNNs. One popular choice are Long short-term memory (LSTM). LSTM is a fairly complex layer involving many components. Tensorflow already implements LSTM internally. \n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1374/1*FCVyju8lPTvfFfxT-rzInA.png\" alt=\"drawing\" width=\"400\"/></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f12602-7908-4d29-8d96-0aaf7c1cf4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=257, output_dim=512)\n",
    "        self.lstm1 = tf.keras.layers.LSTM(512, dropout=0.1, return_sequences=True)\n",
    "        self.lstm2 = tf.keras.layers.LSTM(512, dropout=0.1, return_sequences=True)\n",
    "        self.lstm3 = tf.keras.layers.LSTM(512, dropout=0.1, return_sequences=True)\n",
    "        self.lstm4 = tf.keras.layers.LSTM(512, dropout=0.1, return_sequences=True)\n",
    "        self.lstm5 = tf.keras.layers.LSTM(512, dropout=0.1, return_sequences=True)\n",
    "        \n",
    "        self.lnrm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.lnrm2 = tf.keras.layers.LayerNormalization()\n",
    "        self.lnrm3 = tf.keras.layers.LayerNormalization()\n",
    "        self.lnrm4 = tf.keras.layers.LayerNormalization()\n",
    "        self.lnrm5 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(257)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.lnrm1(x + self.lstm1(x))\n",
    "        x = self.lnrm2(x + self.lstm2(x))\n",
    "        x = self.lnrm3(x + self.lstm3(x))\n",
    "        x = self.lnrm4(x + self.lstm4(x))\n",
    "        x = self.lnrm5(x + self.lstm5(x))\n",
    "    \n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss  = tf.keras.losses.CategoricalCrossentropy(from_logits=True, axis=-1)\n",
    "model = MyModel()\n",
    "\n",
    "def loss_fn(batchY, batchP):\n",
    "    batchP = tf.reshape(batchP[batchY != tkn2id[\"PAD\"]], (-1,257))\n",
    "    batchY = tf.one_hot(tf.cast(batchY[batchY != tkn2id[\"PAD\"]],dtype=tf.int32), depth=257)\n",
    "    return loss(batchY, batchP)\n",
    "\n",
    "def accuracy(batchY, batchP):\n",
    "    return tf.reduce_mean(tf.cast(tf.argmax(batchP,-1)[batchY != tkn2id[\"PAD\"]] == tf.cast(batchY[batchY != tkn2id[\"PAD\"]],tf.int64), tf.float64))\n",
    "\n",
    "model.compile(loss=loss_fn, optimizer=optim, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f1dc6-367b-4116-a7cd-ea0dce4894fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tfdataset, \n",
    "          steps_per_epoch=100, \n",
    "          verbose=True,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ca691-c420-440e-8486-6246631d0a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [tkn2id[\"SOS\"]] + str2ascii(\"only because\")\n",
    "for i in range(40):\n",
    "    x1 = tf.convert_to_tensor(x, dtype=tf.int64)\n",
    "    x2 = tf.expand_dims(x1,0)\n",
    "    p1 = model.predict(x2)\n",
    "    p2 = tf.argmax(p1,-1)[0]\n",
    "    x += [p2[-1].numpy()]\n",
    "\n",
    "print(ascii2str(x))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
