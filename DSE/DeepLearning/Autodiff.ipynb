{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae67d3b-e35e-48b8-8f42-db3db6fe4be7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Preliminaries.\n",
    "\n",
    "Before talking about deep learning and deep learning framework. Let's establish a bit of notation. From here on, we will talk about optimizing models. Think of a model as a set of functions. For example, a model could be a family of functions described as \n",
    "$$F(x; w) = x + w \\text{ with } x,w \\in \\mathbb{R}$$\n",
    "$$\\text{or equivantly } F(x; w) = \\{f:\\mathbb{R}\\rightarrow\\mathbb{R}\\texttt{ }\\vert\\texttt{ }f(x) = x + w, w \\in \\mathbb{R}\\}$$\n",
    "\n",
    "However, we will use the first notation. Now, let's make a very important distinction. We call the arguments before \";\" **inputs** (such is $x$). Instead, we call the argument after \";\" **parameters** (such is $w$).\n",
    "\n",
    "In python a model could look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "192e7c25-9b24-402b-baca-04ea1f0fa3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F returns functions. For example F(5) = <function F.<locals>.<lambda> at 0x7fe3a81a3d00>\n",
      "if F returns functions we can call F(5). For example, F(5)(5) = 10\n"
     ]
    }
   ],
   "source": [
    "def F(w):\n",
    "    return lambda x: x + w\n",
    "\n",
    "print(f\"F returns functions. For example F(5) = {F(5)}\")\n",
    "print(f\"if F returns functions we can call F(5). For example, F(5)(5) = {F(5)(5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633ff2cc-d1a8-4dbb-a476-394d7da7bdb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "______\n",
    "\n",
    "## 1.1 Training/Optimizing\n",
    "\n",
    "What does it mean to train or optimize a model? It means searching among the family of functions for those who satisfy desired properties. For example, we could want to search for symmetric functions. Or we could search for functions that interpolate a set of points. Ultimately, this amount to find good parameters. You can search for good parameters any ways you desire. You can even randomly sample parameters until you find good ones.\n",
    "\n",
    "______\n",
    "\n",
    "## 1.2 Optimize a constant function.\n",
    "\n",
    "Let's make a concrete, yet super simple example. This should help you to understand how all blocks fall together. So consider the following model: \n",
    "\n",
    "$$F(;w_1) = w_1^2 + 1$$ \n",
    "\n",
    "Where $w_1 \\in [-10,10]$. $F$ is a model which can be instantiated in infinitely many functions. Each one of these functions has no input and constant real output.  This function is not particularly useful to optimize but it can help you to understand the methodology. \n",
    "\n",
    "Now suppose that we want to find in $F$ the function with the lowest output. For example, if we set $w_1 = 5$ we get $g(x) = 5^2+1 = 26$ which is not very good. Instead, if we set $w_1 = 2$ we get $s(x)=2^2+1=5$ which is already better. Let's formalize what we want:\n",
    "\n",
    "$$\\min_{w \\in [-10,10]}\\{F(;w_1)\\} = \\min_{w \\in [-10, 10]}\\{w_1^2 + 1\\}$$\n",
    "\n",
    "______\n",
    "\n",
    "### 1.1.1 Zeroth Order Optimization (or derivative-free optimization)\n",
    "Zeroth order optimization tries to optimize models without knowledge of gradients. 0th order optimization is useful when you cannot compute gradients of the model (it may require too much time, too much memory, or you do not know the model at all). These methods are often called black-box optimization, as they require only to be able to query the model. One common kind of zeroth-order algorithm is evolutionary ones.\n",
    "\n",
    "Let's see a super simple example of Zeroth Order optimization that randomly search for the optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c799e7c8-6c90-4ee4-8c25-ef55eb3367d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter found w1=-0.0003224070760445841 which yield f(w1)=1.0000001039463227\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# let's define our model as before\n",
    "def F1(w1):\n",
    "    return lambda: w1**2 + 1\n",
    "    \n",
    "# let's search for good functions by randomly sampling w1.\n",
    "best_score = float(\"+inf\")\n",
    "best_param = None\n",
    "for i in range(10000):\n",
    "    w1 = random.uniform(-10, 10)\n",
    "    if F1(w1)() < best_score:\n",
    "        best_score, best_param = F1(w1)(), w1\n",
    "        \n",
    "print(f\"best parameter found w1={best_param} which yield f(w1)={F1(best_param)()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0e0e83-ad10-4f4e-8f2b-99d74ff31d5b",
   "metadata": {},
   "source": [
    "Which is pretty close to the optimal value ($1$). Unfortunately, randomly searching works only on super simple cases. If we want to tackle real world problems we need to searching more intelligently. \n",
    "\n",
    "______\n",
    "\n",
    "### 1.1.2 First Order Optimization\n",
    "\n",
    "First-order optimization algorithms require the knowledge of the derivative of $F$. The most used first-order technique is gradient descent. With respect to zeroth-order algorithms, we have more knowledge so usually, we can obtain good results faster. However, computing gradients does require time and memory. Let's compute the derivative of $F$ wrt. $w_1$.\n",
    "\n",
    "$$\\frac{dF(;w_1)}{dw_1} = \\frac{d w_1^2 + 1}{d w_1} = 2w_1$$\n",
    "\n",
    "Now, no matter how we choose $w1$, we can compute the slope of the model. By knowing the slope, we can make small steps towards smaller and smaller functions. Let me visualize a little bit better what I mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1915c9c5-e53f-4628-b950-90b09b3fc95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ccb1f5360542b8a5dc47531e3866af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='w1', max=10.0, min=-10.0, step=0.25), Output()), _do…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib widget\n",
    "%matplotlib inline\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@widgets.interact(w1=(-10, 10, 0.25))\n",
    "def update(w1 = 1.0):\n",
    "    w = np.linspace(-10, 10,1000)\n",
    "    y = F1(w)() \n",
    "    plt.plot(w, y)\n",
    "    \n",
    "    plt.scatter(w1, F1(w1)())\n",
    "    \n",
    "    x = np.linspace(-10,10)\n",
    "    y = 2*x*w1 - F1(w1)() + 2\n",
    "    plt.plot(x,y)\n",
    "    \n",
    "    plt.gca().set_xlim((-5,+5))\n",
    "    plt.gca().set_ylim((-2,+10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b7917-4d93-4adc-b732-d10df46559e7",
   "metadata": {},
   "source": [
    "The blue line is our model. Remember, each point of our model is actually a function, a constant function.For each of these points we can compute a derivative, the derivative tells us the slope of the model. By following the slope we can obtain a function with a higher value. By following the negative slope, we can obtain a function with a lower value. Step by step, we can obtain increasingly bigger functions of increasingly smaller functions.   \n",
    "\n",
    "Just for fun, let's write an algorithm that performs the gradient descent of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b283019-831d-44f3-a5c9-acf7a645286b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter found w1=-1.281963925035065e-08 which yield f(w1)=1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def F     (w1): return lambda: w1**2 + 1\n",
    "def dF_dw1(w1): return 2*w1\n",
    "\n",
    "param_w1 = random.uniform(-10,10)\n",
    "learning_rate = 0.001\n",
    "\n",
    "for i in range(10000):\n",
    "    param_w1 = param_w1 - learning_rate * dF_dw1(param_w1) # compute a small step in the direction of the slope\n",
    "\n",
    "print(f\"best parameter found w1={param_w1} which yield f(w1)={F(param_w1)()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f20d9-3560-4e79-ad4d-1f5cfc92ea84",
   "metadata": {},
   "source": [
    "______\n",
    "\n",
    "### 1.1.3 Second Order Optimization\n",
    "\n",
    "We could go a step beyond and use even the second derivative. There are a lot of reasons to use and to not use the second derivative. However, we will not cover these optimization methods.\n",
    "______\n",
    "\n",
    "## 1.2 Optimize a simple function\n",
    "\n",
    "Now that we have seen how to optimize a model that produced constant functions, we can now try to optimize a model that describes more interesting functions. Consider this model:\n",
    "\n",
    "$$ F(x;w_1, w_2) = x*w_1 + w_2 $$\n",
    "\n",
    "Firstly, let's see what kinds of functions does our $F$ describes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b646947e-7c4c-40ad-b27d-ae1fef12ccc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b80ea6c0a44b52bddce6c4ee427090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=7.5, description='w1', max=10.0, min=-10.0, step=0.25), FloatSlider(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def F(w1, w2):\n",
    "    return lambda x: x*w1+w2\n",
    "\n",
    "@widgets.interact(w1=(-10, 10, 0.25), w2=(-10, 10, 0.25))\n",
    "def plot(w1=7.5, w2=7.5):\n",
    "    x = np.linspace(-10, 10)\n",
    "    y = F(w1, w2)(x)\n",
    "    plt.plot(x, y)\n",
    "    plt.grid()\n",
    "    plt.gca().set_xlim((-5,+5))\n",
    "    plt.gca().set_ylim((-10,+10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45838325-4942-49a7-9340-d0e68914b185",
   "metadata": {},
   "source": [
    "These are all straight lines. No matter which $w_1$ and $w_2$ you will choose. $f$ will always describe a straight line. Let's suppose that we want to find the function with these input-output relations:\n",
    "\n",
    "| x | y |\n",
    "|---|---|\n",
    "| 1 | 2 |\n",
    "| 2 | 3 |\n",
    "| 3 | 4 |\n",
    "| 4 | 5 |\n",
    "\n",
    "A function from our model is good when fed with $x$s it outputs something close to the described $y$s. Let's define a bit more formally what does this means. So, we derive yet another model from F, which we call L.\n",
    "\n",
    "$$ L(x; w_1, w_2) = \\sum_{(x,y) \\in D} (F(x; w1, w2) - y)^2 $$\n",
    "\n",
    "$L$ is close to $0$ when $F(x_i; w1, w2)$ is similar to $y_i$, for all $i$. In fact, $L$ is exactly $0$ when $F(x_i; w1, w2) = y_i$, for all $i$. So, optimizing this model yields exactly the property that we desire. Again, we need to compute the gradients.\n",
    "\n",
    "Now we need the derivative wrt. $w_1$: \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L(x;w_1,w_2)}{\\partial w_1} = & \\frac{\\partial \\sum_{(x,y) \\in D} (F(x; w1, w2) - y)^2}{d w_1}  \\\\\n",
    "= & \\sum_{(x,y) \\in D} \\frac{\\partial(F(x; w1, w2) - y)^2}{\\partial w_1} \\\\\n",
    "= & \\sum_{(x,y) \\in D} \\frac{\\partial(x*w_1+w_2 - y)^2}{\\partial w_1}  \\\\\n",
    "= & \\sum_{(x,y) \\in D} 2(x*w_1+w_2 - y) \\frac{\\partial x*w_1+w_2 - y}{\\partial w_1}  \\\\\n",
    "= & \\sum_{(x,y) \\in D} 2(x*w_1+w_2 - y)x\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And also wrt. $w_2$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L(x;w_1,w_2)}{\\partial w_2} = & \\frac{d \\sum_{(x,y) \\in D} (F(x; w1, w2) - y)^2}{\\partial w_2} \\\\\n",
    "= & \\sum_{(x,y) \\in D} \\frac{\\partial(F(x; w1, w2) - y)^2}{\\partial w_2} \\\\\n",
    "= & \\sum_{(x,y) \\in D} \\frac{\\partial(x*w_1+w_2 - y)^2}{\\partial w_2} \\\\\n",
    "= & \\sum_{(x,y) \\in D} 2(x*w_1+w_2 - y) \\frac{dx*w_1+w_2 - y}{\\partial w_2} \\\\\n",
    "= & \\sum_{(x,y) \\in D} 2(x*w_1+w_2 - y)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now, we have again all the pieces to perform our first order optimization, as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c176299-183c-489b-aa03-f3da009ccf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter found w1=(1.0000000000000249, 0.999999999999931) which yield f(2;w1,w2)=2.999999999999981\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "x = np.array([1,2,3,4])\n",
    "y = np.array([2,3,4,5])\n",
    "\n",
    "def F(w1, w2):\n",
    "    return lambda x: x*w1+w2\n",
    "\n",
    "def dF_dw1(w1, w2):\n",
    "    return (2*(x * w1 + w2 - y)*x).sum()\n",
    "\n",
    "def dF_dw2(w1, w2):\n",
    "    return (2*(x * w1 + w2 - y)).sum()\n",
    "\n",
    "param_w1 = random.uniform(-10,10)\n",
    "param_w2 = random.uniform(-10,10)\n",
    "learning_rate = 0.001\n",
    "\n",
    "for i in range(100000):\n",
    "    param_w1, param_w2 = (param_w1 - learning_rate * dF_dw1(param_w1, param_w2), \n",
    "                          param_w2 - learning_rate * dF_dw2(param_w1, param_w2))\n",
    "\n",
    "print(f\"best parameter found w1={param_w1, param_w2} which yield f(2;w1,w2)={F(param_w1, param_w2)(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d5980a-a23e-48e2-96d2-95f5172eac5e",
   "metadata": {},
   "source": [
    "_________________________________________\n",
    "# 2. Automatic Differentiation.\n",
    "\n",
    "As you have noticed, computing the gradients by hand is a real pain, and it becomes unmanageable real fast. Fortunately, this work can be done automatically. Again there are many ways to perform differentiation. Let's review a few of them. First let's define the gradient of a function $f(x_1, x_2, \\dots, x_n)$\n",
    "\n",
    "$$\\nabla f(x_1, x_2, \\dots, x_n) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1}(x_1, x_2, \\dots, x_n) \\\\\n",
    "\\frac{\\partial f}{\\partial x_2}(x_1, x_2, \\dots, x_n) \\\\\n",
    "\\dots \\\\\n",
    "\\frac{\\partial f}{\\partial x_n}(x_1, x_2, \\dots, x_n)\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "There are two main ways to compute $\\nabla f(x_1, x_2, \\dots, x_n)$.\n",
    "* **Numerical differentiation**. We are only interested in computing the value of the gradient for a particular point in the domain of $f$.\n",
    "* **Symbolic differentiation**. We are interested in the formula representing the gradients for $f$ for each point in the domain of $f$. \n",
    "\n",
    "There are several techniques to compute both Numerical differentiation and Symbolic differentiation. Let's review very simple ones. \n",
    "______\n",
    "\n",
    "# 2.1. Numerical Differentiation.\n",
    "\n",
    "To get an approximation of the derivative of a function $f$ in a point $(x_0, x_1, \\dots, x_n)$, we can use the definition: \n",
    "\n",
    "$$\\nabla f(x_0, x_1, \\dots, x_n) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1}(x_1, x_2, \\dots, x_n) \\\\\n",
    "\\frac{\\partial f}{\\partial x_2}(x_1, x_2, \\dots, x_n) \\\\\n",
    "\\dots \\\\\n",
    "\\frac{\\partial f}{\\partial x_n}(x_1, x_2, \\dots, x_n)\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\lim_{\\epsilon \\rightarrow 0} \\frac{f(x_0+\\epsilon,x_1,\\dots,x_n)-f(x_1, x_2, \\dots, x_n)}{\\epsilon} \\\\\n",
    "\\lim_{\\epsilon \\rightarrow 0} \\frac{f(x_0,x_1+\\epsilon,\\dots,x_n)-f(x_1, x_2, \\dots, x_n)}{\\epsilon} \\\\\n",
    "\\dots \\\\\n",
    "\\lim_{\\epsilon \\rightarrow 0} \\frac{f(x_0,x_1,\\dots,x_n+\\epsilon)-f(x_1, x_2, \\dots, x_n)}{\\epsilon}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e9c16a8-2627-47b0-ba20-be0d7ab390ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f7c5852a9c4c418b602dcdb7cd6f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='x0', max=2.0, min=-2.0, step=0.001), Output()), _dom…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f1(x): return x**3\n",
    "def dfdx(f,eps=0.001): return lambda x:(f(x+eps) - f(x))/(eps)\n",
    "\n",
    "@widgets.interact(x0=(-2, 2, 0.001))\n",
    "def plot(x0=1):\n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    y = f1(x)\n",
    "    plt.plot(x, y)\n",
    "    \n",
    "    plt.scatter(x0, f1(x0))\n",
    "    \n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    y = (x-x0)*dfdx(f1)(x0) + f1(x0)\n",
    "    y1 = 3*x**2\n",
    "    plt.plot(x, y)\n",
    "    \n",
    "    plt.gca().set_xlim((-5,+5))\n",
    "    plt.gca().set_ylim((-10,+10))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118301e0-65cf-46e5-9064-a5d99c8d0eaa",
   "metadata": {},
   "source": [
    "This method is perfectly fine and usable. However, it requires computing $f$ as many times as the number of its arguments. Computing $f$ this way may require too much time.\n",
    "______\n",
    "\n",
    "## 2.2. Symbolic Differentiation\n",
    "\n",
    "Instead of recomputing $f$ with small changes, we can compute the gradient symbolically ones. We can use the symbolic formula to obtain the numerical gradients at any point of the domain. Moreover, we can also apply optimizations to the symbolic representation of the gradients. But first, we need to talk about how we represent functions. Functions such as \n",
    "$$f(x) = x^2 + 5x + 2$$ \n",
    "which may look complex, are actually a composition of simpler operations. In this case, there are only three operations: exponentiation ($pow(x,y)=x^y$), multiplication ($mul(x,y)=x*y$), and summation ($sum(x,y)=x+y$). Therefore:\n",
    "\n",
    "$$f(x) = x^2 + 5x + 2 = sum(sum(pow(x,2),mul(5,x)),2)$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae55878-5d75-4e7b-9189-4e1d3332e711",
   "metadata": {
    "tags": []
   },
   "source": [
    "_____\n",
    "### 2.2.1 Chain Rule.\n",
    "\n",
    "Before stepping into the heart of symbolic automatic differentiation, let's see how you can differentiate a composition of functions. Let $f(x) = g(h(x))$ then,  \n",
    "\n",
    "$$\\frac{df(x)}{dx} = \\frac{dg(h(x))}{dx} = \\frac{dg}{dh}\\frac{dh}{dx}$$\n",
    "\n",
    "Or in the multivariate case. Let $f(x) = g(h_1(x),\\dots,h_n(x))$ \n",
    "\n",
    "$$\\frac{df(x)}{dx} = \n",
    "\\frac{g(h_1(x),\\dots,h_n(x))}{dx} = \n",
    "\\frac{\\partial g}{\\partial h_1}\\frac{dh_1}{dx} + \\dots + \\frac{\\partial g}{\\partial h_n}\\frac{dh_n}{dx}$$\n",
    "______\n",
    "\n",
    "#### 2.2.1.1 Example 1 sum().\n",
    "\n",
    "$$\\frac{d sum(f(x),g(x))}{dx} = \n",
    "\\frac{\\partial sum}{\\partial f}\\frac{df}{dx} + \\frac{\\partial sum}{\\partial g}\\frac{dg}{dx} = \n",
    "\\frac{df}{dx} + \\frac{dg}{dx}$$\n",
    "______\n",
    "\n",
    "#### 2.2.1.2 Example 2 mul().\n",
    "\n",
    "$$\\frac{d mul(f(x),g(x))}{dx} = \n",
    "\\frac{\\partial mul}{\\partial f}\\frac{df}{dx} + \\frac{\\partial mul}{\\partial g}\\frac{dg}{dx} = \n",
    "g\\frac{df}{dx} + f\\frac{dg}{dx}$$\n",
    "______\n",
    "\n",
    "#### 2.2.1.3 Example 3 pow().\n",
    "\n",
    "$$\\frac{d pow(f(x),g(x))}{dx} = \n",
    "\\frac{\\partial pow}{\\partial f}\\frac{df}{dx} + \\frac{\\partial pow}{\\partial g}\\frac{dg}{dx} = \n",
    "gf^{g-1}\\frac{df}{dx} + log(f)f^g\\frac{dg}{dx}$$\n",
    "\n",
    "Now, we have the tools to differentiate the function mentioned before (the one in the tree).\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{d sum(sum(pow(x,2),mul(5,x)),2)}{dx} =& \\frac{d sum(pow(x,2),mul(5,x))}{dx} + \\frac{d2}{dx} \\\\\n",
    "= & \\frac{d pow(x,2)}{dx} + \\frac{d mul(5,x)}{dx} + 0 \\\\\n",
    "= & 2x^{2-1}\\frac{dx}{dx} + log(x)x^2\\frac{d2}{dx} + x\\frac{d5}{dx} + 5\\frac{dx}{dx} \\\\\n",
    "= & 2x + 0 + 0 + 5 \\\\\n",
    "= & 2x + 5\n",
    "\\end{aligned}\n",
    "$$ \n",
    "\n",
    "Ok, that was quite a bit of work. At least, it was fairly mechanical. Note that, we only used the knowledge of the derivative of $sum$, $sum$, and $pow$. Given these three pieces, we can derivate any composition of $sum$, $mul$, and $pow$. You can write a differentiation engine in very few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e7f0cfb-959c-4a77-9198-25ed2c056e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sum:\n",
    "    def __init__(self, f, g): \n",
    "        self.f, self.g = f, g\n",
    "    def diff(self, var):\n",
    "        return f\"({self.f.diff(var)} + {self.g.diff(var)})\"\n",
    "    def tostr(self):\n",
    "        return f\"({self.f.tostr()} + {self.g.tostr()})\"\n",
    "        \n",
    "class mul:\n",
    "    def __init__(self, f, g):\n",
    "        self.f, self.g = f, g\n",
    "    def diff(self, var):\n",
    "        return f\"({self.f.tostr()} * {self.g.diff(var)} + {self.g.tostr()} * {self.f.diff(var)})\"\n",
    "    def tostr(self):\n",
    "        return f\"({self.f.tostr()} * {self.g.tostr()})\"\n",
    "                   \n",
    "class pow:\n",
    "    def __init__(self, f, g):\n",
    "        self.f, self.g = f, g\n",
    "    def diff(self, var):\n",
    "        return f\"({self.g.tostr()} * {self.f.tostr()} ^ ({self.g.tostr()}-1))({self.f.diff(var)}) + (log({self.f.tostr()}) * {self.f.tostr()} ^ {self.g.tostr()})({self.g.diff(var)})\"\n",
    "    def tostr(self):\n",
    "        return f\"({self.f.tostr()}) ^ ({self.g.tostr()})\"\n",
    "                   \n",
    "class var:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    def diff(self, var):\n",
    "        if self.name == var.name: return \"1\"\n",
    "        return \"0\"\n",
    "    def tostr(self):\n",
    "        return self.name\n",
    "                   \n",
    "class const:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def diff(self, var):\n",
    "        return \"0\"\n",
    "    def tostr(self):\n",
    "        return self.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "280a5926-1fce-44d0-9cbb-bcfe10fb35f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     f(x) = (((x) ^ (2) + (5 * x)) + 2)\n",
      "d/dx f(x) = (((2 * x ^ (2-1))(1) + (log(x) * x ^ 2)(0) + (5 * 1 + x * 0)) + 0)\n"
     ]
    }
   ],
   "source": [
    "x    = var(\"x\")\n",
    "two  = const(\"2\")\n",
    "five = const(\"5\")\n",
    "\n",
    "f = sum(sum(pow(x, two), mul(five, x)), const(2))\n",
    "\n",
    "print(\"     f(x) =\", f.tostr())\n",
    "print(\"d/dx f(x) =\", f.diff(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "465b19c2-47bf-4fe0-8c53-11dbfdef2fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     f(x,y) = (((x) ^ (2) + (5 * y)) + 2)\n",
      "d/dx f(x,y) = (((2 * x ^ (2-1))(1) + (log(x) * x ^ 2)(0) + (5 * 0 + y * 0)) + 0)\n",
      "d/dy f(x,y) = (((2 * x ^ (2-1))(0) + (log(x) * x ^ 2)(0) + (5 * 1 + y * 0)) + 0)\n"
     ]
    }
   ],
   "source": [
    "x    = var(\"x\")\n",
    "y    = var(\"y\")\n",
    "two  = const(\"2\")\n",
    "five = const(\"5\")\n",
    "\n",
    "f = sum(sum(pow(x, two), mul(five, y)), const(2))\n",
    "print(\"     f(x,y) =\", f.tostr())\n",
    "print(\"d/dx f(x,y) =\", f.diff(x))\n",
    "print(\"d/dy f(x,y) =\", f.diff(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82433b-585b-4370-af4a-0c90d95337e9",
   "metadata": {},
   "source": [
    "We have seen a method for computing derivatives symbolically and a method for computing derivatives numerically. In practice, neither of these methods are used in modern automatic differentiation APIs. Usually, much more efficient algorithms are used to perform automatic differentiation. However, this introduction should give you a brief introduction to the main concepts used in automatic differentiation algorithms.\n",
    "\n",
    "_____________________\n",
    "\n",
    "# 3.Tensorflow\n",
    "\n",
    "[Tensorflow] is one of the most famous automatic differentiation libraries. It provides API to access fast, parallel automatic differentiation libraries. [Tensorflow] does provide much more than automatic differentiation. It provides the whole infrastructure to define and train complex models with millions of parameters.  \n",
    "______\n",
    "\n",
    "## 3.1 Tensorflow: Automatic Differentiation\n",
    "Before defining a complex neural network, let's see how we can use [Tensorflow] to do what we have already done by hand. \n",
    "\n",
    "\n",
    "[Tensorflow]: https://www.tensorflow.org/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fd448033-d0ed-47a7-95a1-97f9d437002c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x**2 + 5*x + 2 is low for x=-0.8116925954818726. (-0.8116925954818726)**2 + 5*(-0.8116925954818726) + 2=-1.399618107849264\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "x = tf.Variable(10.0) # A variable is what we called a parameter.\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Here, operations perfomed are registered.\n",
    "        # Later, these operations will be used to compute gradients.\n",
    "        y = x**2 + 5*x + 2\n",
    "\n",
    "    # Now, we can get the derivative of y with respect of x\n",
    "    dy_dx = tape.gradient(y, x)\n",
    "    \n",
    "    # With the derivative, we can, for example, minimize f(x).\n",
    "    x.assign(x - 0.001*dy_dx)\n",
    "\n",
    "print(f\"x**2 + 5*x + 2 is low for x={x.numpy()}. ({x.numpy()})**2 + 5*({x.numpy()}) + 2={x.numpy()**2 + 5*x.numpy() + 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba2e1dd-dcea-400b-93e6-e2de0725cd1a",
   "metadata": {},
   "source": [
    "In something like 5 lines of code, we obtained the same behavior that we obtained with our custom functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f74fb5a-a58d-4dfe-bc36-7acef202f8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6575ddff14cf4c55bff0870468d72332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='step', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a275c1f8ba594e8591af100b997eb03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='lr', max=1.0, step=0.001)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b118ebf8f1b4f8ba998c95cac5f9276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib widget\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import ipywidgets        as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "import tensorflow        as tf\n",
    "\n",
    "slider = widgets.FloatSlider(description=\"lr\", min=0, max=1,step=0.001)\n",
    "button = widgets.Button(description=\"step\") \n",
    "output = widgets.Output()\n",
    "display(button, slider, output)\n",
    "\n",
    "x = tf.Variable(10.0) \n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        with tf.GradientTape() as tape: y = x**2 + 5*x + 2\n",
    "        x.assign(x - slider.value*tape.gradient(y, x))\n",
    "        clear_output(wait=True)\n",
    "        plt.plot(np.linspace(-10,10,100),\n",
    "                 np.linspace(-10,10,100)**2 + 5*np.linspace(-10,10,100) + 2)\n",
    "        plt.scatter([x],[x**2+5*x+2], c=\"red\")\n",
    "        plt.show()\n",
    "\n",
    "button.on_click(on_button_clicked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c91011-8f67-499f-b8cd-dade756a11a2",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "Try out the previous methods by minimizing these functions:\n",
    "* $x^2 + x$ wrt. $x$.\n",
    "* $x^4 -3x^2 +2$ wrt. $x$.\n",
    "* $x^2 + y^2$ wrt. $x$ and $y$.\n",
    "\n",
    "Try maximizing these functions:\n",
    "* $-x^2 + 2 $\n",
    "* $x^2 -x^4$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-DL",
   "language": "python",
   "name": "venv-dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
