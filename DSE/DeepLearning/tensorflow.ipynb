{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae67d3b-e35e-48b8-8f42-db3db6fe4be7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Preliminaries.\n",
    "\n",
    "Before talking about deep learning and deep learning framework. Let's establish a bit of notation. From here on, we will talk about optimizing models. Think of a model as a set of functions. For example, a model could be a family of functions described as \n",
    "$$F(x; w) = x + w \\text{ with } x,w \\in \\mathbb{R}$$\n",
    "$$\\text{or equivantly } F(x; w) = \\{f:\\mathbb{R}\\rightarrow\\mathbb{R}\\texttt{ }\\vert\\texttt{ }f(x) = x + w, w \\in \\mathbb{R}\\}$$\n",
    "\n",
    "However, we will use the first notation. Now, let's make a very important distinction. We call the arguments before \";\" **inputs** (such is $x$). Instead, we call the argument after \";\" **parameters** (such is $w$).\n",
    "\n",
    "In python a model could look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "192e7c25-9b24-402b-baca-04ea1f0fa3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F returns functions. For example F(5) = <function F.<locals>.<lambda> at 0x7f88885ec4c0>\n",
      "if F returns functions we can call F(5). For example, F(5)(5) = 10\n"
     ]
    }
   ],
   "source": [
    "def F(w):\n",
    "    return lambda x: x + w\n",
    "\n",
    "print(f\"F returns functions. For example F(5) = {F(5)}\")\n",
    "print(f\"if F returns functions we can call F(5). For example, F(5)(5) = {F(5)(5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633ff2cc-d1a8-4dbb-a476-394d7da7bdb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 Training/Optimizing\n",
    "\n",
    "What does it mean to train or optimize a model? It means searching among the family of functions for those who satisfy desired properties. For example, we could want to search for function that are symmetric. Or we could search for functions that interpolate a set of points. Ultimately, this amount to find good parameters. You can search for good parameters any ways you desire. You can even randomly sample parameters until you find good ones.\n",
    "\n",
    "## 1.2 Optimize a constant function.\n",
    "\n",
    "Let's make a concrete, yet super simple example. This should help you to understand how all blocks fall together. So consider the following model: \n",
    "\n",
    "$$F(;w_1) = w_1^2 + 1$$ \n",
    "\n",
    "Where $w_1 \\in [-10,10]$. $F$ is a model which can be instantiated in infinitely many functions. Each one of these functions has no input and constant real output.  This function is not particularly useful to optimize but it can help you to understand the methodology. \n",
    "\n",
    "Now suppose that we want to find in $F$ the function with lowest output. For example, if we set $w_1 = 5$ we get $g(x) = 5^2+1 = 26$ which is not very good. Instead, if we set $w_1 = 2$ we get $s(x)=2^2+1=5$ which is already better. Let's formalize wath we want:\n",
    "\n",
    "$$\\min_{w \\in [-10,10]}\\{F(;w_1)\\} = \\min_{w \\in [-10, 10]}\\{w_1^2 + 1\\}$$\n",
    "\n",
    "### 1.1.1 Zeroth Order Optimization (or derivative-free optimization)\n",
    "Zeroth order optimization tries to optimize models without knowledge of gradients. 0th order optimization is useful when you cannot compute gradients of the model (it may require too much time, too much memory, or you do not know the model at all). These methods are often called black-box optimization, as they require only to be able to query the model. One common kind of zeroth order algorithms are evolutionary ones.\n",
    "\n",
    "Let's see a super simple example of Zeroth Order optimization that randomly search for the optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c799e7c8-6c90-4ee4-8c25-ef55eb3367d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter found w1=0.003616493625024475 which yield f(w1)=1.00001307902614\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# let's define our model as before\n",
    "def F(w1):\n",
    "    return lambda: w1**2 + 1\n",
    "    \n",
    "# let's search for good functions by randomly sampling w1.\n",
    "best_score = float(\"+inf\")\n",
    "best_param = None\n",
    "for i in range(10000):\n",
    "    w1 = random.uniform(-10, 10)\n",
    "    if F(w1)() < best_score:\n",
    "        best_score, best_param = F(w1)(), w1\n",
    "        \n",
    "print(f\"best parameter found w1={best_param} which yield f(w1)={F(best_param)()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0e0e83-ad10-4f4e-8f2b-99d74ff31d5b",
   "metadata": {},
   "source": [
    "Which is pretty close to the optimal value ($1$). Unfortunately, randomly searching works only on super simple cases. If we want to tackle real world problems we need to searching more intelligently. \n",
    "\n",
    "### 1.1.2 First Order Optimization\n",
    "\n",
    "First-order optimization algorithms require the knowledge of the derivative of $F$. The most used first-order technique is gradient descent. With respect to zeroth-order algorithms, we have more knowledge so usually, we can obtain good results faster. However, computing gradients does require time and memory. Let's compute the derivative of $F$ wrt. $w_1$.\n",
    "\n",
    "$$\\frac{dF(;w_1)}{dw_1} = \\frac{d w_1^2 + 1}{d w_1} = 2w_1$$\n",
    "\n",
    "Now, no matter how we choose $w1$, we can compute the slope of the model. By knowing the slope, we can make small steps towards smaller and smaller functions. Let me visualize a little bit better what I mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1915c9c5-e53f-4628-b950-90b09b3fc95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67a7c2439ba4e85be58939c63b64070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='w1', max=10.0, min=-10.0, step=0.25), Output()), _do…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib widget\n",
    "%matplotlib inline\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@widgets.interact(w1=(-10, 10, 0.25))\n",
    "def update(w1 = 1.0):\n",
    "    w = np.linspace(-10, 10,1000)\n",
    "    y = F(w)() \n",
    "    plt.plot(w, y)\n",
    "    \n",
    "    plt.scatter(w1, F(w1)())\n",
    "    \n",
    "    x = np.linspace(-10,10)\n",
    "    y = 2*x*w1 - F(w1)() + 2\n",
    "    plt.plot(x,y)\n",
    "    \n",
    "    plt.gca().set_xlim((-5,+5))\n",
    "    plt.gca().set_ylim((-2,+10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b7917-4d93-4adc-b732-d10df46559e7",
   "metadata": {},
   "source": [
    "The blue line is our model. Remember, each point of our model is actually a function, a constant function. For each of this point we can compute a derivative, the derivative tells us the slope of model. By following the slope we can obtain a function with higher value. By following the negative slope, we can obtain a function with lower value. Step by step, we can obtain incresingly bigger functions of icreasingly smaller functions.   \n",
    "\n",
    "Just for fun, let's write an algorithm that performs the gradient descent of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b283019-831d-44f3-a5c9-acf7a645286b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter found w1=1.3722884804228272e-08 which yield f(w1)=1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def F     (w1): return lambda: w1**2 + 1\n",
    "def dF_dw1(w1): return 2*w1\n",
    "\n",
    "param_w1 = random.uniform(-10,10)\n",
    "learning_rate = 0.001\n",
    "\n",
    "for i in range(10000):\n",
    "    param_w1 = param_w1 - learning_rate * dF_dw1(param_w1) # compute a small step in the direction of the slope\n",
    "\n",
    "print(f\"best parameter found w1={param_w1} which yield f(w1)={F(param_w1)()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f20d9-3560-4e79-ad4d-1f5cfc92ea84",
   "metadata": {},
   "source": [
    "### 1.1.3 Second Order Optimization\n",
    "\n",
    "We could go a step beyond and use even the second derivative. There are a lot of reasons to use and to not use the second derivative. However, we will not cover these optimization methods.\n",
    "\n",
    "## 1.2 Optimize a simple function\n",
    "\n",
    "Now that we have seen how to optimize a model that produced constant functions, we can now try to optimize a model that describes more interesting functions. Consider this model:\n",
    "\n",
    "$$ F(x;w_1, w_2) = x*w_1 + w_2 $$\n",
    "\n",
    "Firstly, let's see what kinds of functions does our $F$ describes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b646947e-7c4c-40ad-b27d-ae1fef12ccc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02e2b77806343a8a0358176b97d8406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=7.5, description='w1', max=10.0, min=-10.0, step=0.25), FloatSlider(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def F(w1, w2):\n",
    "    return lambda x: x*w1+w2\n",
    "\n",
    "@widgets.interact(w1=(-10, 10, 0.25), w2=(-10, 10, 0.25))\n",
    "def plot(w1=7.5, w2=7.5):\n",
    "    x = np.linspace(-10, 10)\n",
    "    y = F(w1, w2)(x)\n",
    "    plt.plot(x, y)\n",
    "    plt.grid()\n",
    "    plt.gca().set_xlim((-5,+5))\n",
    "    plt.gca().set_ylim((-10,+10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45838325-4942-49a7-9340-d0e68914b185",
   "metadata": {},
   "source": [
    "These are all straight lines. No matter which $w_1$ and $w_2$ you will chose. $f$ will always describe a straight line. Let's suppose that we want to find the function with these input-output relation:\n",
    "\n",
    "| x | y |\n",
    "|---|---|\n",
    "| 1 | 2 |\n",
    "| 2 | 3 |\n",
    "| 3 | 4 |\n",
    "| 4 | 5 |\n",
    "\n",
    "A function from our model is good when fed with $x$s it outputs something close to the described $y$s. Let's define a bit more formally what does this means. So, we derive yet another model from F, which we call L.\n",
    "\n",
    "$$ L(x; w_1, w_2) = \\sum_{(x,y) \\in D} (F(x; w1, w2) - y)^2 $$\n",
    "\n",
    "$L$ is close to $0$ when $F(x_i; w1, w2)$ is similar to $y_i$, for all $i$. In fact $L$ is exactly $0$ when $F(x_i; w1, w2) = y_i$, for all $i$. So, optimizing this model yields exactly the property that we desire. Again, we need to compute the gradients.\n",
    "\n",
    "Now we need the derivative wrt. $w_1$: \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L(x;w_1,w_2)}{\\partial w_1} = & \\frac{\\partial \\sum_{(x,y) \\in D} (F(x; w1, w2) - y)^2}{d w_1}  \\\\\n",
    "= & \\sum_{(x,y) \\in D} \\frac{\\partial(F(x; w1, w2) - y)^2}{\\partial w_1} \\\\\n",
    "= & \\sum_{(x,y) \\in D} \\frac{\\partial(x*w_1+w_2 - y)^2}{\\partial w_1}  \\\\\n",
    "= & \\sum_{(x,y) \\in D} 2(x*w_1+w_2 - y) \\frac{\\partial x*w_1+w_2 - y}{\\partial w_1}  \\\\\n",
    "= & \\sum_{(x,y) \\in D} 2(x*w_1+w_2 - y)x\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And also wrt. $w_2$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L(x;w_1,w_2)}{\\partial w_2} = & \\frac{d \\sum_{(x,y) \\in D} (F(x; w1, w2) - y)^2}{\\partial w_2} \\\\\n",
    "= & \\sum_{(x,y) \\in D} \\frac{\\partial(F(x; w1, w2) - y)^2}{\\partial w_2} \\\\\n",
    "= & \\sum_{(x,y) \\in D} \\frac{\\partial(x*w_1+w_2 - y)^2}{\\partial w_2} \\\\\n",
    "= & \\sum_{(x,y) \\in D} 2(x*w_1+w_2 - y) \\frac{dx*w_1+w_2 - y}{\\partial w_2} \\\\\n",
    "= & \\sum_{(x,y) \\in D} 2(x*w_1+w_2 - y)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now, we have again all the pieces to perform our first order optimization, like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c176299-183c-489b-aa03-f3da009ccf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter found w1=(0.9999999999999668, 0.9999999999999668) which yield f(2;w1,w2)=3.00000000000003\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "x = np.array([1,2,3,4])\n",
    "y = np.array([2,3,4,5])\n",
    "\n",
    "def F(w1, w2):\n",
    "    return lambda x: x*w1+w2\n",
    "\n",
    "def dF_dw1(w1, w2):\n",
    "    return (2*(x * w1 + w2 - y)*x).sum()\n",
    "\n",
    "def dF_dw2(w1, w2):\n",
    "    return (2*(x * w1 + w2 - y)).sum()\n",
    "\n",
    "param_w1 = random.uniform(-10,10)\n",
    "param_w2 = random.uniform(-10,10)\n",
    "learning_rate = 0.001\n",
    "\n",
    "for i in range(100000):\n",
    "    param_w1, param_w2 = (param_w1 - learning_rate * dF_dw1(param_w1, param_w2), \n",
    "                          param_w2 - learning_rate * dF_dw2(param_w1, param_w2))\n",
    "\n",
    "print(f\"best parameter found w1={param_w1, param_w1} which yield f(2;w1,w2)={F(param_w1, param_w2)(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d5980a-a23e-48e2-96d2-95f5172eac5e",
   "metadata": {},
   "source": [
    "# 2. Automatic Differentiation.\n",
    "\n",
    "As you have noticed, computing the the gradients by hand is a real pain, and it becomes unmanageble real fast. Fortunately, this work can be done automatically. Again there are many ways to perform differentiation. Let's review few of them. First let's define the gradient of a function $f(x_1, x_2, \\dots, x_n)$\n",
    "\n",
    "$$\\nabla f(x_1, x_2, \\dots, x_n) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1}(x_1, x_2, \\dots, x_n) \\\\\n",
    "\\frac{\\partial f}{\\partial x_2}(x_1, x_2, \\dots, x_n) \\\\\n",
    "\\dots \\\\\n",
    "\\frac{\\partial f}{\\partial x_n}(x_1, x_2, \\dots, x_n)\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "There are two main way to compute $\\nabla f(x_1, x_2, \\dots, x_n)$.\n",
    "* **Numerical differentiation**. We are only interested in computing the value of the gradient for a particular point in the domain of $f$.\n",
    "* **Symbolic differentiation**. We are interested in the formula representing the gradients for $f$ for each point in the domain of $f$. \n",
    "\n",
    "There are several technique to compute both Numerical differentiation and Symbolic differentiation. Let's review a very simple ones. \n",
    "\n",
    "# 2.1. Numerical Differentiation.\n",
    "\n",
    "To get an approximation of the derivative of a function $f$ in a point $(x_0, x_1, \\dots, x_n)$, we can use the definition: \n",
    "\n",
    "$$\\nabla f(x_0, x_1, \\dots, x_n) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1}(x_1, x_2, \\dots, x_n) \\\\\n",
    "\\frac{\\partial f}{\\partial x_2}(x_1, x_2, \\dots, x_n) \\\\\n",
    "\\dots \\\\\n",
    "\\frac{\\partial f}{\\partial x_n}(x_1, x_2, \\dots, x_n)\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\lim_{\\epsilon \\rightarrow 0} \\frac{f(x_0+\\epsilon,x_1,\\dots,x_n)-f(x_1, x_2, \\dots, x_n)}{\\epsilon} \\\\\n",
    "\\lim_{\\epsilon \\rightarrow 0} \\frac{f(x_0,x_1+\\epsilon,\\dots,x_n)-f(x_1, x_2, \\dots, x_n)}{\\epsilon} \\\\\n",
    "\\dots \\\\\n",
    "\\lim_{\\epsilon \\rightarrow 0} \\frac{f(x_0,x_1,\\dots,x_n+\\epsilon)-f(x_1, x_2, \\dots, x_n)}{\\epsilon}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e9c16a8-2627-47b0-ba20-be0d7ab390ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b75fc68ffc40edb06c45e59e398bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='x0', max=2.0, min=-2.0, step=0.001), Output()), _dom…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(x): return x**3\n",
    "def dfdx(f,eps=0.001): return lambda x:(f(x+eps) - f(x))/(eps)\n",
    "\n",
    "@widgets.interact(x0=(-2, 2, 0.001))\n",
    "def plot(x0=1):\n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    y = f(x)\n",
    "    plt.plot(x, y)\n",
    "    \n",
    "    plt.scatter(x0, f(x0))\n",
    "    \n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    y = (x-x0)*dfdx(f)(x0) + f(x0)\n",
    "    y1 = 3*x**2\n",
    "    plt.plot(x, y)\n",
    "    \n",
    "    plt.gca().set_xlim((-5,+5))\n",
    "    plt.gca().set_ylim((-10,+10))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118301e0-65cf-46e5-9064-a5d99c8d0eaa",
   "metadata": {},
   "source": [
    "This method is perfectly fine and usable. However, it requires computing $f$ as many times as the number or its arguments. Computing $f$ this way may require too much time.\n",
    "\n",
    "## 2.2. Symbolic Differentiation\n",
    "\n",
    "Instead of recomputing $f$ with a small changes, we can compute the gradient symbolically ones. We can use the symbolic formula to obtain the numerical gradients in any point of the domain. Moreover, we can also apply optimizations to the symbolic representation of the gradients. But first, we need to talk about how we represent functions. Functions such as \n",
    "$$f(x) = x^2 + 5x + 2$$ \n",
    "which may look complex, are actually a composition of simpler operations. In this case, there are only three operations: exponentiation ($pow(x,y)=x^y$), multiplication ($mul(x,y)=x*y$) and summation ($sum(x,y)=x+y$). Therefore:\n",
    "\n",
    "$$f(x) = x^2 + 5x + 2 = sum(sum(pow(x,2),mul(5,x)),2)$$ \n",
    "\n",
    "Now, it should be more clear that we can represent $f(x)$ as a tree:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae55878-5d75-4e7b-9189-4e1d3332e711",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, we know how to represent complex composition of functions... as a tree.\n",
    "\n",
    "### 2.2.1 Chain Rule.\n",
    "\n",
    "Before stepping into the heart of symbolic automatic differentiation, let's see how you can differentiate a composition of functions. Let $f(x) = g(h(x))$ then,  \n",
    "\n",
    "$$\\frac{df(x)}{dx} = \\frac{dg(h(x))}{dx} = \\frac{dg}{dh}\\frac{dh}{dx}$$\n",
    "\n",
    "Or in the multivariate case. Let $f(x) = g(h_1(x),\\dots,h_n(x))$ \n",
    "\n",
    "$$\\frac{df(x)}{dx} = \n",
    "\\frac{g(h_1(x),\\dots,h_n(x))}{dx} = \n",
    "\\frac{\\partial g}{\\partial h_1}\\frac{dh_1}{dx} + \\dots + \\frac{\\partial g}{\\partial h_n}\\frac{dh_n}{dx}$$\n",
    "\n",
    "#### 2.2.1.1 Example 1.\n",
    "\n",
    "$$\\frac{d sum(f(x),g(x))}{dx} = \n",
    "\\frac{\\partial sum}{\\partial f}\\frac{df}{dx} + \\frac{\\partial sum}{\\partial g}\\frac{dg}{dx} = \n",
    "\\frac{df}{dx} + \\frac{dg}{dx}$$\n",
    "\n",
    "#### 2.2.1.2 Example 2.\n",
    "\n",
    "$$\\frac{d mul(f(x),g(x))}{dx} = \n",
    "\\frac{\\partial mul}{\\partial f}\\frac{df}{dx} + \\frac{\\partial mul}{\\partial g}\\frac{dg}{dx} = \n",
    "g\\frac{df}{dx} + f\\frac{dg}{dx}$$\n",
    "\n",
    "#### 2.2.1.3 Example 3.\n",
    "\n",
    "$$\\frac{d pow(f(x),g(x))}{dx} = \n",
    "\\frac{\\partial pow}{\\partial f}\\frac{df}{dx} + \\frac{\\partial pow}{\\partial g}\\frac{dg}{dx} = \n",
    "gf^{g-1}\\frac{df}{dx} + log(f)f^g\\frac{dg}{dx}$$\n",
    "\n",
    "Now, we have the tools to differentiate the function mentioned before (the one in the tree).\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{d sum(sum(pow(x,2),mul(5,x)),2)}{dx} = & \\\\ \n",
    "\\frac{d sum(pow(x,2),mul(5,x))}{dx} + \\frac{d2}{dx} = &\\\\\n",
    "\\frac{d pow(x,2)}{dx} + \\frac{d mul(5,x)}{dx} = &\\\\\n",
    "2x^{2-1}\\frac{dx}{dx} + log(x)x^2\\frac{d2}{dx} + x\\frac{d5}{dx} + 5\\frac{dx}{dx} = &\\\\\n",
    "2x + 5\n",
    "\\end{aligned}\n",
    "$$ \n",
    "\n",
    "Ok, that was quite a bit of work. At least, it was fairly mechanical. Note that, we only used the knowledge of the derivative of $sum$, $sum$, and $pow$. Given these three pieces, we can derivate any composition of $sum$, $mul$, and $pow$. You can write a differentiation engine in very few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e7f0cfb-959c-4a77-9198-25ed2c056e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sum:\n",
    "    def __init__(self, f, g): \n",
    "        self.f, self.g = f, g\n",
    "    def diff(self, var):\n",
    "        return f\"({self.f.diff(var)} + {self.g.diff(var)})\"\n",
    "    def tostr(self):\n",
    "        return f\"({self.f.tostr()} + {self.g.tostr()})\"\n",
    "        \n",
    "class mul:\n",
    "    def __init__(self, f, g):\n",
    "        self.f, self.g = f, g\n",
    "    def diff(self, var):\n",
    "        return f\"({self.f.tostr()} * {self.g.diff(var)} + {self.g.tostr()} * {self.f.diff(var)})\"\n",
    "    def tostr(self):\n",
    "        return f\"({self.f.tostr()} * {self.g.tostr()})\"\n",
    "                   \n",
    "class pow:\n",
    "    def __init__(self, f, g):\n",
    "        self.f, self.g = f, g\n",
    "    def diff(self, var):\n",
    "        return f\"({self.g.tostr()} * {self.f.tostr()} ^ ({self.g.tostr()}-1))({self.f.diff(var)}) + (log({self.f.tostr()}) * {self.f.tostr()} ^ {self.g.tostr()})({self.g.diff(var)})\"\n",
    "    def tostr(self):\n",
    "        return f\"({self.f.tostr()}) ^ ({self.g.tostr()})\"\n",
    "                   \n",
    "class var:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    def diff(self, var):\n",
    "        return \"1\" if self.name == var.name else \"0\"\n",
    "    def tostr(self):\n",
    "        return self.name\n",
    "                   \n",
    "class const:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def diff(self, var):\n",
    "        return \"0\"\n",
    "    def tostr(self):\n",
    "        return self.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "280a5926-1fce-44d0-9cbb-bcfe10fb35f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     f(x) = (((x) ^ (2) + (5 * x)) + 2)\n",
      "d/dx f(x) = (((2 * x ^ (2-1))(1) + (log(x) * x ^ 2)(0) + (5 * 1 + x * 0)) + 0)\n"
     ]
    }
   ],
   "source": [
    "x    = var(\"x\")\n",
    "two  = const(\"2\")\n",
    "five = const(\"5\")\n",
    "\n",
    "f = sum(sum(pow(x, two), mul(five, x)), const(2))\n",
    "\n",
    "print(\"     f(x) =\", f.tostr())\n",
    "print(\"d/dx f(x) =\", f.diff(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "465b19c2-47bf-4fe0-8c53-11dbfdef2fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     f(x,y) = (((x) ^ (2) + (5 * y)) + 2)\n",
      "d/dx f(x,y) = (((2 * x ^ (2-1))(1) + (log(x) * x ^ 2)(0) + (5 * 0 + y * 0)) + 0)\n",
      "d/dy f(x,y) = (((2 * x ^ (2-1))(0) + (log(x) * x ^ 2)(0) + (5 * 1 + y * 0)) + 0)\n"
     ]
    }
   ],
   "source": [
    "x    = var(\"x\")\n",
    "y    = var(\"y\")\n",
    "two  = const(\"2\")\n",
    "five = const(\"5\")\n",
    "\n",
    "f = sum(sum(pow(x, two), mul(five, y)), const(2))\n",
    "print(\"     f(x,y) =\", f.tostr())\n",
    "print(\"d/dx f(x,y) =\", f.diff(x))\n",
    "print(\"d/dy f(x,y) =\", f.diff(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd448033-d0ed-47a7-95a1-97f9d437002c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m w1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(\u001b[38;5;241m10.0\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "w1 = tf.Variable(10.0, name=\"x\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  f = w1**2 + 1\n",
    "\n",
    "df_dx = tape.gradient(f, w1)\n",
    "\n",
    "print(df_dx.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d1960-a96b-461a-b774-8838ec7875db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-DL",
   "language": "python",
   "name": "venv-dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
