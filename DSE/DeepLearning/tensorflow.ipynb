{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae67d3b-e35e-48b8-8f42-db3db6fe4be7",
   "metadata": {},
   "source": [
    "# 1. Preliminaries.\n",
    "\n",
    "Before talking about deep learning and deep learning framework. Let's establish a bit of notation. From here on, we will talk about optimizing models. Think of a model as a set of functions. For example, a model could be a family of functions described as \n",
    "$$F(x; w) = x + w \\text{ with } x,w \\in \\mathbb{R}$$\n",
    "$$\\text{or equivantly } F(x; w) = \\{f:\\mathbb{R}\\rightarrow\\mathbb{R}\\texttt{ }\\vert\\texttt{ }f(x) = x + w, w \\in \\mathbb{R}\\}$$\n",
    "\n",
    "However, we will use the first notation. Now, let's make a very important distinction. We call the arguments before \";\" **inputs** (such is $x$). Instead, we call the argument after \";\" **parameters** (such is $w$).\n",
    "\n",
    "In python a model could look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "192e7c25-9b24-402b-baca-04ea1f0fa3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F returns functions. For example F(5) = <function F.<locals>.<lambda> at 0x7fdfae908160>\n",
      "if F returns functions we can call F(5). For example, F(5)(5) = 10\n"
     ]
    }
   ],
   "source": [
    "def F(w):\n",
    "    return lambda x: x + w\n",
    "\n",
    "print(f\"F returns functions. For example F(5) = {F(5)}\")\n",
    "print(f\"if F returns functions we can call F(5). For example, F(5)(5) = {F(5)(5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633ff2cc-d1a8-4dbb-a476-394d7da7bdb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 Training/Optimizing\n",
    "\n",
    "What does it mean to train or optimize a model? It means searching among the family of functions for those who satisfy desired properties. For example, we could want to search for function that are symmetric. Or we could search for functions that interpolate a set of points. Ultimately, this amount to find good parameters. You can search for good parameters any ways you desire. You can even randomly sample parameters until you find good ones.\n",
    "\n",
    "## 1.2 Optimize a constant function.\n",
    "\n",
    "Let's make a concrete, yet super simple example. This should help you to understand how all blocks fall together. So consider the following model: \n",
    "\n",
    "$$F(;w_1) = w_1^2 + 1$$ \n",
    "\n",
    "Where $w_1 \\in [-10,10]$. $F$ is a model which can be instantiated in infinitely many functions. Each one of these functions has no input and constant real output.  This function is not particularly useful to optimize but it can help you to understand the methodology. \n",
    "\n",
    "Now suppose that we want to find in $F$ the function with lowest output. For example, if we set $w_1 = 5$ we get $g(x) = 5^2+1 = 26$ which is not very good. Instead, if we set $w_1 = 2$ we get $s(x)=2^2+1=5$ which is already better. Let's formalize wath we want:\n",
    "\n",
    "$$\\min_{w \\in [-10,10]}\\{F(;w_1)\\} = \\min_{w \\in [-10, 10]}\\{w_1^2 + 1\\}$$\n",
    "\n",
    "### 1.1.1 Zeroth Order Optimization (or derivative-free optimization)\n",
    "Zeroth order optimization tries to optimize models without knowledge of gradients. 0th order optimization is useful when you cannot compute gradients of the model (it may require too much time, too much memory, or you do not know the model at all). These methods are often called black-box optimization, as they require only to be able to query the model. One common kind of zeroth order algorithms are evolutionary ones.\n",
    "\n",
    "Let's see a super simple example of Zeroth Order optimization that randomly search for the optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c799e7c8-6c90-4ee4-8c25-ef55eb3367d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter found w1=0.00020506493425997974 which yield f(w1)=1.0000000420516273\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# let's define our model as before\n",
    "def F(w1):\n",
    "    return lambda: w1**2 + 1\n",
    "    \n",
    "# let's search for good functions by randomly sampling w1.\n",
    "best_score = float(\"+inf\")\n",
    "best_param = None\n",
    "for i in range(10000):\n",
    "    w1 = random.uniform(-10, 10)\n",
    "    if F(w1)() < best_score:\n",
    "        best_score, best_param = F(w1)(), w1\n",
    "        \n",
    "print(f\"best parameter found w1={best_param} which yield f(w1)={F(best_param)()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0e0e83-ad10-4f4e-8f2b-99d74ff31d5b",
   "metadata": {},
   "source": [
    "Which is pretty close to the optimal value ($1$). Unfortunately, randomly searching works only on super simple cases. If we want to tackle real world problems we need to searching more intelligently. \n",
    "\n",
    "### 1.1.2 First Order Optimization\n",
    "\n",
    "First-order optimization algorithms require the knowledge of the derivative of $F$. The most used first-order technique is gradient descent. With respect to zeroth-order algorithms, we have more knowledge so usually, we can obtain good results faster. However, computing gradients does require time and memory. Let's compute the derivative of $F$ wrt. $w_1$.\n",
    "\n",
    "$$\\frac{dF(;w_1)}{dw_1} = \\frac{d w_1^2 + 1}{d w_1} = 2w_1$$\n",
    "\n",
    "Now, no matter how we choose $w1$, we can compute the slope of the model. By knowing the slope, we can make small steps towards smaller and smaller functions. Let me visualize a little bit better what I mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1915c9c5-e53f-4628-b950-90b09b3fc95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332e8dac3aae4bfd92eca76c8e230149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='w1', max=10.0, min=-10.0, step=0.25), Output()), _do…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "%matplotlib inline\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@widgets.interact(w1=(-10, 10, 0.25))\n",
    "def update(w1 = 1.0):\n",
    "    w = np.linspace(-10, 10)\n",
    "    y = F(w)() \n",
    "    plt.plot(w, y)\n",
    "    \n",
    "    plt.scatter(w1, F(w1)())\n",
    "    \n",
    "    x = np.linspace(-10,10)\n",
    "    y = 2*x*w1 - F(w1)() +2\n",
    "    plt.plot(x,y)\n",
    "    \n",
    "    plt.gca().set_xlim((-5,+5))\n",
    "    plt.gca().set_ylim((-2,+10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b7917-4d93-4adc-b732-d10df46559e7",
   "metadata": {},
   "source": [
    "The blue line is our model. Remember, each point of our model is actually a function, a constant function. For each of this point we can compute a derivative, the derivative tells us the slope of model. By following the slope we can obtain a function with higher value. By following the negative slope, we can obtain a function with lower value. Step by step, we can obtain incresingly bigger functions of icreasingly smaller functions.   \n",
    "\n",
    "Just for fun, let's write an algorithm that performs the gradient descent of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b283019-831d-44f3-a5c9-acf7a645286b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter found w1=-9.375544264783508e-09 which yield f(w1)=1.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def F     (w1): return lambda: w1**2 + 1\n",
    "def dF_dw1(w1): return 2*w1\n",
    "\n",
    "param_w1 = random.uniform(-10,10)\n",
    "learning_rate = 0.001\n",
    "\n",
    "for i in range(10000):\n",
    "    param_w1 = param_w1 - learning_rate * dF_dw1(param_w1) # compute a small step in the direction of the slope\n",
    "\n",
    "print(f\"best parameter found w1={param_w1} which yield f(w1)={F(param_w1)()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f20d9-3560-4e79-ad4d-1f5cfc92ea84",
   "metadata": {},
   "source": [
    "### 1.1.3 Second Order Optimization\n",
    "\n",
    "We could go a step beyond and use even the second derivative. There are a lot of reasons to use and to not use the second derivative. However, we will not cover these optimization methods.\n",
    "\n",
    "## 1.2 Optimize a simple function\n",
    "\n",
    "Now that we have seen how to optimize a model that produced constant functions, we can now try to optimize a model that describes more interesting functions. Consider this model:\n",
    "\n",
    "$$ F(x;w_1, w_2) = x*w_1 + w_2 $$\n",
    "\n",
    "Firstly, let's see what kinds of functions does our $F$ describes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b646947e-7c4c-40ad-b27d-ae1fef12ccc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7150f9317844ccb18951b7c3402c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=7.5, description='w1', max=10.0, min=-10.0, step=0.25), FloatSlider(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def F(w1, w2):\n",
    "    return lambda x: x*w1+w2\n",
    "\n",
    "@widgets.interact(w1=(-10, 10, 0.25), w2=(-10, 10, 0.25))\n",
    "def plot(w1=7.5, w2=7.5):\n",
    "    x = np.linspace(-10, 10)\n",
    "    y = F(w1, w2)(x)\n",
    "    plt.plot(x, y)\n",
    "    \n",
    "    plt.gca().set_xlim((-5,+5))\n",
    "    plt.gca().set_ylim((-10,+10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45838325-4942-49a7-9340-d0e68914b185",
   "metadata": {},
   "source": [
    "These are all straight lines. No matter which $w_1$ and $w_2$ you will chose. $f$ will always describe a straight line. Let's suppose that we want to find the function with these input-output relation:\n",
    "\n",
    "| x | y |\n",
    "|---|---|\n",
    "| 1 | 2 |\n",
    "| 2 | 3 |\n",
    "| 3 | 4 |\n",
    "| 4 | 5 |\n",
    "\n",
    "A function from our model is good when fed with $x$s it outputs something close to the described $y$s. Let's define a bit more formally what does this means. So, we derive yet another model from F, which we call L.\n",
    "\n",
    "$$ L(x; w_1, w_2) = \\sum_{(x,y) \\in D} (F(x; w1, w2) - y)^2 $$\n",
    "\n",
    "$L$ is close to $0$ when $F(x_i; w1, w2)$ is similar to $y_i$, for all $i$. In fact $L$ is exactly $0$ when $F(x_i; w1, w2) = y_i$, for all $i$. So, optimizing this model yields exactly the property that we desire. Again, we need to compute the gradients.\n",
    "\n",
    "Now we need the derivative wrt. $w_1$: \n",
    "$$\\frac{d L(x;w_1,w_2)}{d w_1} = \n",
    "\\frac{d \\sum_{(x,y) \\in D} (F(x; w1, w2) - y)^2}{d w_1} = \n",
    "\\sum_{(x,y) \\in D} \\frac{d(F(x; w1, w2) - y)^2}{dw_1} =\n",
    "\\sum_{(x,y) \\in D} \\frac{d(x*w_1+w_2 - y)^2}{dw_1} = \n",
    "\\sum_{(x,y) \\in D} 2(x*w_1+w_2 - y) \\frac{dx*w_1+w_2 - y}{dw_1} = \n",
    "\\sum_{(x,y) \\in D} 2(x*w_1+w_2 - y)x\n",
    "$$\n",
    "\n",
    "And also wrt. $w_2$:\n",
    "\n",
    "$$\\frac{d L(x;w_1,w_2)}{d w_2} = \n",
    "\\frac{d \\sum_{(x,y) \\in D} (F(x; w1, w2) - y)^2}{d w_2} = \n",
    "\\sum_{(x,y) \\in D} \\frac{d(F(x; w1, w2) - y)^2}{dw_2} =\n",
    "\\sum_{(x,y) \\in D} \\frac{d(x*w_1+w_2 - y)^2}{dw_2} = \n",
    "\\sum_{(x,y) \\in D} 2(x*w_1+w_2 - y) \\frac{dx*w_1+w_2 - y}{dw_2} = \n",
    "\\sum_{(x,y) \\in D} 2(x*w_1+w_2 - y)\n",
    "$$\n",
    "\n",
    "Now, we have again all the pieces to perform our first order optimization, like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c176299-183c-489b-aa03-f3da009ccf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter found w1=(1.0000000000000249, 1.0000000000000249) which yield f(2;w1,w2)=2.999999999999981\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "x = np.array([1,2,3,4])\n",
    "y = np.array([2,3,4,5])\n",
    "\n",
    "def F(w1, w2):\n",
    "    return lambda x: x*w1+w2\n",
    "\n",
    "def dF_dw1(w1, w2):\n",
    "    return (2*(x * w1 + w2 - y)*x).sum()\n",
    "\n",
    "def dF_dw2(w1, w2):\n",
    "    return (2*(x * w1 + w2 - y)).sum()\n",
    "\n",
    "param_w1 = random.uniform(-10,10)\n",
    "param_w2 = random.uniform(-10,10)\n",
    "learning_rate = 0.001\n",
    "\n",
    "for i in range(100000):\n",
    "    param_w1, param_w2 = (param_w1 - learning_rate * dF_dw1(param_w1, param_w2), \n",
    "                          param_w2 - learning_rate * dF_dw2(param_w1, param_w2))\n",
    "\n",
    "print(f\"best parameter found w1={param_w1, param_w1} which yield f(2;w1,w2)={F(param_w1, param_w2)(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d5980a-a23e-48e2-96d2-95f5172eac5e",
   "metadata": {},
   "source": [
    "# 2. Automatic Differentiation\n",
    "\n",
    "As you have noticed, computing the the gradients by hand is a real pain, and it becomes unmanageble real fast. Fortunately, this work can be done automatically.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd448033-d0ed-47a7-95a1-97f9d437002c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 18:15:07.044616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 18:15:07.148009: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-02-23 18:15:07.148031: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-02-23 18:15:07.148977: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "w1 = tf.Variable(10.0, name=\"x\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  f = w1**2 + 1\n",
    "\n",
    "df_dx = tape.gradient(f, w1)\n",
    "\n",
    "print(df_dx.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-DL",
   "language": "python",
   "name": "venv-dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
