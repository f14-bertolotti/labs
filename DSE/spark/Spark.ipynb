{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef531395-0f0f-4afd-b533-1a74690c6bfa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spark Tutorial\n",
    "_______________________\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Your data is split across several disks across several computers connected by a network. And, probably, all your data combined take so much space that they would never fit a single computer. On the other hand, you still need to process your data. Well, without proper infrastructure, you would need to track down each piece of information, process them separately, and combine the information together. On top of that, you will need to repeat this process many times, too many times. \n",
    "\n",
    "Luckily, the Hadoop Distributed File System (HDFS) solves most of these problems. It provides also MapReduce API to do the processing. However, the MapReduce API is still very low-level, and getting things done requires too much time.\n",
    "\n",
    "Here is where Spark comes into play. Spark provides high-level APIs for large-scale data processing. Spark can run over an existing HDFS or it can run in a standalone mode (which does not require setting up any HDFS). To be fair, Spark in standalone mode is not particularly useful but it provides an easy way to access its functionalities and test them. Morever, whatever code you write in standalone mode that runs on your laptop will work automatically on huge HDFS. \n",
    "______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd6e40-e75f-4152-93e5-b4dfe9637596",
   "metadata": {},
   "source": [
    "### Spark Context\n",
    "\n",
    "The [SparkContext] represents the entry point to access spark functionalities. \n",
    "\n",
    "In the Spark framework, there are two main actors: the driver and the executors. The driver has jobs that need to be run. The driver splits jobs into tasks. These tasks are submitted to executors. Once completed, results are sent back to the driver. \n",
    "\n",
    "We will run Spark in local mode, so that, we can avoid running a whole HDFS on our machine. We will focus mainly on the programming paradigm. However, it may be useful to know that platforms such as [DataBricks] do exist. They simplify a lot of the work necessary to set up a real cluster on which spark can run. \n",
    "\n",
    "In local mode you can access the Spark Web UI in http://localhost:4040.\n",
    "\n",
    "[SparkContext]: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.html?highlight=pyspark%20sparkcontext#pyspark.SparkContext\n",
    "[DataBricks]: https://databricks.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3902093-8c4c-4491-9fe0-3a3d0009a95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/19 12:43:39 WARN Utils: Your hostname, ataxia resolves to a loopback address: 127.0.1.1; using 192.168.1.91 instead (on interface enp4s0)\n",
      "22/02/19 12:43:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/f14/Devel/labs/DSE/spark/.venv/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/19 12:43:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.91:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe8583ec430>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1098a2-a3c0-44d9-b749-87aa527e8243",
   "metadata": {},
   "source": [
    "Regardless of whether you are familiar with python or not, these functions will help you a lot. [help] ```help(x)``` shows the documentation of ```x```. [type] ```type``` show a string representing the type of ```x```. [dir] ```dir(x)``` shows anything that is accessible inside of ```x```. You can find many more on the [built-in] documentation page.\n",
    "\n",
    "[help]:https://docs.python.org/3/library/functions.html#help\n",
    "[type]:https://docs.python.org/3/library/functions.html#type\n",
    "[dir]:https://docs.python.org/3/library/functions.html#dir[built-in]\n",
    "[built-in]:https://docs.python.org/3/library/functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afba3a22-3984-45b6-89fa-f05dd1b1feb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=test>\n",
      "<class 'pyspark.context.SparkContext'>\n"
     ]
    }
   ],
   "source": [
    "print(sc)\n",
    "print(type(sc))\n",
    "# help(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f9762d-c45d-4236-927f-02993d9dc6ab",
   "metadata": {},
   "source": [
    "### Resilient Distributed Dataset (RDD)\n",
    "\n",
    "[RDDs] are one of the main abstraction of Spark. They represent immutable elements distributed across different nodes.\n",
    "- **Resilient**: The system is able to recompute/recover missing or damaged partitions due to node failures.\n",
    "- **Distributed**: Data resides on multiple nodes in a cluster.\n",
    "- **Dataset**: Collection of data.\n",
    "- **Immutable**: Once created, they cannot change.\n",
    "- **Lazy evaluated**: Operations are performed only when necessary.\n",
    "- **Parallel**: Operations are performed parallely.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"http://spark-mooc.github.io/web-assets/images/partitions.png\" alt=\"drawing\" width=\"600\"/></div>\n",
    "\n",
    "An RDD can be created by calling SparkContextâ€™s [parallelize] method ```sc.parallelize()``` on an existing collection in your driver program. The elements of the collection are copied to form a distributed dataset. ```sc.parallelize``` takes two arguments:\n",
    "   1. The collection used to form the RDD.\n",
    "   2. the number of partitions to cut the dataset into. Spark tries to set the number of partitions automatically based on your cluster.\n",
    "\n",
    "[parallelize]: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.parallelize.html?highlight=parallelize\n",
    "[RDDs]:https://spark.apache.org/docs/latest/rdd-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bcb6824-3afa-4966-a5c4-39e8f27a3586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 100)\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "# Parallelize data using 8 partitions\n",
    "# This operation is a transformation of data into an RDD\n",
    "# Spark uses lazy evaluation, so no Spark jobs are run at this point\n",
    "data = range(100)\n",
    "rdd  = sc.parallelize(data, 4)\n",
    "\n",
    "print(data)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c44b7d82-87b8-4c0c-9ce0-2eea1ae8e66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd id: 1\n"
     ]
    }
   ],
   "source": [
    "# Each RDD gets a unique ID\n",
    "print('rdd id: {0}'.format(rdd.id()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7521a782-5698-415b-aade-5cb0f7c6da05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "My first rdd PythonRDD[1] at collect at /tmp/ipykernel_20377/3543828524.py:8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can name each newly created RDD using the setName() method\n",
    "rdd.setName('My first rdd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c9b566c-c2f8-4db9-8403-083c9af4b117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(4) My first rdd PythonRDD[1] at collect at /tmp/ipykernel_20377/3543828524.py:8 []\\n |  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274 []'\n"
     ]
    }
   ],
   "source": [
    "# Let's view the lineage (the set of transformations) of the RDD using toDebugString()\n",
    "print(rdd.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc65f1e-2a19-48f0-ab35-e4a219be4318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how many partitions the RDD will be split into by using the getNumPartitions()\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b946187-aca6-4558-ac60-ed7138fe7e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea362e0-8f19-4ab0-9c08-a61ca4e329ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Transformations vs Actions\n",
    "There are two types of operations that you can perform on an RDD: Transformations and Actions. \n",
    "- **Transformations**. Transformations are applied on RDDs and produce other RDDs. Additionally, Transformations are lazily evaluated, meaning that, they are not computed until an action is performed. Some common transformations are [map], and [filter].\n",
    "- **Actions**. Actions do not return RDDs anymore. Actions do set in motion the sequence of transformation required to produce the result. Once the computation is done you get the result as output. Some common actions are [collect], [count], [reduce], and [take].\n",
    "\n",
    "[map]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html?highlight=map\n",
    "[filter]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.filter.html?highlight=filter\n",
    "[reduce]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduce.html?highlight=reduce\n",
    "[collect]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.collect.html?highlight=collect\n",
    "[count]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.count.html?highlight=count\n",
    "[take]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.take.html?highlight=take\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8afe15-d38d-43da-875b-0d05158d3b01",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The map() Transformation\n",
    "```map(f)```, the most common Spark transformation: it applies a function ```f``` to each item in the dataset, and outputs the resulting dataset. When you run [map] on a dataset, a single stage of tasks is launched. A stage is a group of tasks that all perform the same computation, but on different input data. One task is launched for each partitition, as shown in the example below. A task is a unit of execution that runs on a single machine. When we run ```map(f)``` within a partition, a new task applies ```f``` to all of the entries in a particular partition, and outputs a new partition. In this example figure, the dataset is broken into four partitions (using three workers), so four ```map()``` tasks are launched.\n",
    "\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/tasks.png\" alt=\"drawing\" width=\"600\"/><img src=\"http://spark-mooc.github.io/web-assets/images/map.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[map]: https://spark.apache.org/docs/3.2.1/api/python/reference/api/pyspark.RDD.map.html?highlight=map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d38221cf-6316-4d30-83c4-b02d5cbafdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sub function to subtract 1\n",
    "def sub(value): return (value - 1)\n",
    "\n",
    "# let's apply this function three times.\n",
    "rdd2 = rdd.map(sub).map(sub).map(sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fe6dc7-d5a7-4758-af7e-9e0b7f627e70",
   "metadata": {},
   "source": [
    "We have applied ```sub()``` to ```rdd``` three times in a row. So, each element in ```rdd``` gets decremented three times of ```1```. However, no computation as yet started. As mentioned earlier, spark is lazily evaluated. This means, that only when we require certain operation to be done the whole computantion will actually start. Let's see one of these operation that force the computation to start, the [collect] action.\n",
    "\n",
    "You should feel a little bit of fear each time you call ```.collect()``` as it brings the data you requested on your machine memory. But what if you requested several GBs of data by mistake. Well, your machine may crash as the memory gets saturated and you may loose several our worth of work.\n",
    "\n",
    "[collect]: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.collect.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5d905c7-5297-4e1d-8184-ddb10e7ad2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96]\n"
     ]
    }
   ],
   "source": [
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16177199-23bc-4e92-b0a7-7b61b72846fd",
   "metadata": {},
   "source": [
    "### The Filter() Transformation\n",
    "The [filter] transformation is used, surprisingly, to ```filter()``` elements of an RDD. It works similarly to the ```map()``` transformations. It applies a function to all elements in an RDD. For example, suppose that ```f``` does return ```True``` if the input is odd and ```False``` otherwise. Suppose that you have an RDD containing a list of numbers. If you apply the ```f``` to the RDD you obtain again another RDD but with only odd numbers. Let's try it out.\n",
    "\n",
    "[filter]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.filter.html?highlight=filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8ca141f-f7e8-49c2-9176-667b399e4a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99]\n"
     ]
    }
   ],
   "source": [
    "### keep only Perfect Squares\n",
    "\n",
    "def isOdd(x): return True if x % 2 == 1 else False\n",
    "\n",
    "result = rdd.filter(isOdd).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb395f66-fbdb-4488-b22a-8b6315bc4bbd",
   "metadata": {},
   "source": [
    "### The reduce() function\n",
    "Let us see the reduce function outside the Spark framework. The [reduce] function is a little bit harder to understand. ```reduce(f)``` takes, again, a function which we will call ```f```. This time around ```f(*,*)``` will take two arguments: The first one, we will call it the **accumulator**. The second one, we will call it the **current value**. For now, forget that data are stored in RDDs. Suppose that you have a list of 100 numbers [1,2,3,...,100]. Once again, ```f``` is called on every element of our list.\n",
    "1) the first time ```f``` is called, the accumulator takes the first value of the list (```1```, in our example). Meanwhile the second argument of ```f``` is the second number in the list (```2```, in our example).\n",
    "2) the second time ```f``` is called, the accumulator takes the value of the output of 1). Meanwhile the second argument of ```f``` is the third number in the list (```3```, in our example).\n",
    "3) the third time ```f``` is called, the accumulator takes the value of the output of 2). Meanwhile the second argument of ```f``` is the fourth number in the list (```4```, in our example).\n",
    "4) and so on ...\n",
    "99) the 99th time ```f``` is called, the accumulator takes the value of the output of 98). Meanwhile the second argument of ```f``` is the 100th number in the list (```100```, in our example).\n",
    "\n",
    "In practice, the [reduce] function applies a function to every element of the list. However, meanwhile it computed can accumulate results.\n",
    "\n",
    "[reduce]:https://docs.python.org/3/library/functools.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3568f541-cee6-45d6-945a-ea5133235759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4950\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce ### not using spark\n",
    "\n",
    "def sumAll(acc, x): return acc + x\n",
    "\n",
    "result = reduce(sumAll, range(100))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c142d56f-6a2a-46dc-94d1-f0ff93708f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99]\n"
     ]
    }
   ],
   "source": [
    "# Now let us accumulate only odd numbers. \n",
    "# You do not need to understand this function too deeply.\n",
    "# Just keep in mind that the reduce function is a lot more flexible than it appears.\n",
    "def AccumulateOdds(acc, x):\n",
    "    if type(acc) != list: return [e for e in (acc,x) if isOdd(e)]\n",
    "    else: return acc + [x] if isOdd(x) else acc\n",
    "                \n",
    "print(reduce(AccumulateOdds,list(range(100))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f78763-4f85-44d9-968a-8d1314507adb",
   "metadata": {},
   "source": [
    "### The reduce() action\n",
    "Now the [reduce] spark action differs a bit from the functools Â·```reduce()```, but the main concepts are still valid. Again the ```reduce(*)``` action takes a function that is applied to evey element in the RDD. This function, call it ```f(*,*)```, takes two arguments. The first one accumulates the results and it is fed back to successive ```f(*,*)``` calls. However, the second argument can be an accumulator too. With simple reducer such as our ```sumAll(*,*)```, this does not make any difference. However, To reduces such as ```AccumulateOdds(*,*)```, it makes a lot of difference. \n",
    "\n",
    "\n",
    "[reduce]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduce.html?highlight=reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4bd604-94cc-436d-9422-396ae3dd754c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4950\n"
     ]
    }
   ],
   "source": [
    "# again a function that sums the whole elements\n",
    "result = rdd.reduce(sumAll)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1fe35f-61cd-4508-b27d-f79e7bc1c6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99]\n"
     ]
    }
   ],
   "source": [
    "# Now let us accumulate only odd numbers. \n",
    "# You do not need to understand this function too deeply.\n",
    "# Just keep in mind that the reduce function is a lot more flexible than it appears.\n",
    "\n",
    "def AccumulateOddsSpark(acc, x):\n",
    "    if type(acc) != list and type(x) != list: return [e for e in (acc,x) if isOdd(e)]\n",
    "    if type(acc) == list and type(x) != list: return acc + [x] if isOdd(x) else acc\n",
    "    if type(acc) == list and type(x) == list: return acc + x\n",
    "\n",
    "result = rdd.reduce(AccumulateOddsSpark)\n",
    "print(result)\n",
    "\n",
    "# A little side note. There is a drastic difference from using the .filter(isOdd).collect() and using this reducer.\n",
    "# In the first case the spark context is responsible for gathering all the filtered results. \n",
    "# Instead, in this case, we are directly gathering results oursevels. \n",
    "# Of course, this can lead to inefficiencies and it is quite error prone.\n",
    "# Again, this is to show the flexibility of the reduce function.\n",
    "# If you can obtain your result using map and reduce, just use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1844ff1-4492-40f2-9899-518f117ee277",
   "metadata": {},
   "source": [
    "### The Count() Action\n",
    "\n",
    "One of the most basic actions that we can run is the [count()] method which will count the number of elements in an RDD.\n",
    "\n",
    "Each task counts the entries in its partition and sends the result to your SparkContext, which adds up all of the counts. The figure below shows what would happen if we ran `count()` on a small example dataset with just four partitions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[count()]: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.count.html?highlight=count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e1ca040-e516-49d3-86c9-a6567a5137d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "# count the number of odds number in our rdd\n",
    "result = rdd.filter(isOdd).count()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b455f7-ba8f-4eb5-934b-edd7ea72e13f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
