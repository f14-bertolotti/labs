{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef531395-0f0f-4afd-b533-1a74690c6bfa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spark Tutorial\n",
    "_______________________\n",
    "\n",
    "## 1. The Problem\n",
    "\n",
    "Your data is split across several disks across several computers connected by a network. And, probably, all your data combined take so much space that they would never fit a single computer. On the other hand, you still need to process your data. Well, without proper infrastructure, you would need to track down each piece of information, process them separately, and combine the information together. On top of that, you will need to repeat this process many times, too many times. \n",
    "\n",
    "Luckily, the Hadoop Distributed File System (HDFS) solves most of these problems. It provides also MapReduce API to do the processing. However, the MapReduce API is still very low-level, and getting things done requires too much time.\n",
    "\n",
    "Here is where Spark comes into play. Spark provides high-level APIs for large-scale data processing. Spark can run over an existing HDFS or it can run in a standalone mode (which does not require setting up any HDFS). To be fair, Spark in standalone mode is not particularly useful but it provides an easy way to access its functionalities and test them. Moreover, whatever code you write in standalone mode that runs on your laptop will work automatically on huge HDFS. \n",
    "______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd6e40-e75f-4152-93e5-b4dfe9637596",
   "metadata": {},
   "source": [
    "### 2. Spark Context\n",
    "\n",
    "The [SparkContext] represents the entry point to access spark functionalities. \n",
    "\n",
    "In the Spark framework, there are two main actors: the driver and the executors. The driver has jobs that need to be run. The driver splits jobs into tasks. These tasks are submitted to executors. Once completed, results are sent back to the driver. \n",
    "\n",
    "We will run Spark in local mode, so that, we can avoid running a whole HDFS on our machine. We will focus mainly on the programming paradigm. However, it may be useful to know that platforms such as [DataBricks] do exist. They simplify a lot of the work necessary to set up a real cluster on which spark can run. \n",
    "\n",
    "In local mode, you can access the Spark Web UI in http://localhost:4040.\n",
    "\n",
    "[SparkContext]: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.html?highlight=pyspark%20sparkcontext#pyspark.SparkContext\n",
    "[DataBricks]: https://databricks.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3902093-8c4c-4491-9fe0-3a3d0009a95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ataxia:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc596ee9b70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1098a2-a3c0-44d9-b749-87aa527e8243",
   "metadata": {},
   "source": [
    "Regardless of whether you are familiar with python or not, these functions will help you a lot. [help] ```help(x)``` shows the documentation of ```x```. [type] ```type``` show a string representing the type of ```x```. [dir] ```dir(x)``` shows anything that is accessible inside of ```x```. You can find many more on the [built-in] documentation page.\n",
    "\n",
    "[help]:https://docs.python.org/3/library/functions.html#help\n",
    "[type]:https://docs.python.org/3/library/functions.html#type\n",
    "[dir]:https://docs.python.org/3/library/functions.html#dir[built-in]\n",
    "[built-in]:https://docs.python.org/3/library/functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afba3a22-3984-45b6-89fa-f05dd1b1feb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=test>\n",
      "<class 'pyspark.context.SparkContext'>\n"
     ]
    }
   ],
   "source": [
    "print(sc)\n",
    "print(type(sc))\n",
    "# help(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f9762d-c45d-4236-927f-02993d9dc6ab",
   "metadata": {},
   "source": [
    "________________________\n",
    "## 3. Resilient Distributed Dataset (RDD)\n",
    "\n",
    "[RDDs] are one of the main abstractions of Spark. They represent immutable elements distributed across different nodes.\n",
    "- **Resilient**: The system is able to recompute/recover missing or damaged partitions due to node failures.\n",
    "- **Distributed**: Data resides on multiple nodes in a cluster.\n",
    "- **Dataset**: Collection of data.\n",
    "- **Immutable**: Once created, they cannot change.\n",
    "- **Lazy evaluated**: Operations are performed only when necessary.\n",
    "- **Parallel**: Operations are performed parallelly.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"http://spark-mooc.github.io/web-assets/images/partitions.png\" alt=\"drawing\" width=\"600\"/></div>\n",
    "\n",
    "An RDD can be created by calling SparkContext’s [parallelize] method ```sc.parallelize()``` on an existing collection in your driver program. The elements of the collection are copied to form a distributed dataset. ```sc.parallelize``` takes two arguments:\n",
    "   1. The collection used to form the RDD.\n",
    "   2. the number of partitions to cut the dataset into. Spark tries to set the number of partitions automatically based on your cluster.\n",
    "\n",
    "[parallelize]: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.parallelize.html?highlight=parallelize\n",
    "[RDDs]:https://spark.apache.org/docs/latest/rdd-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bcb6824-3afa-4966-a5c4-39e8f27a3586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 30)\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "# Parallelize data using 8 partitions\n",
    "# This operation is a transformation of data into an RDD\n",
    "# Spark uses lazy evaluation, so no Spark jobs are run at this point\n",
    "data = range(30)\n",
    "rdd  = sc.parallelize(data, 4)\n",
    "\n",
    "print(data)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c44b7d82-87b8-4c0c-9ce0-2eea1ae8e66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd id: 5\n"
     ]
    }
   ],
   "source": [
    "# Each RDD gets a unique ID\n",
    "print('rdd id: {0}'.format(rdd.id()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7521a782-5698-415b-aade-5cb0f7c6da05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "My first rdd PythonRDD[5] at collect at /tmp/ipykernel_26395/3337392560.py:8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can name each newly created RDD using the setName() method\n",
    "rdd.setName('My first rdd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c9b566c-c2f8-4db9-8403-083c9af4b117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(4) My first rdd PythonRDD[5] at collect at /tmp/ipykernel_26395/3337392560.py:8 []\\n |  ParallelCollectionRDD[4] at readRDDFromFile at PythonRDD.scala:274 []'\n"
     ]
    }
   ],
   "source": [
    "# Let's view the lineage (the set of transformations) of the RDD using toDebugString()\n",
    "print(rdd.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbc65f1e-2a19-48f0-ab35-e4a219be4318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how many partitions the RDD will be split into by using the getNumPartitions()\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b946187-aca6-4558-ac60-ed7138fe7e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea362e0-8f19-4ab0-9c08-a61ca4e329ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "_______________\n",
    "\n",
    "## 4. Transformations vs Actions\n",
    "There are two types of operations that you can perform on an RDD: Transformations and Actions. \n",
    "- **Transformations**. Transformations are applied on RDDs and produce other RDDs. Additionally, Transformations are lazily evaluated, meaning that, they are not computed until an action is performed. Some common transformations are [map], and [filter].\n",
    "- **Actions**. Actions do not return RDDs anymore. Actions do set in motion the sequence of transformation required to produce the result. Once the computation is done you get the result as output. Some common actions are [collect], [count], [reduce], and [take].\n",
    "\n",
    "[map]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html?highlight=map\n",
    "[filter]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.filter.html?highlight=filter\n",
    "[reduce]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduce.html?highlight=reduce\n",
    "[collect]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.collect.html?highlight=collect\n",
    "[count]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.count.html?highlight=count\n",
    "[take]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.take.html?highlight=take\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8afe15-d38d-43da-875b-0d05158d3b01",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1 The map() Transformation\n",
    "```map(f)```, the most common Spark transformation: it applies a function ```f``` to each item in the dataset, and outputs the resulting dataset. When you run [map] on a dataset, a single stage of tasks is launched. A stage is a group of tasks that all perform the same computation, but on different input data. One task is launched for each partition, as shown in the example below. A task is a unit of execution that runs on a single machine. When we run ```map(f)``` within a partition, a new task applies ```f``` to all of the entries in a particular partition and outputs a new partition. In this example figure, the dataset is broken into four partitions (using three workers), so four ```map()``` tasks are launched.\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/tasks.png\" alt=\"drawing\" width=\"600\"/><img src=\"http://spark-mooc.github.io/web-assets/images/map.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "[map]: https://spark.apache.org/docs/3.2.1/api/python/reference/api/pyspark.RDD.map.html?highlight=map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d38221cf-6316-4d30-83c4-b02d5cbafdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sub function to subtract 1\n",
    "def sub(value): return (value - 1)\n",
    "\n",
    "# Apply sub\n",
    "rdd2 = rdd.map(sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fe6dc7-d5a7-4758-af7e-9e0b7f627e70",
   "metadata": {},
   "source": [
    "We have applied ```sub()``` to ```rdd```. So, each element in ```rdd``` gets decremented of ```1```. However, no computation as yet started. As mentioned earlier, spark is lazily evaluated. This means, that only when we require certain operation to be done, the whole computantion will actually start. Let's see one of these operation that force the computation to start, the [collect] action.\n",
    "\n",
    "You should feel a little bit of fear each time you call ```.collect()``` as it brings the data you requested on your machine memory. But what if you requested several GBs of data by mistake. Well, your machine may crash as the memory gets saturated and you may loose several hours worth of work.\n",
    "\n",
    "[collect]: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.collect.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5d905c7-5297-4e1d-8184-ddb10e7ad2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n"
     ]
    }
   ],
   "source": [
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16177199-23bc-4e92-b0a7-7b61b72846fd",
   "metadata": {},
   "source": [
    "### 4.2 The Filter() Transformation\n",
    "The [filter] transformation is used, surprisingly, to ```filter()``` elements of an RDD. It works similarly to the ```map()``` transformations. It applies a function to all elements in an RDD. For example, suppose that ```f``` does return ```True``` if the input is odd and ```False``` otherwise. Suppose that you have an RDD containing a list of numbers. If you apply the ```f``` to the RDD you obtain again another RDD but with only odd numbers. Let's try it out.\n",
    "\n",
    "The figure below shows how this would work on the small four-partition dataset.\n",
    " \n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/tasks.png\" alt=\"drawing\" width=\"600\"/><img src=\"http://spark-mooc.github.io/web-assets/images/filter.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "[filter]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.filter.html?highlight=filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8ca141f-f7e8-49c2-9176-667b399e4a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "### keep only Perfect Squares\n",
    "\n",
    "def isLessThan10(x): return True if x < 10 else False\n",
    "\n",
    "result = rdd.filter(isLessThan10).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb395f66-fbdb-4488-b22a-8b6315bc4bbd",
   "metadata": {},
   "source": [
    "### 4.3 The reduce() function\n",
    "Let us see the reduce function outside the Spark framework. The [reduce] function is a little bit harder to understand. ```reduce(f)``` takes, again, a function which we will call ```f```. This time around ```f(*,*)``` will take two arguments: The first one, we will call it the **accumulator**. The second one, we will call it the **current value**. For now, forget that data are stored in RDDs. Suppose that you have a list of 100 numbers [1,2,3,...,100]. Once again, ```f``` is called on every element of our list.\n",
    "1) the first time ```f``` is called, the accumulator takes the first value of the list (```1```, in our example). Meanwhile the second argument of ```f``` is the second number in the list (```2```, in our example).\n",
    "2) the second time ```f``` is called, the accumulator takes the value of the output of 1). Meanwhile, the second argument of ```f``` is the third number in the list (```3```, in our example).\n",
    "3) the third time ```f``` is called, the accumulator takes the value of the output of 2). Meanwhile, the second argument of ```f``` is the fourth number in the list (```4```, in our example).\n",
    "4) and so on ...\n",
    "99) the 99th time ```f``` is called, the accumulator takes the value of the output of 98). Meanwhile, the second argument of ```f``` is the 100th number in the list (```100```, in our example).\n",
    "\n",
    "In practice, the [reduce] function applies a function to every element of the list while accumulating results.\n",
    "\n",
    "[reduce]:https://docs.python.org/3/library/functools.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3568f541-cee6-45d6-945a-ea5133235759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce ### not using spark\n",
    "\n",
    "def sumAll(acc, x): return acc + x\n",
    "\n",
    "result = reduce(sumAll, range(30))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c142d56f-6a2a-46dc-94d1-f0ff93708f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29]\n"
     ]
    }
   ],
   "source": [
    "# Now let us accumulate only odd numbers. \n",
    "# You do not need to understand this function too deeply.\n",
    "# Just keep in mind that the reduce function is a lot more flexible than it appears.\n",
    "\n",
    "def isOdd(x):\n",
    "    return True if x % 2 == 1 else False\n",
    "\n",
    "def AccumulateOdds(acc, x):\n",
    "    if type(acc) != list: return [e for e in (acc,x) if isOdd(e)]\n",
    "    else: return acc + [x] if isOdd(x) else acc\n",
    "                \n",
    "print(reduce(AccumulateOdds,list(range(30))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487cafb8-7634-4a5e-af23-27797aa8d02a",
   "metadata": {},
   "source": [
    "### 4.3.1 Exercises\n",
    "\n",
    "* (★) given a list `[1,2,3,4,5]` multiply all the elements togheter using `reduce` from functools (result: `120`).\n",
    "* (★) given a list `[1,2,3,4,5]` sum all the elements greater than `2` using `reduce`  from functools (result: `12`).\n",
    "* (★) given a list `[1,2,3,4,5]` compute the average using `reduce` function  from functools (result: `3`).\n",
    "* (★★) given a list `[1,2,3,4,5]` filter even elements using the `reduce` function  from functools (result: `[1,3,5]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f78763-4f85-44d9-968a-8d1314507adb",
   "metadata": {},
   "source": [
    "### 4.4 The reduce() action\n",
    "Now the [reduce] spark action differs a bit from the functools ·```reduce()```, but the main concepts are still valid. Again the ```reduce(*)``` action takes a function that is applied to evey element in the RDD. This function, call it ```f(*,*)```, takes two arguments. The first one accumulates the results and it is fed back to successive ```f(*,*)``` calls. However, the second argument can be an accumulator too. With simple reducer such as our ```sumAll(*,*)```, this does not make any difference. However, To reduces such as ```AccumulateOdds(*,*)```, it makes a lot of difference. \n",
    "\n",
    "\n",
    "[reduce]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduce.html?highlight=reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e4bd604-94cc-436d-9422-396ae3dd754c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435\n"
     ]
    }
   ],
   "source": [
    "# again a function that sums the whole elements\n",
    "result = rdd.reduce(sumAll)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c1fe35f-61cd-4508-b27d-f79e7bc1c6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29]\n"
     ]
    }
   ],
   "source": [
    "# Now let us accumulate only odd numbers. \n",
    "# You do not need to understand this function too deeply.\n",
    "# Just keep in mind that the reduce function is a lot more flexible than it appears.\n",
    "\n",
    "def AccumulateOddsSpark(acc, x):\n",
    "    if type(acc) != list and type(x) != list: return [e for e in (acc,x) if isOdd(e)]\n",
    "    if type(acc) == list and type(x) != list: return acc + [x] if isOdd(x) else acc\n",
    "    if type(acc) == list and type(x) == list: return acc + x\n",
    "\n",
    "result = rdd.reduce(AccumulateOddsSpark)\n",
    "print(result)\n",
    "\n",
    "# A little side note. There is a drastic difference from using the .filter(isOdd).collect() and using this reducer.\n",
    "# In the first case the spark context is responsible for gathering all the filtered results. \n",
    "# Instead, in this case, we are directly gathering results oursevels. \n",
    "# Of course, this can lead to inefficiencies and it is quite error prone.\n",
    "# Again, this is to show the flexibility of the reduce function.\n",
    "# If you can obtain your result using map and reduce, just use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9316dca7-178b-4741-bd4d-eea625a715c2",
   "metadata": {},
   "source": [
    "### 4.3.1 Exercises\n",
    "\n",
    "* (★) given a list `[1,2,3,4,5]` multiply all the elements togheter using `reduce` from spark (result: `120`).\n",
    "* (★) given a list `[1,2,3,4,5]` sum all the elements greater than `2` using `reduce` from spark (result: `12`).\n",
    "* (★) given a list `[1,2,3,4,5]` compute the average using `reduce` from spark (result: `3`).\n",
    "* (★★) given a list `[1,2,3,4,5]` filter even elements using the `reduce` function from spark (result: `[1,3,5]`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1844ff1-4492-40f2-9899-518f117ee277",
   "metadata": {},
   "source": [
    "### 4.5 The Count() Action\n",
    "\n",
    "One of the most basic actions that we can run is the [count()] method which will count the number of elements in an RDD.\n",
    "\n",
    "Each task counts the entries in its partition and sends the result to your SparkContext, which adds up all of the counts. The figure below shows what would happen if we ran `count()` on a small example dataset with just four partitions.\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/count.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "\n",
    "[count()]: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.count.html?highlight=count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e1ca040-e516-49d3-86c9-a6567a5137d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(rdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab15f99-5860-4c6e-b259-15ac288b250f",
   "metadata": {},
   "source": [
    "____________\n",
    "## 5. Additional Actions\n",
    "\n",
    "* [first]: `first()` returns the first available elements from the RDD. it depends on how the RDD is partitioned.\n",
    "* [take]: `take(num)` returns the first `num` elements from the RDD. it depends on how the RDD is partitioned.\n",
    "* [top]: `top(num, key=None)` returns the first (in descending order) `num` elements according to the `key`. \n",
    "* [takeOrdered]: `takeOrdered(num, key=None)`returns the first (int ascending order) `num` elements according to the `key`.\n",
    "* [takeSample]: `takeSample(withReplacement, num, seed=None)` it randomly select `num` elements from the RDD. If `withReplacement=True` then it can return the same elements multiple times. Using two times the same `seed` yields the same results.\n",
    "* [countByValue]: `countByValue()` returns the count of each unique value in this RDD as a dictionary of (value, count) pairs.\n",
    "\n",
    "[first]: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.first.html?highlight=first#pyspark.RDD.first \n",
    "[take]: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.take.html?highlight=take#pyspark.RDD.take\n",
    "[top]: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.top.html?highlight=top#pyspark.RDD.top\n",
    "[takeOrdered]: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.takeOrdered.html?highlight=takeordered#pyspark.RDD.takeOrdered\n",
    "[takeSample]: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.takeSample.html?highlight=takesample#pyspark.RDD.takeSample\n",
    "[countByValue]: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.countByValue.html?highlight=countbyvalue#pyspark.RDD.countByValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b7d9575-a1a5-4125-a925-6363fdf72259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd.first()                    = 0\n",
      "rdd.take(5)                    = [0, 1, 2, 3, 4]\n",
      "rdd.top(5, lambda x:x)         = [29, 28, 27, 26, 25]\n",
      "rdd.takeOrdered(5, lambda x:x) = [0, 1, 2, 3, 4]\n",
      "rdd.takeSample(True, 5, 14)    = [8, 21, 28, 22, 3]\n",
      "rdd.countByValue()             = defaultdict(<class 'int'>, {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 1, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 1})\n"
     ]
    }
   ],
   "source": [
    "print(f\"rdd.first()                    = {rdd.first()}\")\n",
    "print(f\"rdd.take(5)                    = {rdd.take(5)}\")\n",
    "print(f\"rdd.top(5, lambda x:x)         = {rdd.top(5, lambda x:x)}\")\n",
    "print(f\"rdd.takeOrdered(5, lambda x:x) = {rdd.takeOrdered(5, lambda x:x)}\")\n",
    "print(f\"rdd.takeSample(True, 5, 14)    = {rdd.takeSample(True, 5, 14)}\")\n",
    "print(f\"rdd.countByValue()             = {rdd.countByValue()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69e8751-0d13-4d0e-8776-7fa71d1c6b0c",
   "metadata": {},
   "source": [
    "________________\n",
    "## 6. Additional Transformations\n",
    "\n",
    "* [flatMap]: `flatMap(f)` returns a new RDD by first applying `f` to all elements of this RDD, and then flattening the results.\n",
    "* [groupByKey]: `groupByKey()` Groups the values for each key in the RDD into a single sequence. This transformation operates on pair RDDs.  \n",
    "* [reduceByKey]:  `reduceByKey(func)` Merges the values for each key using an associative and commutative reduce function.\n",
    "\n",
    "\n",
    "\n",
    "Both of these transformations ([groupByKey] and [reduceByKey]) operate on pair RDDs.  A pair RDD is an RDD where each element is a pair tuple (key, value).  For example, `sc.parallelize([('a', 1), ('a', 2), ('b', 1)])` would create a pair RDD where the keys are 'a', 'a', 'b' and the values are 1, 2, 1.\n",
    "The `reduceByKey()` transformation gathers together pairs that have the same key and applies a function to two associated values at a time. `reduceByKey()` operates by applying the function first within each partition on a per-key basis and then across the partitions.\n",
    "While both the `groupByKey()` and `reduceByKey()` transformations can often be used to solve the same problem and will produce the same answer, the `reduceByKey()` transformation works much better for large distributed datasets. This is because Spark knows it can combine output with a common key on each partition *before* shuffling (redistributing) the data across nodes.  Only use `groupByKey()` if the operation would not benefit from reducing the data before the shuffle occurs.\n",
    " \n",
    "Look at the diagram below to understand how `reduceByKey` works.  Notice how pairs on the same machine with the same key are combined (by using the lamdba function passed into reduceByKey) before the data is shuffled. Then the lamdba function is called again to reduce all the values from each partition to produce one final result.\n",
    "\n",
    "On the other hand, when using the `groupByKey()` transformation - all the key-value pairs are shuffled around, causing a lot of unnecessary data to be transferred over the network.\n",
    " \n",
    "To determine which machine to shuffle a pair to, Spark calls a partitioning function on the key of the pair. Spark spills data to disk when there is more data shuffled onto a single executor machine than can fit in memory. However, it flushes out the data to disk one key at a time, so if a single key has more key-value pairs than can fit in memory an  out-of-memory exception occurs. This will be more gracefully handled in a later release of Spark so that the job can still proceed, but should still be avoided.  When Spark needs to spill to disk, performance is severely impacted.\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/group_by.png\" alt=\"drawing\" width=\"600\"/> <img src=\"http://spark-mooc.github.io/web-assets/images/reduce_by.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "[flatMap]: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.flatMap.html?highlight=flatmap#pyspark.RDD.flatMap\n",
    "[groupByKey]: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.groupByKey.html?highlight=groupbykey#pyspark.RDD.groupByKey\n",
    "[reduceByKey]: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.reduceByKey.html?highlight=reducebykey#pyspark.RDD.reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b42d336-ad51-47bc-94ac-d7f15ef893eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatMap    : [('a', 1), ('a', 1), ('a', 2), ('a', 2), ('b', 1), ('b', 1)]\n",
      "groupByKey : [('a', [1, 2]), ('b', [1])]\n",
      "reduceByKey: [('a', 3), ('b', 1)]\n"
     ]
    }
   ],
   "source": [
    "pair_rdd = sc.parallelize([('a', 1), ('a', 2), ('b', 1)])\n",
    "print(\"flatMap    :\", pair_rdd.flatMap(lambda x: [x,x]).collect())\n",
    "print(\"groupByKey :\", pair_rdd.groupByKey().map(lambda x:(x[0],list(x[1]))).collect())\n",
    "print(\"reduceByKey:\", pair_rdd.reduceByKey(lambda x,y:x+y).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5024bb18-3cc6-4297-a385-8d63770d55dc",
   "metadata": {},
   "source": [
    "### 6.1 Exercises\n",
    "\n",
    "You will work on this pair rdd (name, price): \n",
    "```\n",
    "('alpha', '9/2/22' , '932$'), \n",
    "('alpha', '10/2/22', '904$'), \n",
    "('alpha', '11/2/22', '806$'),\n",
    "('beta' , '9/2/22' , '2831$'), \n",
    "('beta' , '10/2/22', '2732$'), \n",
    "('beta' , '11/2/22', '2685$'),\n",
    "('gamma', '9/2/22' , '312$'), \n",
    "('gamma', '10/2/22', '301$'), \n",
    "('gamma', '11/2/22', '285$')\n",
    "```\n",
    "\n",
    "* (★) compute the total price. (result: `11788`) \n",
    "* (★★) compute the average per name. (result: `('alpha', 880.666), ('beta', 2749.333), ('gamma',299.333)`) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed59db59-1589-4f7a-8a4c-fc9a40a0df14",
   "metadata": {},
   "source": [
    "______________________________________\n",
    "## 7. RDDs Memory Management\n",
    "\n",
    "For efficiency Spark keeps your RDDs in RAM memory. By keeping the contents in memory, Spark can quickly access the data. However, memory is limited, so if you try to keep too many RDDs in memory, Spark will automatically delete RDDs from memory to make space for new RDDs. If you later refer to one of the RDDs, Spark will automatically recreate the RDD for you, but that takes time.\n",
    " \n",
    "So, if you plan to use an RDD more than once, then you should tell Spark to cache that RDD. You can use the `cache()` operation to keep the RDD in memory. However, you must still trigger an action on the RDD, such as `collect()` for the RDD to be created, and only then will the RDD be cached. Keep in mind that if you cache too many RDDs and Spark runs out of memory, it will delete the least recently used (LRU) RDD first. Again, the RDD will be automatically recreated when accessed.\n",
    " \n",
    "You can check if an RDD is cached by using the `is_cached` attribute, and you can see your cached RDD in the \"Storage\" section of the Spark web UI. If you click on the RDD's name, you can see more information about where the RDD is stored.\n",
    "\n",
    "[cache]: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.cache.html?highlight=cache#pyspark.RDD.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55906d78-c1df-40e5-8308-b5d4bd87d3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before .cache()              : False\n",
      "after  .cache()              : True\n",
      "after  .cache() and an action: True\n"
     ]
    }
   ],
   "source": [
    "new_rdd = sc.parallelize(range(10))\n",
    "new_rdd.setName(\"MyRDD\")\n",
    "print(\"before .cache()              :\", new_rdd.is_cached)\n",
    "new_rdd.cache()\n",
    "print(\"after  .cache()              :\", new_rdd.is_cached)\n",
    "new_rdd.collect()\n",
    "print(\"after  .cache() and an action:\", new_rdd.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4c75fe-960e-4b3e-94dc-8833a15d0287",
   "metadata": {},
   "source": [
    "Spark automatically manages the RDDs cached in memory and will save them to disk if it runs out of memory. For efficiency, once you are finished using an RDD, you can optionally tell Spark to stop caching it in memory by using the RDD's `unpersist()` method to inform Spark that you no longer need the RDD in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad8d3de5-e4bc-490e-a245-265f5bee61c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyRDD PythonRDD[31] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rdd.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-spark",
   "language": "python",
   "name": "venv-spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
