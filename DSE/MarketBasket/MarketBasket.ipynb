{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f503bb-6766-4fc2-9ccb-fe216d65d4c3",
   "metadata": {},
   "source": [
    "# MarketBasket\n",
    "### Overview\n",
    "In market basket analysis, the **basket** refers to a customer's collection of items during a single purchase trip. It's not literally a physical basket, but rather the data that represents what products a customer buys together. The basket are represented as **sets** of indices, each index referring to a specific product (e.g. `{112, 41, 1020}` is a basket). \n",
    "\n",
    "By identifying frequently bought itemsets (groups of items purchased together), we can reveal associations between products. This allows retailers to understand which items customers tend to buy together.  This knowledge is crucial for strategies like targeted promotions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03ba4b-cc64-4d66-8cac-5fd98324847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need a dataset. \n",
    "# Let us generate a dataset in python\n",
    "def random_basket(items, max_basket_size=15):\n",
    "    basket_size = random.randint(1,max_basket_size)\n",
    "    return {random.randint(0,items-1) for i in range(basket_size)}\n",
    "\n",
    "import random\n",
    "dataset_size, items = 100000, 100\n",
    "dataset = [random_basket(items, 10) for i in range(dataset_size)]\n",
    "print(dataset[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170e834b-122a-4147-a387-322b720db899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's intialize the spark context and let's parallelize the data\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Apriori\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521a69e-fc52-44ec-bb47-f30797bc90a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's insert data in spark\n",
    "rdd = spark.sparkContext.parallelize(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967e2ea4-a8d5-4bb9-91bb-95dd9d4960ec",
   "metadata": {},
   "source": [
    "## The A-Priori Algorithm\n",
    "This algorithm leverages a key property of itemsets - if a large itemset is frequent, all its smaller subsets must also be frequent. (e.g. if `{12,54,22,92,69,4}` is frequent then also all its subsets are frequent, therefore sets as `{12,54,22}` and `{12,69,4}` are frequent). Itemsets are considered frequent (or interesting) when their frequency exceeds a threshold parameterd---called **support**.\n",
    "\n",
    "> The Role of Support:  The Apriori algorithm uses a minimum support threshold. This threshold defines how frequent an itemset needs to be considered \"interesting\" for further analysis.  Items or itemsets that appear less frequently than the threshold are discarded. Therefore, it is crucial to select a support that filters most of the data (to maintain the algorithm light) while not discarding interesting connections.\n",
    "\n",
    "- The first step of the A-priori algorithm is to count occurencies of each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721fa09-9840-4799-a463-ae798b49fd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "support = 310\n",
    "first_pass = rdd.flatMap(lambda basket:[(e,1) for e in basket]) \\\n",
    "                .reduceByKey(lambda x,y:x+y) \\\n",
    "                .filter(lambda x:x[1]>support)\n",
    "\n",
    "print(\"remaining singleton\", first_pass.count())\n",
    "print(\"5 random singleton\", first_pass.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26913401-dd1d-43af-9c62-6b9e29cb8a25",
   "metadata": {},
   "source": [
    "- Now we need to count all pair composed of frequent singletons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15240c9f-19e5-471a-9e39-fe2225493d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_singletons = set(first_pass.map(lambda x:x[0]).collect())\n",
    "second_pass = rdd.flatMap(lambda basket:[((e0,e1),1) for e0 in basket for e1 in basket if e1 != e0 and e0 in frequent_singletons and e1 in frequent_singletons]) \\\n",
    "                 .reduceByKey(lambda x,y: x+y) \\\n",
    "                 .filter(lambda x:x[1]>support)\n",
    "print(second_pass.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53938dd4-e99f-41ee-b289-68886ec14e2d",
   "metadata": {},
   "source": [
    "- Now we need to count all the triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf7ef9-aa25-444d-8760-3adb437fc085",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_pairs = set(second_pass.map(lambda x:x[0]).collect())\n",
    "third_pass = rdd.flatMap(lambda basket:[((e0,e1,2),1) for e0 in basket for e1 in basket for e2 in basket if \\\n",
    "                                         e1 != e0 and e0 != e2 and (e0,e1) in frequent_pairs and (e1,e2) in frequent_pairs and (e0,e2) in frequent_pairs]) \\\n",
    "                 .reduceByKey(lambda x,y: x+y) \\\n",
    "                 .filter(lambda x:x[1]>support)\n",
    "print(third_pass.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fda68c2-5a39-4286-a8bb-13cd4de4ffd7",
   "metadata": {},
   "source": [
    "You get the point, we simply reiterate this simple steps until we have no more frequent itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b2ebf4-342b-4321-8937-42c26467b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "support = 3\n",
    "\n",
    "frdd = rdd.flatMap(lambda basket:[(e,1) for e in basket]) \\\n",
    "          .reduceByKey(lambda x,y:x+y) \\\n",
    "          .filter(lambda x:x[1] > support)\n",
    "frequent = set(first_pass.map(lambda x:(x[0],)).collect())\n",
    "\n",
    "print(f\"remaining: {len(frequent)}, frdd {frdd.take(5)}\")\n",
    "    \n",
    "k = 2\n",
    "while frdd.count() != 0:\n",
    "    frdd = rdd.flatMap(lambda basket: [(x,1) for x in combinations(basket,k) if all([y in frequent for y in combinations(x,len(x)-1)])]) \\\n",
    "              .reduceByKey(lambda x,y:x+y) \\\n",
    "              .filter(lambda x:x[1] > support)\n",
    "    \n",
    "    frequent = set(frdd.map(lambda x:x[0]).collect())\n",
    "    print(k, len(frequent), frdd.take(5))\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8d8cee-ae12-44cd-9410-50354011fae8",
   "metadata": {},
   "source": [
    "(⭐⭐⭐) repeat this algorithm with the data in `data.txt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
