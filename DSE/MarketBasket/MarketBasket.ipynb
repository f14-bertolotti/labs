{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f503bb-6766-4fc2-9ccb-fe216d65d4c3",
   "metadata": {},
   "source": [
    "# MarketBasket\n",
    "### Overview\n",
    "In market basket analysis, the **basket** refers to a customer's collection of items during a single purchase trip. It's not literally a physical basket, but rather the data that represents what products a customer buys together. The basket are represented as **sets** of indices, each index referring to a specific product (e.g. `{112, 41, 1020}` is a basket). \n",
    "\n",
    "By identifying frequently bought itemsets (groups of items purchased together), we can reveal associations between products. This allows retailers to understand which items customers tend to buy together.  This knowledge is crucial for strategies like targeted promotions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03ba4b-cc64-4d66-8cac-5fd98324847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need a dataset. \n",
    "# Let us generate a dataset in python\n",
    "def random_basket(items, max_basket_size=15):\n",
    "    basket_size = random.randint(1,max_basket_size)\n",
    "    return {random.randint(0,items-1) for i in range(basket_size)}\n",
    "\n",
    "import random\n",
    "dataset_size, items = 100000, 100\n",
    "dataset = [random_basket(items, 10) for i in range(dataset_size)]\n",
    "print(dataset[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170e834b-122a-4147-a387-322b720db899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's intialize the spark context and let's parallelize the data\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Apriori\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521a69e-fc52-44ec-bb47-f30797bc90a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's insert data in spark\n",
    "rdd = spark.sparkContext.parallelize(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967e2ea4-a8d5-4bb9-91bb-95dd9d4960ec",
   "metadata": {},
   "source": [
    "## The A-Priori Algorithm\n",
    "This algorithm leverages a key property of itemsets - if a large itemset is frequent, all its smaller subsets must also be frequent. (e.g. if `{12,54,22,92,69,4}` is frequent then also all its subsets are frequent, therefore sets as `{12,54,22}` and `{12,69,4}` are frequent). Itemsets are considered frequent (or interesting) when their frequency exceeds a threshold parameterd---called **support**.\n",
    "\n",
    "> The Role of Support:  The Apriori algorithm uses a minimum support threshold. This threshold defines how frequent an itemset needs to be considered \"interesting\" for further analysis.  Items or itemsets that appear less frequently than the threshold are discarded. Therefore, it is crucial to select a support that filters most of the data (to maintain the algorithm light) while not discarding interesting connections.\n",
    "\n",
    "- The first step of the A-priori algorithm is to count occurencies of each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721fa09-9840-4799-a463-ae798b49fd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "support = 310\n",
    "first_pass = rdd.flatMap(lambda basket:[(e,1) for e in basket]) \\\n",
    "                .reduceByKey(lambda x,y:x+y) \\\n",
    "                .filter(lambda x:x[1]>support)\n",
    "\n",
    "print(\"remaining singleton\", first_pass.count())\n",
    "print(\"5 random singleton\", first_pass.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26913401-dd1d-43af-9c62-6b9e29cb8a25",
   "metadata": {},
   "source": [
    "- Now we need to count all pair composed of frequent singletons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15240c9f-19e5-471a-9e39-fe2225493d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_singletons = set(first_pass.map(lambda x:x[0]).collect())\n",
    "second_pass = rdd.flatMap(lambda basket:[((e0,e1),1) for e0 in basket for e1 in basket if e1 != e0 and e0 in frequent_singletons and e1 in frequent_singletons]) \\\n",
    "                 .reduceByKey(lambda x,y: x+y) \\\n",
    "                 .filter(lambda x:x[1]>support)\n",
    "print(second_pass.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53938dd4-e99f-41ee-b289-68886ec14e2d",
   "metadata": {},
   "source": [
    "- Now we need to count all the triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf7ef9-aa25-444d-8760-3adb437fc085",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_pairs = set(second_pass.map(lambda x:x[0]).collect())\n",
    "third_pass = rdd.flatMap(lambda basket:[((e0,e1,2),1) for e0 in basket for e1 in basket for e2 in basket if \\\n",
    "                                         e1 != e0 and e0 != e2 and (e0,e1) in frequent_pairs and (e1,e2) in frequent_pairs and (e0,e2) in frequent_pairs]) \\\n",
    "                 .reduceByKey(lambda x,y: x+y) \\\n",
    "                 .filter(lambda x:x[1]>support)\n",
    "print(third_pass.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fda68c2-5a39-4286-a8bb-13cd4de4ffd7",
   "metadata": {},
   "source": [
    "You get the point, we simply reiterate this simple steps until we have no more frequent itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b2ebf4-342b-4321-8937-42c26467b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "support = 3\n",
    "\n",
    "frdd = rdd.flatMap(lambda basket:[(e,1) for e in basket]) \\\n",
    "          .reduceByKey(lambda x,y:x+y) \\\n",
    "          .filter(lambda x:x[1] > support)\n",
    "frequent = set(first_pass.map(lambda x:(x[0],)).collect())\n",
    "\n",
    "print(f\"remaining: {len(frequent)}, frdd {frdd.take(5)}\")\n",
    "    \n",
    "k = 2\n",
    "while frdd.count() != 0:\n",
    "    frdd = rdd.flatMap(lambda basket: [(x,1) for x in combinations(basket,k) if all([y in frequent for y in combinations(x,len(x)-1)])]) \\\n",
    "              .reduceByKey(lambda x,y:x+y) \\\n",
    "              .filter(lambda x:x[1] > support)\n",
    "    \n",
    "    frequent = set(frdd.map(lambda x:x[0]).collect())\n",
    "    print(k, len(frequent), frdd.take(5))\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8d8cee-ae12-44cd-9410-50354011fae8",
   "metadata": {},
   "source": [
    "(⭐⭐⭐) repeat this algorithm with the data in `data.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb8bc38-c528-4718-a3bc-1cec9857e05e",
   "metadata": {},
   "source": [
    "## PCY Algorithm\n",
    "The PCY algorithm, also referred to as the Park-Chen-Yu algorithm, serves as a data mining technique specifically designed to detect frequently occurring itemsets within expansive datasets. It represents an enhancement over the Apriori algorithm.\n",
    "\n",
    "In the Apriori algorithm, we would only tally itemsets if all their respective subsets were frequent. For instance, an itemset like `(i,j)` would only be counted if both `i` and `j` were frequent. With PCY, an additional discriminator is employed to ascertain the actual frequency of an itemset—**bucket**.\n",
    "\n",
    "A bucket essentially functions as a counter linked to a set of itemsets. Let's denote these itemsets as $X=\\{\\text{itemset}_1,\\dots,\\text{itemset}_m\\}$ associated with the same bucket. If these itemsets collectively appear a total of $k$ times, the bucket's counter will reflect this count. Consequently, it's evident that each itemset within $X$ cannot exceed a count of $k$ (as exceeding this count would result in a bucket count greater than $k$).\n",
    "\n",
    "To illustrate, consider the itemset collection $X=\\{\\{11,3\\},\\{1,12\\},\\{13,2\\},\\{4,5\\}\\}$. Let's assume all itemsets in $X$ are linked to the same bucket. If the itemsets $\\{11,3\\},\\{1,12\\},\\{13,2\\},\\{4,5\\}$ respectively occur 1, 2, 5, and 2 times among the baskets, the associated bucket count would be $1+2+5+2=10$. Consequently, we deduce that no itemset in $X$ appears more than 10 times. Hence, if our chosen support threshold were $3$, we infer the presence of frequent itemsets within $X$. Conversely, if the support threshold were $15$, we conclude that none of the itemsets in $X$ are frequent.\n",
    "\n",
    "We can link itemset to bucket by simply using hash functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35984e36-9410-4063-8c40-dd968ce7bbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hash((1,2)))\n",
    "# what if we want a bucket ?\n",
    "print(hash((1,2))%10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3082c7a5-de30-4bdf-936d-6fc86836c468",
   "metadata": {},
   "source": [
    "In the first step of the algorithm we will compute the counts for each singleton (same as in the Apriori algorithm) and we will also compute the bucket frequency for each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b833e9ba-e791-4341-a54f-ed0348c152aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_buckets_to_bitmap(buckets, nobuckets):\n",
    "    zero = [0] * nobuckets\n",
    "    for b in buckets.collect(): zero[b[0]] = b[1]\n",
    "    return zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1fcf8c-2adf-44f6-a9fe-3861470d58eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "support = 310\n",
    "buckets = 10000\n",
    "first_pass = rdd.flatMap(lambda basket:[(e,1) for e in basket]) \\\n",
    "                .reduceByKey(lambda x,y:x+y) \\\n",
    "                .filter(lambda x:x[1]>support)\n",
    "\n",
    "first_buckets = rdd.flatMap(lambda basket:[(hash(e)%buckets,1) for e in combinations(basket,2)]) \\\n",
    "                .reduceByKey(lambda x,y:x+y) \\\n",
    "                .map(lambda x:(x[0],int(x[1] > support)))\n",
    "first_bitmap = from_buckets_to_bitmap(first_buckets, buckets)\n",
    "\n",
    "\n",
    "print(\"remaining singleton\", first_pass.count())\n",
    "print(\"5 random singleton\", first_pass.take(5))\n",
    "print(f\"frequent buckets:{sum(first_bitmap)}, bitmap:\", \"\".join(map(str,first_bitmap[:100])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e53af9f-89ed-438d-a3bc-3b4285909729",
   "metadata": {},
   "source": [
    "In the next step, we will compute the frequency of pairs for which\n",
    "- the respective bucket is set to `1`.\n",
    "- their singleton are all frequent (exceed the support)\n",
    "\n",
    "Additionally, we will also compute buckets for the triplets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0170c9cf-a8bd-4fc0-8419-4fd2ccc94134",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_singletons = set(first_pass.map(lambda x:x[0]).collect())\n",
    "\n",
    "second_pass = rdd.flatMap(lambda basket:[(e,1) for e in combinations(basket,2) if  \n",
    "                                         e[0] in frequent_singletons and \n",
    "                                         e[1] in frequent_singletons]) \\\n",
    "                 .filter(lambda x:bitmap[hash(x[0])%buckets]) \\\n",
    "                 .reduceByKey(lambda x,y: x+y) \\\n",
    "                 .filter(lambda x:x[1]>support)\n",
    "\n",
    "second_buckets = rdd.flatMap(lambda basket:[(hash(e)%buckets,1) for e in combinations(basket,3)]) \\\n",
    "                .reduceByKey(lambda x,y:x+y) \\\n",
    "                .map(lambda x:(x[0],int(x[1] > support)))\n",
    "\n",
    "second_bitmap = from_buckets_to_bitmap(second_buckets, buckets)\n",
    "\n",
    "print(second_pass.take(5))\n",
    "print(f\"frequent buckets:{sum(second_bitmap)}, bitmap:\", \"\".join(map(str,second_bitmap[:100])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fdc34c-cd81-4da4-aa90-9f2d317499e0",
   "metadata": {},
   "source": [
    "In the next step, we need to compute the frequency of triplets for which\n",
    "- the respective bucket is set to `1`.\n",
    "- their pars are all frequent (exceed the support)\n",
    "\n",
    "Additionally, we will need also compute buckets for the quadruplets. And so on.\n",
    "\n",
    "Let us instead focus on the iterative algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aae873-e801-499d-a3fc-53ab8d1a95fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "support = 3\n",
    "buckets = 1000000\n",
    "\n",
    "k = 1\n",
    "# lets compute the frequent singletons\n",
    "frdd = rdd.flatMap(lambda basket:[(e,1) for e in basket]) \\\n",
    "          .reduceByKey(lambda x,y:x+y) \\\n",
    "          .filter(lambda x:x[1] > support)\n",
    "\n",
    "# lets compute bucket counters for all the pairs in the baskets\n",
    "fbuckets = rdd.flatMap(lambda basket:[(hash(e)%buckets,1) for e in combinations(basket,2)]) \\\n",
    "            .reduceByKey(lambda x,y:x+y) \\\n",
    "            .map(lambda x:(x[0],int(x[1] > support)))\n",
    "\n",
    "# from the buckets counters, we get the bitmap (1 for frequent bucket)\n",
    "bitmap = from_buckets_to_bitmap(fbuckets, buckets)\n",
    "\n",
    "# the frequent elements (same as the Apriori)\n",
    "frequent = set(first_pass.map(lambda x:(x[0],)).collect())\n",
    "\n",
    "print(f\"it:{k}, frequents:{len(frequent)}, bitmap:{sum(bitmap)}, samples:{frdd.take(3)}\")\n",
    "\n",
    "k = 2\n",
    "while frdd.count() != 0:\n",
    "\n",
    "    # we use the frequent elements and the bitmap to filter the itemsets\n",
    "    frdd = (rdd.flatMap(lambda basket: [(x,1) for x in combinations(basket,k)]) # compute all itemsets of size k from each basket\n",
    "              .filter(lambda x: all([y in frequent for y in combinations(x[0],len(x[0])-1)])) # filter itemsets that have a non-frequent sub-itemset\n",
    "              .filter(lambda x:bitmap[hash(x[0])%buckets]) # filter itemsets with 0 bitamp value\n",
    "              .reduceByKey(lambda x,y:x+y) # count remaining itemsets\n",
    "              .filter(lambda x:x[1] > support)) # filter those that are frequent\n",
    "\n",
    "    # now we compute the buckets and the bitmap for the next iteration\n",
    "    fbuckets = (rdd.flatMap(lambda basket:[(hash(e)%buckets,1) for e in combinations(basket,k+1)]) # hash each itemset on its bucket \n",
    "            .reduceByKey(lambda x,y:x+y) # count buckets\n",
    "            .map(lambda x:(x[0],int(x[1] > support)))) # map to 1 those that are frequent \n",
    "    bitmap = from_buckets_to_bitmap(fbuckets, buckets) # compute the bitmap\n",
    "\n",
    "    # get the frequent elements for the next step\n",
    "    frequent = set(frdd.map(lambda x:x[0]).collect())\n",
    "    \n",
    "    print(f\"it:{k}, frequents:{len(frequent)}, bitmap:{sum(bitmap)}, samples:{frdd.take(3)}\")\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce22e14-2756-4f59-9486-de5250aa0ced",
   "metadata": {},
   "source": [
    "(⭐⭐⭐) repeat this algorithm with the data in `data.txt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
