{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef531395-0f0f-4afd-b533-1a74690c6bfa",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n",
    "\n",
    "# Spark Tutorial\n",
    "_______________________\n",
    "\n",
    "## The Problem\n",
    "\n",
    "Your data is distributed across multiple disks on several computers connected by a network, likely surpassing the storage capacity of any single computer. Without proper infrastructure, managing and processing this data would involve tracking down each piece separately, processing them individually, and then combining the results—a cumbersome and time-consuming process.\n",
    "\n",
    "Fortunately, the **Hadoop Distributed File System** (HDFS) resolves many of these challenges. It not only manages distributed data storage but also provides the MapReduce API for processing. However, the MapReduce API is quite low-level and can be time-consuming to work with.\n",
    "\n",
    "**Spark** offers high-level APIs for large-scale data processing. It can operate on an existing HDFS or in standalone mode, which doesn't require setting up an HDFS. Although Spark in standalone mode may not be as useful, it offers an accessible way to access and test its functionalities. Additionally, any code written and tested in standalone mode on a laptop seamlessly scales to process huge datasets on HDFS.\n",
    "______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd6e40-e75f-4152-93e5-b4dfe9637596",
   "metadata": {},
   "source": [
    "### Spark Context\n",
    "\n",
    "The [SparkContext] represents the entry point to access spark functionalities. \n",
    "\n",
    "In the Spark framework, two primary actors are involved: the driver (e.g., a Python notebook) and the executors (e.g., Java Virtual Machines). The driver divides jobs into tasks, which are then submitted to the executors. The executors run these tasks and return the results to the driver.. \n",
    "\n",
    "We will run Spark in local mode, so that, we can avoid running a whole HDFS on our machine. We will focus mainly on the programming paradigm. However, it may be useful to know that platforms such as [DataBricks] do exist. They simplify a lot of the work necessary to set up a real cluster on which spark can run. \n",
    "\n",
    "In local mode, you can access the Spark Web UI in http://localhost:4040.\n",
    "\n",
    "[SparkContext]: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.html?highlight=pyspark%20sparkcontext#pyspark.SparkContext\n",
    "[DataBricks]: https://databricks.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3902093-8c4c-4491-9fe0-3a3d0009a95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/28 11:55:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ataxia:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7578c6b6f5d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f9762d-c45d-4236-927f-02993d9dc6ab",
   "metadata": {},
   "source": [
    "## Resilient Distributed Dataset (RDD)\n",
    "\n",
    "[RDDs] are one of the main abstractions of Spark. They represent **immutable** elements distributed across different nodes.\n",
    "- **Resilient**: The system is able to recompute/recover missing or damaged partitions due to node failures.\n",
    "- **Distributed**: Data resides on multiple nodes in a cluster.\n",
    "- **Dataset**: Collection of data.\n",
    "- **Immutable**: Once created, they cannot change.\n",
    "- **Lazy evaluated**: Operations are performed only when necessary.\n",
    "- **Parallel**: Operations are performed parallelly.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"http://spark-mooc.github.io/web-assets/images/partitions.png\" alt=\"drawing\" width=\"600\"/></div>\n",
    "\n",
    "An RDD can be created by calling SparkContext’s [parallelize] method ```sc.parallelize()``` on an existing collection in your driver program. The elements of the collection are copied to form a distributed dataset. ```sc.parallelize``` takes two arguments:\n",
    "   1. The collection used to form the RDD.\n",
    "   2. the number of partitions to cut the dataset into. Spark tries to set the number of partitions automatically based on your cluster.\n",
    "\n",
    "[parallelize]: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.parallelize.html?highlight=parallelize\n",
    "[RDDs]:https://spark.apache.org/docs/latest/rdd-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bcb6824-3afa-4966-a5c4-39e8f27a3586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Parallelize data using 4. partitions\n",
    "# This operation is a transformation of data into an RDD\n",
    "# Spark uses lazy evaluation, so no Spark jobs are run at this point\n",
    "data = range(30)\n",
    "rdd  = sc.parallelize(data, 4)\n",
    "\n",
    "print(data)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c44b7d82-87b8-4c0c-9ce0-2eea1ae8e66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd id: 1\n"
     ]
    }
   ],
   "source": [
    "# Each RDD gets a unique ID\n",
    "print(f'rdd id: {rdd.id()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7521a782-5698-415b-aade-5cb0f7c6da05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "My first rdd PythonRDD[1] at collect at /tmp/ipykernel_19993/1815227390.py:8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can name each newly created RDD using the setName() method\n",
    "rdd.setName('My first rdd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c9b566c-c2f8-4db9-8403-083c9af4b117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(4) My first rdd PythonRDD[1] at collect at /tmp/ipykernel_19993/1815227390.py:8 []\\n |  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289 []'\n"
     ]
    }
   ],
   "source": [
    "# Let's view the lineage (the set of transformations) of the RDD using toDebugString()\n",
    "print(rdd.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbc65f1e-2a19-48f0-ab35-e4a219be4318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how many partitions the RDD will be split into by using the getNumPartitions()\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b946187-aca6-4558-ac60-ed7138fe7e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea362e0-8f19-4ab0-9c08-a61ca4e329ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transformations vs Actions\n",
    "There are two types of operations that you can perform on an RDD: Transformations and Actions. \n",
    "- **Transformations**. Transformations are applied on RDDs and produce other RDDs. Additionally, Transformations are lazily evaluated, meaning that, they are not computed until an action is performed. Some common transformations are [map], and [filter].\n",
    "- **Actions**. Actions do not return RDDs anymore. Actions do set in motion the sequence of transformations required to produce the result. Once the computation is done you get the result as output. Some common actions are [collect], [count], [reduce], and [take].\n",
    "\n",
    "[map]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html?highlight=map\n",
    "[filter]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.filter.html?highlight=filter\n",
    "[reduce]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduce.html?highlight=reduce\n",
    "[collect]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.collect.html?highlight=collect\n",
    "[count]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.count.html?highlight=count\n",
    "[take]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.take.html?highlight=take\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8afe15-d38d-43da-875b-0d05158d3b01",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The **map** Transformation\n",
    "The ```map(f)``` transformation is the most common Spark transformation: it applies a function ```f``` to each item in the dataset and produces the resulting dataset. When you execute [map] on a dataset, it initiates a **stage**. A stage is a group of tasks that all perform the same computation but on different input data. One task is launched for each partition of the dataset. A task represents a unit of execution that runs on a single machine. In the example below, the dataset is divided into four partitions (utilizing three workers), resulting in the launch of four ```map()``` tasks.\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/tasks.png\" alt=\"drawing\" width=\"500\"/> <img src=\"http://spark-mooc.github.io/web-assets/images/map.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "[map]: https://spark.apache.org/docs/3.2.1/api/python/reference/api/pyspark.RDD.map.html?highlight=map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d38221cf-6316-4d30-83c4-b02d5cbafdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sub function to subtract 1\n",
    "def sub(value): return (value - 1)\n",
    "\n",
    "# Apply sub\n",
    "rdd2 = rdd.map(sub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fe6dc7-d5a7-4758-af7e-9e0b7f627e70",
   "metadata": {},
   "source": [
    "We have applied the ```sub()``` transformation to the RDD. Consequently, each element in the RDD is decremented by ```1```. However, no computation has started yet. As mentioned earlier, Spark employs lazy evaluation, meaning that actual computation begins only when an action is required. Let's consider one such operation that triggers the computation: the [collect] action.\n",
    "\n",
    "It's important to approach calling ```.collect()``` with caution, as it brings the requested data into your machine's memory. If, for example, you inadvertently request several gigabytes of data, your machine may crash as its memory becomes saturated, potentially leading to the loss of several hours' worth of work.\n",
    "\n",
    "[collect]: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.collect.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5d905c7-5297-4e1d-8184-ddb10e7ad2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n"
     ]
    }
   ],
   "source": [
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16177199-23bc-4e92-b0a7-7b61b72846fd",
   "metadata": {},
   "source": [
    "### The **filter** Transformation\n",
    "The [filter] transformation is unsurprisingly used to filter() elements of an RDD. It operates similarly to the map() transformation by applying a function to all elements in an RDD. For example, consider a function `f` that returns `True` if the input is odd and `False` otherwise. If you have an RDD containing a list of numbers and apply `f` to the RDD, you will obtain another RDD containing only the odd numbers.\n",
    "\n",
    "Let's try this out.\n",
    " \n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/tasks.png\" alt=\"drawing\" width=\"500\"/><img src=\"http://spark-mooc.github.io/web-assets/images/filter.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "[filter]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.filter.html?highlight=filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8ca141f-f7e8-49c2-9176-667b399e4a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "### keep only less than 10 numbers\n",
    "\n",
    "def isLessThan10(x): return x < 10\n",
    "\n",
    "result = rdd.filter(isLessThan10).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb395f66-fbdb-4488-b22a-8b6315bc4bbd",
   "metadata": {},
   "source": [
    "### The **reduce** function\n",
    "Let's explore the ```reduce``` function outside of the Spark framework. The [reduce] function can be a bit trickier to grasp. \n",
    "\n",
    "```reduce(f)``` takes a function, which we'll call ```f```. This time, ```f(*,*)``` accepts two arguments: the first one, referred to as the **accumulator**, and the second one, known as the **current value**. For now, let's set aside the fact that data are stored in RDDs. Imagine you have a list of 100 numbers: [1, 2, 3, ..., 100]. The function ```f``` is called for every element of our list:\n",
    "\n",
    "1) The first time ```f``` is called, the accumulator takes the first value of the list (```1``` in our example), while the second argument of ```f``` is the second number in the list (```2``` in our example).\n",
    "2) The second time ```f``` is called, the accumulator takes the value of the output from step 1). Meanwhile, the second argument of ```f``` is the third number in the list (```3``` in our example).\n",
    "3) The third time ```f``` is called, the accumulator takes the value of the output from step 2). Meanwhile, the second argument of ```f``` is the fourth number in the list (```4``` in our example).\n",
    "4) And so on...\n",
    "99) The 99th time ```f``` is called, the accumulator takes the value of the output from step 98). Meanwhile, the second argument of ```f``` is the 100th number in the list (```100``` in our example).\n",
    "\n",
    "In practice, the [reduce] function applies a function to every element of the list while accumulating results.\n",
    "\n",
    "[reduce]:https://docs.python.org/3/library/functools.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3568f541-cee6-45d6-945a-ea5133235759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce ### not using spark\n",
    "\n",
    "def sumAll(acc, x): return acc + x\n",
    "\n",
    "result = reduce(sumAll, range(30))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c142d56f-6a2a-46dc-94d1-f0ff93708f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29]\n"
     ]
    }
   ],
   "source": [
    "# Now let us accumulate only odd numbers. \n",
    "# You do not need to understand this function too deeply.\n",
    "# Just keep in mind that the reduce function is a lot more flexible than it appears.\n",
    "\n",
    "def isOdd(x):\n",
    "    return True if x % 2 == 1 else False\n",
    "\n",
    "def AccumulateOdds(acc, x):\n",
    "    if type(acc) != list: return [e for e in (acc,x) if isOdd(e)]\n",
    "    else: return acc + [x] if isOdd(x) else acc\n",
    "                \n",
    "print(reduce(AccumulateOdds,list(range(30))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487cafb8-7634-4a5e-af23-27797aa8d02a",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "* (★) given a list `[1,2,3,4,5]` multiply all the elements togheter using `reduce` from functools (result: `120`).\n",
    "* (★) given a list `[1,2,3,4,5]` sum all the elements greater than `2` using `reduce`  from functools (result: `12`).\n",
    "* (★) given a list `[1,2,3,4,5]` compute (sum,no.elements) using `reduce` function  from functools (result: `(15,5)`).\n",
    "* (★★) given a list `[1,2,3,4,5]` filter even elements using the `reduce` function  from functools (result: `[1,3,5]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f78763-4f85-44d9-968a-8d1314507adb",
   "metadata": {},
   "source": [
    "### The **reduce** action\n",
    "The [reduce] action in Spark differs slightly from the functools `reduce`, but the main concepts remain valid. Once again, the `reduce(f)` action takes a function that is applied to every element in the RDD. This function, accepts two arguments. The first argument accumulates the results and is fed back to successive `f` calls (like for the python's `reduce`). However, the second argument can also act as an accumulator. \n",
    "\n",
    "For simple reducers like our `sumAll`, this does not make much difference. However, for reducers like ```AccumulateOdds```, it can make a significant difference.\n",
    "\n",
    "[reduce]:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduce.html?highlight=reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e4bd604-94cc-436d-9422-396ae3dd754c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435\n"
     ]
    }
   ],
   "source": [
    "# again a function that sums the whole elements\n",
    "result = rdd.reduce(sumAll)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1fe35f-61cd-4508-b27d-f79e7bc1c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us accumulate only odd numbers. \n",
    "# You do not need to understand this function too deeply.\n",
    "# Just keep in mind that the reduce function is a lot more flexible than it appears.\n",
    "\n",
    "def AccumulateOddsSpark(x, y):\n",
    "    if type(x) != list and type(y) != list: return list(filter(isOdd, [x,y]))\n",
    "    if type(x) == list and type(y) != list: return x + [y] if isOdd(y) else x\n",
    "    if type(x) != list and type(y) == list: return y + [x] if isOdd(x) else y\n",
    "    if type(x) == list and type(y) == list: return x + y\n",
    "\n",
    "result = sc.parallelize(range(20), 4).reduce(AccumulateOddsSpark)\n",
    "print(result)\n",
    "\n",
    "# A little side note. There is a drastic difference from using the .filter(isOdd).collect() and using this reducer.\n",
    "# In the first case the spark context is responsible for gathering all the filtered results. \n",
    "# Instead, in this case, we are directly gathering results oursevels. \n",
    "# Of course, this can lead to inefficiencies and it is quite error prone.\n",
    "# Again, this is to show the flexibility of the reduce function.\n",
    "# If you can obtain your result using map and filter, just use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9316dca7-178b-4741-bd4d-eea625a715c2",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "* (★) given a list `[1,2,3,4,5]` multiply all the elements togheter using `reduce` from spark (result: `120`).\n",
    "* (★) given a list `[1,2,3,4,5]` sum all the elements greater than `2` using `reduce` from spark (result: `12`).\n",
    "* (★) given a list `[1,2,3,4,5]` compute the (sum, no.element) using `reduce` from spark (result: `(15,5)`).\n",
    "* (★★) given a list `[1,2,3,4,5]` filter even elements using the `reduce` function from spark (result: `[1,3,5]`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1844ff1-4492-40f2-9899-518f117ee277",
   "metadata": {},
   "source": [
    "### The **Count** Action\n",
    "\n",
    "One of the most basic actions we can run is the [count] method, which counts the number of elements in an RDD.\n",
    "\n",
    "Each task counts the entries in its partition and sends the result to your SparkContext, which then aggregates all of the counts. The figure below illustrates what would happen if we executed `count` on a small example dataset with just four partitions.\n",
    "\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/count.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "\n",
    "[count]: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.count.html?highlight=count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1ca040-e516-49d3-86c9-a6567a5137d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab15f99-5860-4c6e-b259-15ac288b250f",
   "metadata": {},
   "source": [
    "## Additional Actions\n",
    "\n",
    "* [first]: `first()` returns the first available elements from the RDD. it depends on how the RDD is partitioned.\n",
    "* [take]: `take(num)` returns the first `num` elements from the RDD. it depends on how the RDD is partitioned.\n",
    "* [top]: `top(num, key=None)` returns the first (in descending order) `num` elements according to the `key`. \n",
    "* [takeOrdered]: `takeOrdered(num, key=None)`returns the first (in ascending order) `num` elements according to the `key`.\n",
    "* [takeSample]: `takeSample(withReplacement, num, seed=None)` it randomly select `num` elements from the RDD. If `withReplacement=True` then it can return the same elements multiple times. Using two times the same `seed` yields the same results.\n",
    "* [countByValue]: `countByValue()` returns the count of each unique value in this RDD as a dictionary of (value, count) pairs.\n",
    "\n",
    "[first]: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.first.html?highlight=first#pyspark.RDD.first \n",
    "[take]: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.take.html?highlight=take#pyspark.RDD.take\n",
    "[top]: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.top.html?highlight=top#pyspark.RDD.top\n",
    "[takeOrdered]: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.takeOrdered.html?highlight=takeordered#pyspark.RDD.takeOrdered\n",
    "[takeSample]: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.takeSample.html?highlight=takesample#pyspark.RDD.takeSample\n",
    "[countByValue]: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.countByValue.html?highlight=countbyvalue#pyspark.RDD.countByValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d9575-a1a5-4125-a925-6363fdf72259",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"rdd.first()                     = {rdd.first()}\")\n",
    "print(f\"rdd.take(5)                    = {rdd.take(5)}\")\n",
    "print(f\"rdd.top(5, lambda x:x)         = {rdd.top(5, lambda x:x)}\")\n",
    "print(f\"rdd.takeOrdered(5, lambda x:x) = {rdd.takeOrdered(5, lambda x:x)}\")\n",
    "print(f\"rdd.takeSample(True, 5, 14)    = {rdd.takeSample(True, 5, 14)}\")\n",
    "print(f\"rdd.countByValue()             = {rdd.countByValue()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69e8751-0d13-4d0e-8776-7fa71d1c6b0c",
   "metadata": {},
   "source": [
    "## Additional Transformations\n",
    "* [flatMap]: The `flatMap(f)` transformation returns a new RDD by applying the function `f` to all elements of the RDD and then flattening the results.\n",
    "* [groupByKey]: The `groupByKey()` transformation groups the values for each key in the RDD into a single sequence. This operation operates on pair RDDs.\n",
    "* [reduceByKey]: The `reduceByKey(func)` transformation merges the values for each key using an associative and commutative reduce function.\n",
    "\n",
    "Both of these transformations ([groupByKey] and [reduceByKey]) operate on pair RDDs, where each element is a tuple (key, value). For example, `sc.parallelize([('a', 1), ('a', 2), ('b', 1)])` would create a pair RDD with keys 'a', 'a', 'b', and values 1, 2, 1 respectively.\n",
    "\n",
    "The `reduceByKey()` transformation gathers pairs with the same key and applies a **reduce** function to the associated values. It operates by applying the function within each partition first, and then across partitions.\n",
    "\n",
    "While both `groupByKey()` and `reduceByKey()` can often solve the same problem and produce the same answer, `reduceByKey()` is more efficient for large distributed datasets. This is because Spark can combine output with a common key on each partition before shuffling the data across nodes. Only use `groupByKey()` if reducing the data before redistribution (**shuffling**) would not benefit the operation.\n",
    "\n",
    "To understand how `reduceByKey` works, observe the diagram below. Pairs with the same key on the same machine are combined before data shuffling occurs. Conversely, when using `groupByKey()`, all key-value pairs are shuffled, resulting in unnecessary data transfer over the network.\n",
    "\n",
    "When Spark needs to shuffle data, it calls a partitioning function on the key of the pair to determine which machine to shuffle the pair to. If more data is shuffled onto a single executor machine than can fit in memory, Spark spills data to disk, impacting performance severely. This situation should be avoided to maintain optimal performance.\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/group_by.png\" alt=\"drawing\" width=\"500\"/> <img src=\"http://spark-mooc.github.io/web-assets/images/reduce_by.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "[flatMap]: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.flatMap.html?highlight=flatmap#pyspark.RDD.flatMap\n",
    "[groupByKey]: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.groupByKey.html?highlight=groupbykey#pyspark.RDD.groupByKey\n",
    "[reduceByKey]: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.reduceByKey.html?highlight=reducebykey#pyspark.RDD.reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b42d336-ad51-47bc-94ac-d7f15ef893eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatMap    : [('a', 1), ('a', 1), ('a', 2), ('a', 2), ('b', 1), ('b', 1)]\n",
      "groupByKey : [('a', [1, 2]), ('b', [1])]\n",
      "reduceByKey: [('a', 3), ('b', 1)]\n"
     ]
    }
   ],
   "source": [
    "pair_rdd = sc.parallelize([('a', 1), ('a', 2), ('b', 1)])\n",
    "print(\"flatMap    :\", pair_rdd.flatMap(lambda x: [x,x]).collect())\n",
    "print(\"groupByKey :\", pair_rdd.groupByKey().map(lambda x:(x[0],list(x[1]))).collect())\n",
    "print(\"reduceByKey:\", pair_rdd.reduceByKey(lambda x,y:x+y).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5024bb18-3cc6-4297-a385-8d63770d55dc",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "You will work on this pair rdd (name, price): \n",
    "```\n",
    "('alpha', '9/2/22' , '932$'), \n",
    "('alpha', '10/2/22', '904$'), \n",
    "('alpha', '11/2/22', '806$'),\n",
    "('beta' , '9/2/22' , '2831$'), \n",
    "('beta' , '10/2/22', '2732$'), \n",
    "('beta' , '11/2/22', '2685$'),\n",
    "('gamma', '9/2/22' , '312$'), \n",
    "('gamma', '10/2/22', '301$'), \n",
    "('gamma', '11/2/22', '285$')\n",
    "```\n",
    "\n",
    "* (★) compute the total price. (result: `11788`) \n",
    "* (★★) compute the average per name. (result: `('alpha', 880.666), ('beta', 2749.333), ('gamma',299.333)`) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed59db59-1589-4f7a-8a4c-fc9a40a0df14",
   "metadata": {},
   "source": [
    "## RDDs Memory Management\n",
    "\n",
    "For efficiency, Spark stores RDDs in RAM memory, allowing for quick access to the data. However, memory is limited, so if you attempt to keep too many RDDs in memory, Spark will automatically evict RDDs from memory to make space for new ones. If you later reference one of the evicted RDDs, Spark will recreate it automatically, but this process takes time.\n",
    "\n",
    "To ensure that frequently used RDDs remain in memory, you can use the `cache()` operation to instruct Spark to keep the RDD in memory. However, the RDD will only be cached after you trigger an action on it, such as `collect()`. It's essential to note that if you [cache] too many RDDs and Spark exhausts its memory, it will evict the least recently used (LRU) RDD first. Again, accessing the RDD will trigger automatic recreation.\n",
    "\n",
    "You can verify if an RDD is cached by using the `is_cached` attribute, and you can monitor your cached RDDs in the \"Storage\" section of the Spark web UI. Clicking on the RDD's name provides more details about its storage location.\n",
    "\n",
    "[cache]: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.cache.html?highlight=cache#pyspark.RDD.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55906d78-c1df-40e5-8308-b5d4bd87d3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rdd = sc.parallelize(range(10))\n",
    "new_rdd.setName(\"MyRDD\")\n",
    "print(\"before .cache()              :\", new_rdd.is_cached)\n",
    "new_rdd.cache()\n",
    "print(\"after  .cache()              :\", new_rdd.is_cached)\n",
    "new_rdd.collect()\n",
    "print(\"after  .cache() and an action:\", new_rdd.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4c75fe-960e-4b3e-94dc-8833a15d0287",
   "metadata": {},
   "source": [
    "Spark automatically manages the RDDs cached in memory and will save them to disk if it runs out of memory. For efficiency, once you are finished using an RDD, you can optionally tell Spark to stop caching it in memory by using the RDD's `unpersist()` method to inform Spark that you no longer need the RDD in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8d3de5-e4bc-490e-a245-265f5bee61c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rdd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f556ca-a678-4ff8-83d6-9662d072ad4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
