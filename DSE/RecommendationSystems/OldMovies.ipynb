{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjZgUrnnjkND"
   },
   "source": [
    "# Predicting movie ratings\n",
    "\n",
    "One of the most common uses of big data is to predict what users want. This allows Google to show\n",
    "you relevant ads, Amazon to recommend relevant products, and Netflix to recommend movies that you\n",
    "might like. This lab will demonstrate how we can use Apache Spark to recommend movies to a user.\n",
    "We will start with some basic techniques, and then use the mllib library's Alternating Least Squares\n",
    "method to make more sophisticated predictions.\n",
    "\n",
    "## Tools\n",
    "This assignment is based on Python 3.7, Spark 1.4.1, the pySpark API, and the mllib library.\n",
    "\n",
    "\n",
    "## File\n",
    "For this lab, we will use a subset dataset of ~1,000,000 ratings from the [movielens 10M stable benchmark rating dataset](http://grouplens.org/datasets/movielens/). However, the same code you write will work for the full\n",
    "dataset, or their latest dataset of ~25 million ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Q9SVLoNjkNE",
    "outputId": "a401f006-3133-480d-a991-de7b87368689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "movies_filename = '/content/drive/MyDrive/movies.csv'\n",
    "ratings_filename = '/content/drive/MyDrive/ratings.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bp5iguy2jkNE"
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "We read in each of the files and create an RDD consisting of parsed lines.\n",
    "Each line in the ratings dataset (`ratings.csv`) is formatted as:\n",
    "\n",
    "`UserID::MovieID::Rating::Timestamp`\n",
    "\n",
    "Each line in the movies (`movies.csv`) dataset is formatted as:\n",
    "\n",
    "`MovieID::Title::Genres`\n",
    "\n",
    "The `Genres` field has the format\n",
    "\n",
    "`Genres1|Genres2|Genres3|...`\n",
    "\n",
    "The format of these files is uniform and simple, so we can easily parse them using python:\n",
    "- For each line in the ratings dataset, we create a tuple of (UserID, MovieID, Rating). We drop\n",
    "the timestamp because we do not need it for this exercise.\n",
    "- For each line in the movies dataset, we create a tuple of (MovieID, Title). We drop the Genres\n",
    "because we do not need them for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cHccL-a0jkNF",
    "outputId": "a4a5310b-a9bf-4685-a554-6775187e184a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.1.1)\n",
      "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YfV9zB5bjkNF",
    "outputId": "b9066f31-f052-4dbc-af0e-4aa28264f734"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10000054 ratings and 10681 movies in the datasets\n",
      "Ratings: [(1, 122, 5.0), (1, 185, 5.0), (1, 231, 5.0)]\n",
      "Movies: [(1, 'Toy Story (1995)'), (2, 'Jumanji (1995)'), (3, 'Grumpier Old Men (1995)')]\n"
     ]
    }
   ],
   "source": [
    "num_partitions = 2\n",
    "rawRatings = sc.textFile(ratings_filename).repartition(num_partitions)\n",
    "rawMovies = sc.textFile(movies_filename)\n",
    "\n",
    "def get_ratings_tuple(entry):\n",
    "    \"\"\" Parse a line in the ratings dataset\n",
    "    Args:\n",
    "        entry (str): a line in the ratings dataset in the form of UserID::MovieID::Rating::Timestamp\n",
    "    Returns:\n",
    "        tuple: (UserID, MovieID, Rating)\n",
    "    \"\"\"\n",
    "    \n",
    "    items = entry.split('::')\n",
    "    return int(items[0]), int(items[1]), float(items[2])\n",
    "\n",
    "\n",
    "def get_movie_tuple(entry):\n",
    "    \"\"\" Parse a line in the movies dataset\n",
    "    Args:\n",
    "        entry (str): a line in the movies dataset in the form of MovieID::Title::Genres\n",
    "    Returns:\n",
    "        tuple: (MovieID, Title)\n",
    "    \"\"\"\n",
    "    \n",
    "    items = entry.split('::')\n",
    "    return int(items[0]), items[1]\n",
    "\n",
    "\n",
    "ratingsRDD = rawRatings.map(get_ratings_tuple).cache()\n",
    "moviesRDD = rawMovies.map(get_movie_tuple).cache()\n",
    "\n",
    "ratingsCount = ratingsRDD.count()\n",
    "moviesCount = moviesRDD.count()\n",
    "\n",
    "print('There are {} ratings and {} movies in the datasets'.format(ratingsCount, moviesCount))\n",
    "print('Ratings: {}'.format(ratingsRDD.take(3)))\n",
    "print('Movies: {}'.format(moviesRDD.take(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8ZaGw26jkNG"
   },
   "source": [
    "We will be examining subsets of the tuples we create (e.g., the top rated movies by users). Whenever we examine only a subset of a large dataset, there is the potential that the result will depend on the order we perform operations, such as joins, or how the data is partitioned across the workers. What we want to guarantee is that we always see the same results for a subset, independent of how we manipulate or store the data.\n",
    "\n",
    "We can do that by sorting before we examine a subset. You might think that the most obvious choice when dealing with an RDD of tuples would be to use the `sortByKey()` method. However this choice is problematic, as we can still end up with different results if the key is not unique.\n",
    "\n",
    "Note: It is important to use the [unicode type](https://docs.python.org/2/howto/unicode.html#the-unicode-type) instead of the `string` type as the titles are in unicode characters.\n",
    "\n",
    "Consider the following example, and note that while the sets are equal, the printed lists are usually in different order by value, *although they may randomly match up from time to time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZRnKaqMgjkNG",
    "outputId": "bffe38c6-d419-4e4c-fc2c-d4abab7228fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'alpha'), (1, 'epsilon'), (1, 'delta'), (2, 'alpha'), (2, 'beta'), (3, 'alpha')]\n",
      "[(1, 'delta'), (1, 'epsilon'), (1, 'alpha'), (2, 'alpha'), (2, 'beta'), (3, 'alpha')]\n"
     ]
    }
   ],
   "source": [
    "tmp1 = [(1, u'alpha'), (2, u'alpha'), (2, u'beta'), (3, u'alpha'), (1, u'epsilon'), (1, u'delta')]\n",
    "tmp2 = [(1, u'delta'), (2, u'alpha'), (2, u'beta'), (3, u'alpha'), (1, u'epsilon'), (1, u'alpha')]\n",
    "\n",
    "oneRDD = sc.parallelize(tmp1)\n",
    "twoRDD = sc.parallelize(tmp2)\n",
    "oneSorted = oneRDD.sortByKey(True).collect()\n",
    "twoSorted = twoRDD.sortByKey(True).collect()\n",
    "print(oneSorted)\n",
    "print(twoSorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9B1jPpgjkNH"
   },
   "source": [
    "Even though the two lists contain identical tuples, the difference in ordering *sometimes* yields a different ordering for the sorted RDD (try running the cell repeatedly and see if the results change or the assertion fails). If we only examined the first two elements of the RDD (e.g., using `take(2)`), then we would observe different answers - **that is a really bad outcome as we want identical input data to always yield identical output**. A better technique is to sort the RDD by *both the key and value*, which we can do by combining the key and value into a single string and then sorting on that string. Since the key is an integer and the value is a unicode string, we can use a function to combine them into a single unicode string (e.g., `unicode('%.3f' % key) + ' ' + value`) before sorting the RDD using [sortBy()][sortby].\n",
    "[sortby]: https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "kk2AUbeVjkNH",
    "outputId": "65a8160d-1222-40af-c9d5-ac9fd3af93b5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1.433'"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{:.3f}'.format(1.43324543522)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XzzpxUiwjkNI",
    "outputId": "eb06c18b-85a6-48b5-d89e-0d05b29b1ddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'alpha'), (1, 'delta'), (1, 'epsilon'), (2, 'alpha'), (2, 'beta'), (3, 'alpha')]\n",
      "[(1, 'alpha'), (1, 'delta'), (1, 'epsilon'), (2, 'alpha'), (2, 'beta'), (3, 'alpha')]\n"
     ]
    }
   ],
   "source": [
    "def sortFunction(tuple):\n",
    "    \"\"\" Construct the sort string (does not perform actual sorting)\n",
    "    Args:\n",
    "        tuple: (rating, MovieName)\n",
    "    Returns:\n",
    "        sortString: the value to sort with, 'rating MovieName'\n",
    "    \"\"\"\n",
    "    #key = unicode('%.3f' % tuple[0])\n",
    "    key = '{:.3f}'.format(tuple[0])\n",
    "    \n",
    "    value = tuple[1]\n",
    "    return (key + ' ' + value)\n",
    "\n",
    "print(oneRDD.sortBy(sortFunction, ascending = True).collect())\n",
    "print(twoRDD.sortBy(sortFunction, ascending = True).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHX7NvEnjkNI"
   },
   "source": [
    "If we just want to look at the first few elements of the RDD in sorted order, we can use the [takeOrdered][takeordered] method with the `sortFunction` we defined.\n",
    "[takeordered]: https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.takeOrdered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mWqAbJFmjkNJ",
    "outputId": "ab5cf4a1-01ba-4a24-f37a-3b303e45df96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one is [(1, 'alpha'), (1, 'delta'), (1, 'epsilon')]\n",
      "two is [(1, 'alpha'), (1, 'delta'), (1, 'epsilon')]\n"
     ]
    }
   ],
   "source": [
    "oneSorted1 = oneRDD.takeOrdered(3,key=sortFunction)\n",
    "twoSorted1 = twoRDD.takeOrdered(3,key=sortFunction)\n",
    "print('one is {}'.format(oneSorted1))\n",
    "print('two is {}'.format(twoSorted1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PH4ZH_XVjkNJ"
   },
   "source": [
    "## 1. Basic Recommendations\n",
    "\n",
    "One way to recommend movies is to always recommend the movies with the highest average rating. In this part, we will use Spark to find the name, number of ratings, and the average rating of the 20 movies with the highest average rating and more than 500 reviews. We want to filter our movies with high ratings but fewer than or equal to 500 reviews because movies with few reviews may not have broad appeal to everyone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grE2m9rLjkNJ"
   },
   "source": [
    "### 1.1 Number of Ratings and Average Ratings for a Movie\n",
    "\n",
    "We implement a helper function `getCountsAndAverages` that takes a single tuple of (MovieID, (Rating1, Rating2, Rating3, ...)) and returns a tuple of (MovieID, (number of ratings, averageRating)). For example, given the tuple `(100, (10.0, 20.0, 30.0))`, your function should return `(100, (3, 20.0))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "90vE8-6ijkNK"
   },
   "outputs": [],
   "source": [
    "def getCountsAndAverages(IDandRatingsTuple):\n",
    "    \"\"\" Calculate average rating\n",
    "    Args:\n",
    "        IDandRatingsTuple: a single tuple of (MovieID, (Rating1, Rating2, Rating3, ...))\n",
    "    Returns:\n",
    "        tuple: a tuple of (MovieID, (number of ratings, averageRating))\n",
    "    \"\"\"\n",
    "    ratings = IDandRatingsTuple[1]\n",
    "    num_ratings = len(ratings)\n",
    "    return (IDandRatingsTuple[0], (num_ratings, float(sum(ratings)) / num_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mAoJ6PhWjkNK"
   },
   "outputs": [],
   "source": [
    "assert(getCountsAndAverages((1, (1, 2, 3, 4))) == (1, (4, 2.5)))\n",
    "assert(getCountsAndAverages((100, (10.0, 20.0, 30.0))) == (100, (3, 20.0)))\n",
    "assert(getCountsAndAverages((110, range(20))) == (110, (20, 9.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4evOfpOvjkNL"
   },
   "source": [
    "### 1.2 Movies with Highest Average Ratings\n",
    "\n",
    "Now that we have a way to calculate the average ratings, we will use the `getCountsAndAverages` helper function with Spark to determine movies with highest average ratings:\n",
    "\n",
    "* the `ratingsRDD` contains tuples of the form (UserID, MovieID, Rating). From `ratingsRDD` we create an RDD with tuples of the form (MovieID, Python iterable of Ratings for that MovieID);\n",
    "* using `movieIDsWithRatingsRDD` and the `getCountsAndAverages()` helper function, we compute the number of ratings and average rating for each movie to yield tuples of the form (MovieID, (number of ratings, average rating));\n",
    "* we want to see movie names, instead of movie IDs, thus we apply RDD transformations to `moviesRDD`, using `movieIDsWithAvgRatingsRDD` to get the movie names for `movieIDsWithAvgRatingsRDD`, yielding tuples of the form (average rating, movie name, number of ratings). This set of transformations will yield an RDD of the form: `[(1.0, u'Autopsy (Macchie Solari) (1975)', 1), (1.0, u'Better Living (1998)', 1), (1.0, u'Big Squeeze, The (1996)', 3)]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HnjL9oA3jkNL",
    "outputId": "bf8d2288-ec9e-4b2b-fb27-d25071c612be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieIDsWithRatingsRDD: [(122, <pyspark.resultiterable.ResultIterable object at 0x7f336e6ce9d0>), (292, <pyspark.resultiterable.ResultIterable object at 0x7f336e6cebd0>), (316, <pyspark.resultiterable.ResultIterable object at 0x7f336e6ce690>)]\n",
      "\n",
      "movieIDsWithAvgRatingsRDD: [(122, (2412, 2.861318407960199)), (292, (16075, 3.4184136858475895)), (316, (18925, 3.3493527080581242))]\n",
      "\n",
      "movieNameWithAvgRatingsRDD: [(2.860544217687075, 'Waiting to Exhale (1995)', 1764), (3.131256952169077, 'Tom and Huck (1995)', 899), (2.5671971706454464, 'Dracula: Dead and Loving It (1995)', 2262)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From ratingsRDD with tuples of (UserID, MovieID, Rating) create an RDD with tuples of\n",
    "# the (MovieID, iterable of Ratings for that MovieID)\n",
    "movieIDsWithRatingsRDD = (ratingsRDD\n",
    "                          .map(lambda r: (r[1], r[2]))\n",
    "                          .groupByKey()\n",
    "                         )\n",
    "print('movieIDsWithRatingsRDD: {}\\n'.format(movieIDsWithRatingsRDD.take(3)))\n",
    "\n",
    "# Using `movieIDsWithRatingsRDD`, compute the number of ratings and average rating for each movie to\n",
    "# yield tuples of the form (MovieID, (number of ratings, average rating))\n",
    "movieIDsWithAvgRatingsRDD = movieIDsWithRatingsRDD.map(getCountsAndAverages)\n",
    "print('movieIDsWithAvgRatingsRDD: {}\\n'.format(movieIDsWithAvgRatingsRDD.take(3)))\n",
    "\n",
    "# To `movieIDsWithAvgRatingsRDD`, apply RDD transformations that use `moviesRDD` to get the movie\n",
    "# names for `movieIDsWithAvgRatingsRDD`, yielding tuples of the form\n",
    "# (average rating, movie name, number of ratings)\n",
    "movieNameWithAvgRatingsRDD = (moviesRDD\n",
    "                              .join(movieIDsWithAvgRatingsRDD)\n",
    "                              .map(lambda m: (m[1][1][1], m[1][0], m[1][1][0]))#*\n",
    "                             )\n",
    "print('movieNameWithAvgRatingsRDD: {}\\n'.format(movieNameWithAvgRatingsRDD.take(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CcbQvjNkqx9"
   },
   "source": [
    "*Example of join here (pay attention to ordering!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tBxqgRZBkvX7",
    "outputId": "ea4413fe-c088-4876-e314-a98a40a0fd2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ('Toy Story (1995)', (26449, 3.928768573481039)))]"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviesRDD.join(movieIDsWithAvgRatingsRDD).takeOrdered(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2i6yhwujkNM"
   },
   "source": [
    "### 1.3 Movies with Highest Average Ratings and more than 500 reviews\n",
    "\n",
    "Now that we have an RDD of the movies with averge ratings, we can use Spark to determine the 20 movies with highest average ratings and more than 500 reviews.\n",
    "\n",
    "We apply a single RDD transformation to `movieNameWithAvgRatingsRDD` to limit the results to movies with ratings from more than 500 people. We then use the `sortFunction()` helper function to sort by the average rating to get the movies in order of their rating (highest rating first). You will end up with an RDD of the form: `[(4.5349264705882355, u'Shawshank Redemption, The (1994)', 1088), (4.515798462852263, u\"Schindler's List (1993)\", 1171), (4.512893982808023, u'Godfather, The (1972)', 1047)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ngFZX9SNjkNO",
    "outputId": "59573a87-6a63-4f38-f560-7e608be279c4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies with highest ratings: [(4.457238321660348, 'Shawshank Redemption, The (1994)', 31126), (4.415085293227011, 'Godfather, The (1972)', 19814), (4.367142322253193, 'Usual Suspects, The (1995)', 24037), (4.363482949916592, \"Schindler's List (1993)\", 25777), (4.321966205837174, 'Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)', 3255), (4.319740945070761, 'Casablanca (1942)', 12507), (4.316543909348442, 'Rear Window (1954)', 8825), (4.315439034540158, 'Double Indemnity (1944)', 2403), (4.313629402756509, 'Third Man, The (1949)', 3265), (4.314119283602851, 'Seven Samurai (Shichinin no samurai) (1954)', 5751), (4.306805399325085, 'Paths of Glory (1957)', 1778), (4.303215119343423, 'Godfather: Part II, The (1974)', 13281), (4.298072023101749, 'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)', 11774), (4.297154471544715, 'Lives of Others, The (Das Leben der Anderen) (2006)', 1230), (4.294842186297152, 'Dark Knight, The (2008)', 2598), (4.292379632836855, \"One Flew Over the Cuckoo's Nest (1975)\", 14435), (4.277613703484938, 'Yojimbo (1961)', 1693), (4.275907715582451, 'Wallace & Gromit: The Wrong Trousers (1993)', 7932), (4.274952621604548, 'Wallace & Gromit: A Close Shave (1995)', 6332), (4.267953020134228, 'Big Sleep, The (1946)', 2980)]\n"
     ]
    }
   ],
   "source": [
    "# Apply an RDD transformation to `movieNameWithAvgRatingsRDD` to limit the results to movies with\n",
    "# ratings from more than 500 people. We then use the `sortFunction()` helper function to sort by the\n",
    "# average rating to get the movies in order of their rating (highest rating first)\n",
    "movieLimitedAndSortedByRatingRDD = (movieNameWithAvgRatingsRDD\n",
    "                                    .filter(lambda m: m[2] > 500)\n",
    "                                    .sortBy(sortFunction, ascending = False))\n",
    "print('Movies with highest ratings: {}'.format(movieLimitedAndSortedByRatingRDD.take(20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcbsRpMnjkNP"
   },
   "source": [
    "Using a threshold on the number of reviews is one way to improve the recommendations, but there are many other good ways to improve quality. For example, you could weight ratings by the number of ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INp3MrkbjkNP"
   },
   "source": [
    "## 2. Collaborative Filtering\n",
    "\n",
    "We are going to use a technique called collaborative filtering. Collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue x than to have the opinion on x of a person chosen randomly.\n",
    "\n",
    "At first, people rate different items (like videos, images, games). After that, the system is making predictions about a user's rating for an item, which the user has not rated yet. These predictions are built upon the existing ratings of other users, who have similar ratings with the active user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_I-R1ZRjkNQ"
   },
   "source": [
    "For movie recommendations, we start with a matrix whose entries are movie ratings by users.  Each column represents a user and each row represents a particular movie.\n",
    "\n",
    "Since not all users have rated all movies, we do not know all of the entries in this matrix, which is precisely why we need collaborative filtering.  For each user, we have ratings for only a subset of the movies.  With collaborative filtering, the idea is to approximate the ratings matrix by factorizing it as the product of two matrices: one that describes properties of each user, and one that describes properties of each movie.\n",
    "\n",
    "We want to select these two matrices such that the error for the users/movie pairs where we know the correct ratings is minimized.  The *Alternating Least Squares* algorithm does this by first randomly filling the users matrix with values and then optimizing the value of the movies such that the error is minimized.  Then, it holds the movies matrix constrant and optimizes the value of the user's matrix.  This alternation between which matrix to optimize is the reason for the \"alternating\" in the name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5W9xm5qjkNQ"
   },
   "source": [
    "### 2.1 Creating a Training Set\n",
    "\n",
    "Before we jump into using machine learning, we need to break up the `ratingsRDD` dataset into three pieces:\n",
    "\n",
    "* a training set (RDD), which we will use to train models,\n",
    "* a validation set (RDD), which we will use to choose the best model,\n",
    "* a test set (RDD), which we will use for estimating the predictive power of the recommender system.\n",
    "\n",
    "To randomly split the dataset into the multiple groups, we can use the pySpark `randomSplit` transformation, which takes a set of splits and and seed and returns multiple RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UidNKArajkNQ",
    "outputId": "3ea1c9d1-f13d-47c8-8665-ae3f34e94458"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 6000368, validation: 2000295, test: 1999391\n",
      "\n",
      "[(1, 122, 5.0), (1, 185, 5.0), (1, 316, 5.0)]\n",
      "[(1, 231, 5.0), (1, 292, 5.0), (1, 594, 5.0)]\n",
      "[(1, 329, 5.0), (1, 355, 5.0), (1, 356, 5.0)]\n"
     ]
    }
   ],
   "source": [
    "trainingRDD, validationRDD, testRDD = ratingsRDD.randomSplit([6, 2, 2], seed=0)\n",
    "\n",
    "print('Training: %s, validation: %s, test: %s\\n' % (trainingRDD.count(),\n",
    "                                                    validationRDD.count(),\n",
    "                                                    testRDD.count()))\n",
    "print(trainingRDD.take(3))\n",
    "print(validationRDD.take(3))\n",
    "print(testRDD.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WjT_FYQjkNR"
   },
   "source": [
    "After splitting the dataset, your training set has about 600,000 entries and the validation and test sets each have about 200,000 entries (the exact number of entries in each dataset varies slightly due to the random nature of the `randomSplit` transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaPVypKQjkNR"
   },
   "source": [
    "### 2.2 Root Mean Square Error (RMSE)\n",
    "\n",
    "In the next part, we will generate a few different models, and will need a way to decide which model is best. We will use the *Root Mean Square Error* (RMSE) or Root Mean Square Deviation (RMSD) to compute the error of each model.  RMSE is a frequently used measure of the differences between values (sample and population values) predicted by a model or an estimator and the values actually observed. The RMSD represents the sample standard deviation of the differences between predicted values and observed values. These individual differences are called residuals when the calculations are performed over the data sample that was used for estimation, and are called prediction errors when computed out-of-sample. The RMSE serves to aggregate the magnitudes of the errors in predictions for various times into a single measure of predictive power. RMSE is a good measure of accuracy, but only to compare forecasting errors of different models for a particular variable and not between variables, as it is scale-dependent.\n",
    "\n",
    "As a first step we write a function to compute the sum of squared error given `predictedRDD` and `actualRDD` RDDs. Both RDDs consist of tuples of the form (UserID, MovieID, Rating)\n",
    "\n",
    "Given two ratings RDDs, $x$ and $y$ of size $n$, we define RSME as follows:\n",
    "\n",
    "$$ RMSE = \\sqrt{\\frac{\\sum_{i = 1}^{n} (x_i - y_i)^2}{n}}$$\n",
    "\n",
    "To calculate RSME, the steps we perform are the following ones.\n",
    "\n",
    "* Transform `predictedRDD` into the tuples of the form ((UserID, MovieID), Rating). For example, tuples like `[((1, 1), 5), ((1, 2), 3), ((1, 3), 4), ((2, 1), 3), ((2, 2), 2), ((2, 3), 4)]`.\n",
    "* Transform `actualRDD` into the tuples of the form ((UserID, MovieID), Rating). For example, tuples like `[((1, 2), 3), ((1, 3), 5), ((2, 1), 5), ((2, 2), 1)]`.\n",
    "* Compute the squared error for each *matching* entry (i.e., the same (UserID, MovieID) in each RDD) in the reformatted RDDs. Note that not every (UserID, MovieID) pair will appear in both RDDs - if a pair does not appear in both RDDs, then it does not contribute to the RMSE. We will end up with an RDD with entries of the form $ (x_i - y_i)^2$.\n",
    "* Using an RDD action, we compute the total squared error: $ SE = \\sum_{i = 1}^{n} (x_i - y_i)^2 $.\n",
    "* Compute $n$ by using an RDD action, to count the number of pairs for which you computed the total squared error.\n",
    "* Using the total squared error and the number of pairs, compute the RSME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vhyn-2ItjkNS",
    "outputId": "b859550c-1d36-4c6b-ba0e-ffa3bf973393"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for test dataset (should be 1.22474487139): 1.224744871391589\n",
      "Error for test dataset2 (should be 3.16227766017): 3.1622776601683795\n",
      "Error for testActual dataset (should be 0.0): 0.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def computeError(predictedRDD, actualRDD):\n",
    "    \"\"\" Compute the root mean squared error between predicted and actual\n",
    "    Args:\n",
    "        predictedRDD: predicted ratings for each movie and each user where each entry is in the form\n",
    "                      (UserID, MovieID, Rating)\n",
    "        actualRDD: actual ratings where each entry is in the form (UserID, MovieID, Rating)\n",
    "    Returns:\n",
    "        RSME (float): computed RSME value\n",
    "    \"\"\"\n",
    "    # Transform predictedRDD into the tuples of the form ((UserID, MovieID), Rating)\n",
    "    predictedReformattedRDD = predictedRDD.map(lambda i: ((i[0], i[1]), i[2]))\n",
    "\n",
    "    # Transform actualRDD into the tuples of the form ((UserID, MovieID), Rating)\n",
    "    actualReformattedRDD = actualRDD.map(lambda i: ((i[0], i[1]), i[2]))\n",
    "\n",
    "    # Compute the squared error for each matching entry (i.e., the same (User ID, Movie ID) in each\n",
    "    # RDD) in the reformatted RDDs using RDD transformtions - do not use collect()\n",
    "    squaredErrorsRDD = (predictedReformattedRDD\n",
    "                        .join(actualReformattedRDD)\n",
    "                        .map(lambda i: math.pow(i[1][0] - i[1][1], 2))\n",
    "                       )\n",
    "\n",
    "    # Compute the total squared error - do not use collect()\n",
    "    totalError = squaredErrorsRDD.reduce(lambda a, b: a+b)\n",
    "\n",
    "    # Count the number of entries for which you computed the total squared error\n",
    "    numRatings = squaredErrorsRDD.count()\n",
    "\n",
    "    # Using the total squared error and the number of entries, compute the RSME\n",
    "    return math.pow(float(totalError) / numRatings, 0.5)\n",
    "\n",
    "\n",
    "# sc.parallelize turns a Python list into a Spark RDD.\n",
    "testPredicted = sc.parallelize([\n",
    "    (1, 1, 5),\n",
    "    (1, 2, 3),\n",
    "    (1, 3, 4),\n",
    "    (2, 1, 3),\n",
    "    (2, 2, 2),\n",
    "    (2, 3, 4)])\n",
    "testActual = sc.parallelize([\n",
    "     (1, 2, 3),\n",
    "     (1, 3, 5),\n",
    "     (2, 1, 5),\n",
    "     (2, 2, 1)])\n",
    "testPredicted2 = sc.parallelize([\n",
    "     (2, 2, 5),\n",
    "     (1, 2, 5)])\n",
    "testError = computeError(testPredicted, testActual)\n",
    "print('Error for test dataset (should be 1.22474487139): {}'.format(testError))\n",
    "\n",
    "testError2 = computeError(testPredicted2, testActual)\n",
    "print('Error for test dataset2 (should be 3.16227766017): {}'.format(testError2))\n",
    "\n",
    "testError3 = computeError(testActual, testActual)\n",
    "print('Error for testActual dataset (should be 0.0): {}'.format(testError3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "H2O3eZRcjkNS"
   },
   "outputs": [],
   "source": [
    "# TEST Root Mean Square Error (2b)\n",
    "assert(abs(testError - 1.22474487139) < 0.00000001)\n",
    "assert(abs(testError2 - 3.16227766017) < 0.00000001)\n",
    "assert(abs(testError3 - 0.0) < 0.00000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diO8bwzijkNS"
   },
   "source": [
    "### 2.3 Using ALS.train\n",
    "\n",
    "In this part, we will use the MLlib implementation of Alternating Least Squares, `ALS.train`. ALS takes a training dataset (RDD) and several parameters that control the model creation process. To determine the best values for the parameters, we will use ALS to train several models, and then we will select the best model and use the parameters from that model in the rest of this lab exercise.\n",
    "\n",
    "The process we will use for determining the best model is as follows:\n",
    "* Pick a set of model parameters. The most important parameter to `ALS.train` is the *rank*, which is the number of rows in the Users matrix or the number of columns in the Movies matrix. We will train models with ranks of 4, 8, and 12 using the `trainingRDD` dataset.\n",
    "* Create a model using `ALS.train(trainingRDD, rank, seed=seed, iterations=iterations, lambda_=regularizationParameter)` with three parameters: an RDD consisting of tuples of the form (UserID, MovieID, rating) used to train the model, an integer rank (4, 8, or 12), a number of iterations to execute (we will use 5 for the `iterations` parameter), and a regularization coefficient (we will use 0.1 for the `regularizationParameter`).\n",
    "* For the prediction step, create an input RDD, `validationForPredictRDD`, consisting of (UserID, MovieID) pairs that you extract from `validationRDD`. You will end up with an RDD of the form: `[(1, 1287), (1, 594), (1, 1270)]`\n",
    "* Using the model and `validationForPredictRDD`, we can predict rating values by calling `model.predictAll` with the `validationForPredictRDD` dataset, where `model` is the model we generated with `ALS.train`.  `predictAll` accepts an RDD with each entry in the format (userID, movieID) and outputs an RDD with each entry in the format (userID, movieID, rating).\n",
    "* valuate the quality of the model by using the `computeError` function we wrote in part 2.2 to compute the error between the predicted ratings and the actual ratings in `validationRDD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axiz33AMjkNT",
    "outputId": "5d38fa35-e9fe-4f31-e358-cbf569635ebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 4 the RMSE is 0.8274646853086821\n",
      "For rank 8 the RMSE is 0.8318119521521181\n",
      "For rank 12 the RMSE is 0.8151127619270172\n",
      "The best model was trained with rank 12\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "validationForPredictRDD = validationRDD.map(lambda i: (i[0], i[1]))\n",
    "\n",
    "seed = 5\n",
    "iterations = 5\n",
    "regularizationParameter = 0.1\n",
    "ranks = [4, 8, 12]\n",
    "errors = [0, 0, 0]\n",
    "err = 0\n",
    "tolerance = 0.02\n",
    "\n",
    "minError = float('inf')\n",
    "bestRank = -1\n",
    "bestIteration = -1\n",
    "for rank in ranks:\n",
    "    model = ALS.train(trainingRDD, rank, seed=seed, iterations=iterations,\n",
    "                      lambda_=regularizationParameter)\n",
    "    predictedRatingsRDD = model.predictAll(validationForPredictRDD)\n",
    "    error = computeError(predictedRatingsRDD, validationRDD)\n",
    "    errors[err] = error\n",
    "    err += 1\n",
    "    print('For rank {} the RMSE is {}'.format(rank, error))\n",
    "    if error < minError:\n",
    "        minError = error\n",
    "        bestRank = rank\n",
    "\n",
    "print('The best model was trained with rank {}'.format(bestRank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Il2WdznjkNT"
   },
   "source": [
    "We see that the rank 8 produces the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YIYrpuwjkNU"
   },
   "source": [
    "### 2.4 Testing Your Model\n",
    "\n",
    "So far, we used the `trainingRDD` and `validationRDD` datasets to select the best model.  Since we used these two datasets to determine what model is best, we cannot use them to test how good the model is - otherwise we would be very vulnerable to overfitting.  To decide how good our model is, we need to use the `testRDD` dataset.  We will use the `bestRank` we determined in part 2.3 to create a model for predicting the ratings for the test dataset and then we will compute the RMSE.\n",
    "\n",
    "The steps we will perform are:\n",
    "\n",
    "* Train a model, using the `trainingRDD`, `bestRank` and the parameters used in in part 2.3: `seed=seed`, `iterations=iterations`, and `lambda_=regularizationParameter.\n",
    "* For the prediction step, create an input RDD, `testForPredictingRDD`, consisting of (UserID, MovieID) pairs extracted from `testRDD`. We will end up with an RDD of the form: `[(1, 1287), (1, 594), (1, 1270)]`\n",
    "* Use `myModel.predictAll` to predict rating values for the test dataset.\n",
    "* For validation, use the `testRDD` and the `computeError` function to compute the RMSE between `testRDD` and the `predictedTestRDD` from the model.\n",
    "* Evaluate the quality of the model by using the `computeError` function written in part 2.2 to compute the error between the predicted ratings and the actual ratings in `testRDD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qVtDr2DCjkNU",
    "outputId": "578a5989-e337-45a9-b6ed-ac579ca94d9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model had a RMSE on the test set of 0.8149134493160599\n"
     ]
    }
   ],
   "source": [
    "myModel = ALS.train(trainingRDD, bestRank, seed=seed, iterations=iterations,\n",
    "                    lambda_=regularizationParameter)\n",
    "testForPredictingRDD = testRDD.map(lambda i: (i[0], i[1]))\n",
    "predictedTestRDD = myModel.predictAll(testForPredictingRDD)\n",
    "\n",
    "testRMSE = computeError(testRDD, predictedTestRDD)\n",
    "\n",
    "print('The model had a RMSE on the test set of {}'.format(testRMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-GXlAxZjkNW"
   },
   "source": [
    "We now have code to predict how users will rate movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uozsr0QwjkNW"
   },
   "source": [
    "## 3. Predictions for Ourselves\n",
    "\n",
    "The ultimate goal of this lab is to predict what movies to recommend.  In order to do that, we will first need to add ratings for ourselves to the `ratingsRDD` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qQnSXwsjkNW"
   },
   "source": [
    "### 3.1 Our Movie Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Chtc4oLjkNX"
   },
   "source": [
    "The user ID 0 is unassigned, so we will use it for our ratings. We set the variable `myUserID` to 0 for us. Next, create a new RDD `myRatingsRDD` with our ratings for at least 10 movie ratings. Each entry should be formatted as `(myUserID, movieID, rating)` (i.e., each entry should be formatted in the same way as `trainingRDD`).  As in the original dataset, ratings should be between 1 and 5 (inclusive). If you have not seen at least 10 of these movies, you can increase the parameter passed to `take()` in the above cell until there are 10 movies that you have seen (or you can also guess what your rating would be for movies you have not seen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIn8LFgcjkNX",
    "outputId": "1f9fa0c7-1ce6-4c68-d846-2de3d37ddff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My movie ratings: [(0, 527, 5), (0, 50, 5), (0, 260, 5), (0, 2762, 5), (0, 2571, 5), (0, 1278, 5), (0, 296, 3), (0, 2858, 1), (0, 2028, 1), (0, 1, 3)]\n"
     ]
    }
   ],
   "source": [
    "myUserID = 0\n",
    "\n",
    "# Note that the movie IDs are the *last* number on each line. A common error was to use the number of ratings as the movie ID.\n",
    "myRatedMovies = [\n",
    "     # The format of each line is (myUserID, movie ID, your rating)\n",
    "     # For example, to give the movie \"Star Wars: Episode IV - A New Hope (1977)\" a five rating, you would add the following line:\n",
    "     #   (myUserID, 260, 5),\n",
    "     (myUserID, 527, 5), # u\"Schindler's List (1993)\"\n",
    "     (myUserID, 50, 5), # u'Usual Suspects, The (1995)'\n",
    "     (myUserID, 260, 5), # u'Star Wars: Episode IV - A New Hope (1977)'\n",
    "     (myUserID, 2762, 5), # u'Sixth Sense, The (1999)'\n",
    "     (myUserID, 2571, 5), # u'Matrix, The (1999)'\n",
    "     (myUserID, 1278, 5), # u'Young Frankenstein (1974)'\n",
    "     (myUserID, 296, 3), # u'Pulp Fiction (1994)'\n",
    "     (myUserID, 2858, 1), # u'American Beauty (1999)'\n",
    "     (myUserID, 2028, 1), # u'Saving Private Ryan (1998)'\n",
    "     (myUserID, 1, 3), # u'Toy Story (1995)'\n",
    "    ]\n",
    "myRatingsRDD = sc.parallelize(myRatedMovies)\n",
    "print('My movie ratings: {}'.format(myRatingsRDD.take(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bOgmRuwjkNX"
   },
   "source": [
    "### 3.2 Add Your Movies to Training Dataset\n",
    "\n",
    "Now that we have ratings for ourselves, you need to add your ratings to the `training` dataset so that the model we train will incorporate our preferences. Spark's `union` transformation combines two RDDs; use `union()` to create a new training dataset that includes your ratings and the data in the original training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVm3XcqBjkNY",
    "outputId": "31b16a00-5b4d-405b-b85a-86506c233eed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset now has 10 more entries than the original training dataset\n"
     ]
    }
   ],
   "source": [
    "trainingWithMyRatingsRDD = trainingRDD.union(myRatingsRDD)\n",
    "\n",
    "print('The training dataset now has {}'\n",
    "      ' more entries than the original training dataset'.format(\n",
    "             (trainingWithMyRatingsRDD.count() - trainingRDD.count())))\n",
    "assert(trainingWithMyRatingsRDD.count() - trainingRDD.count()) == myRatingsRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DOSu-DijkNY"
   },
   "source": [
    "### 3.3 Train a Model with Your Ratings\n",
    "\n",
    "Now, we train a model with our ratings added and the parameters used in part 2.3: `bestRank`, `seed=seed`, `iterations=iterations`, and `lambda_=regularizationParameter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pfByefYRjkNY"
   },
   "outputs": [],
   "source": [
    "myRatingsModel = ALS.train(trainingWithMyRatingsRDD, bestRank, seed=seed,\n",
    "                           iterations=iterations,\n",
    "                           lambda_=regularizationParameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GESo8GymjkNZ"
   },
   "source": [
    "### 3.4 Check RMSE for the New Model with Our Ratings\n",
    "\n",
    "Compute the RMSE for this new model on the test set.\n",
    "\n",
    "* For the prediction step, we reuse `testForPredictingRDD`, consisting of (UserID, MovieID) pairs that we extracted from `testRDD`. The RDD has the form: `[(1, 1287), (1, 594), (1, 1270)]`\n",
    "* Use `myRatingsModel.predictAll()` to predict rating values for the `testForPredictingRDD` test dataset, set this as `predictedTestMyRatingsRDD`\n",
    "* For validation, use the `testRDD`and the `computeError` function to compute the RMSE between `testRDD` and the `predictedTestMyRatingsRDD` from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZHwt7VVAjkNZ",
    "outputId": "7007f34c-ebc4-49f5-9d31-eefcb1a04dd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model had a RMSE on the test set of 0.8210007283089136\n"
     ]
    }
   ],
   "source": [
    "predictedTestMyRatingsRDD = myRatingsModel.predictAll(testForPredictingRDD)\n",
    "testRMSEMyRatings = computeError(testRDD, predictedTestMyRatingsRDD)\n",
    "print('The model had a RMSE on the test set of {}'.format(testRMSEMyRatings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCKkITkqjkNZ"
   },
   "source": [
    "### 3.5 Predict Our Ratings\n",
    "\n",
    "So far, we have only used the `predictAll` method to compute the error of the model.  Here, use the `predictAll` to predict what ratings we would give to the movies that we did not already provide ratings for.\n",
    "\n",
    "The steps we will perform are:\n",
    "* Use the Python list `myRatedMovies` to transform the `moviesRDD` into an RDD with entries that are pairs of the form (myUserID, Movie ID) and that does not contain any movies that you have rated. This transformation will yield an RDD of the form: `[(0, 1), (0, 2), (0, 3), (0, 4)]`.\n",
    "* For the prediction step, use the input RDD, `myUnratedMoviesRDD`, with `myRatingsModel.predictAll` to predict your ratings for the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "mz8VKKP4jkNZ"
   },
   "outputs": [],
   "source": [
    "# Use the Python list myRatedMovies to transform the moviesRDD into an RDD\n",
    "# with entries that are pairs of the form (myUserID, Movie ID) and that does not\n",
    "# contain any movies that you have rated.\n",
    "myUnratedMoviesRDD = (moviesRDD\n",
    "                      .filter(lambda i: i[0] not in [m[1] for m in myRatedMovies])\n",
    "                      .map(lambda i: (myUserID, i[0]))\n",
    "                     )\n",
    "\n",
    "# Use the input RDD, myUnratedMoviesRDD, with myRatingsModel.predictAll() to predict your ratings for the movies\n",
    "predictedRatingsRDD = myRatingsModel.predictAll(myUnratedMoviesRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOQcbvSFjkNa"
   },
   "source": [
    "### 3.6 Predict Our Ratings\n",
    "\n",
    "We have our predicted ratings. Now we can print out the 25 movies with the highest predicted ratings.\n",
    "\n",
    "The steps we perform are:\n",
    "* From Parts 1.2 and 1.3, we know that we should look at movies with a reasonable number of reviews (e.g., more than 75 reviews). We can experiment with a lower threshold, but fewer ratings for a movie may yield higher prediction errors. Transform `movieIDsWithAvgRatingsRDD` from Part 1.2, which has the form (MovieID, (number of ratings, average rating)), into an RDD of the form (MovieID, number of ratings): `[(2, 332), (4, 71), (6, 442)]`\n",
    "* We want to see movie names, instead of movie IDs. Transform `predictedRatingsRDD` into an RDD with entries that are pairs of the form (Movie ID, Predicted Rating): `[(3456, -0.5501005376936687), (1080, 1.5885892024487962), (320, -3.7952255522487865)]`\n",
    "* Use RDD transformations with `predictedRDD` and `movieCountsRDD` to yield an RDD with tuples of the form (Movie ID, (Predicted Rating, number of ratings)): `[(2050, (0.6694097486155939, 44)), (10, (5.29762541533513, 418)), (2060, (0.5055259373841172, 97))]`\n",
    "* Use RDD transformations with `predictedWithCountsRDD` and `moviesRDD` to yield an RDD with tuples of the form (Predicted Rating, Movie Name, number of ratings), _for movies with more than 75 ratings._ For example: `[(7.983121900375243, u'Under Siege (1992)'), (7.9769201864261285, u'Fifth Element, The (1997)')]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R25LCD5ejkNa",
    "outputId": "9ea9602e-7355-4144-a6d0-e9ace881ee96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122, (2412, 2.861318407960199))"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieIDsWithAvgRatingsRDD.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w4JHeohyjkNb",
    "outputId": "4aa9fefd-9498-4462-9a36-a38a05f5a678"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My highest rated movies as predicted (for movies with more than 75 reviews):\n",
      "(63131, (3.2590244625136977, 88))\n",
      "(63113, (3.4312300021011986, 389))\n",
      "(63082, (3.4157228376449336, 108))\n",
      "(62956, (3.683582291082152, 141))\n",
      "(62434, (2.302120453469999, 158))\n",
      "(62394, (2.744556506833817, 106))\n",
      "(62374, (3.2972070381575227, 109))\n",
      "(62081, (3.547863059309362, 136))\n",
      "(61729, (3.676086898344186, 94))\n",
      "(61352, (3.1897657165034095, 86))\n",
      "(61350, (3.233978342226265, 83))\n",
      "(61323, (2.864362333153166, 572))\n",
      "(61248, (2.7018498082292535, 96))\n",
      "(61240, (3.595194271399606, 95))\n",
      "(61160, (3.606771369694213, 127))\n",
      "(61132, (3.2745751965751855, 588))\n",
      "(61024, (2.9699920388153656, 264))\n",
      "(60950, (3.1627092013856353, 161))\n",
      "(60937, (2.9323536774730625, 141))\n",
      "(60766, (3.6118529723213046, 80))\n"
     ]
    }
   ],
   "source": [
    "# Transform movieIDsWithAvgRatingsRDD from part (1b),\n",
    "# which has the form (MovieID, (number of ratings, average rating)),\n",
    "# into and RDD of the form (MovieID, number of ratings)\n",
    "movieCountsRDD = movieIDsWithAvgRatingsRDD.map(lambda m: (m[0], m[1][0]))\n",
    "\n",
    "# Transform predictedRatingsRDD into an RDD with entries that are pairs of the form\n",
    "# (Movie ID, Predicted Rating)\n",
    "predictedRDD = predictedRatingsRDD.map(lambda r: (r.product, r.rating))\n",
    "\n",
    "# Use RDD transformations with predictedRDD and movieCountsRDD to yield an RDD\n",
    "# with tuples of the form (Movie ID, (Predicted Rating, number of ratings))\n",
    "predictedWithCountsRDD  = (predictedRDD\n",
    "                           .join(movieCountsRDD))\n",
    "\n",
    "# Use RDD transformations with PredictedWithCountsRDD and moviesRDD to yield an RDD\n",
    "#with tuples of the form (Predicted Rating, Movie Name, number of ratings), for movies\n",
    "# with more than 75 ratings\n",
    "ratingsWithNamesRDD = (predictedWithCountsRDD\n",
    "                       .filter(lambda p: p[1][1] > 75))\n",
    "\n",
    "predictedHighestRatedMovies = ratingsWithNamesRDD.takeOrdered(20, key=lambda x: -x[0])\n",
    "print('My highest rated movies as predicted '\n",
    "      '(for movies with more than 75 reviews):\\n{}'.format(\n",
    "             '\\n'.join(map(str, predictedHighestRatedMovies))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVZXdXwOjkNb",
    "outputId": "ad353ce7-0709-4c79-b32f-cd525cfd87a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(60766, 'Man on Wire (2008)'),\n",
       " (60937, 'Mummy: Tomb of the Dragon Emperor, The (2008)'),\n",
       " (60950, 'Vicky Cristina Barcelona (2008)'),\n",
       " (61024, 'Pineapple Express (2008)'),\n",
       " (61132, 'Tropic Thunder (2008)'),\n",
       " (61160, 'Star Wars: The Clone Wars (2008)'),\n",
       " (61240, 'Let the Right One In (Låt den rätte komma in) (2008)'),\n",
       " (61248, 'Death Race (2008)'),\n",
       " (61323, 'Burn After Reading (2008)'),\n",
       " (61350, 'Babylon A.D. (2008)'),\n",
       " (61352, 'Traitor (2008)'),\n",
       " (61729, 'Ghost Town (2008)'),\n",
       " (62081, 'Eagle Eye (2008)'),\n",
       " (62374, 'Body of Lies (2008)'),\n",
       " (62394, 'Max Payne (2008)'),\n",
       " (62434, 'Zack and Miri Make a Porno (2008)'),\n",
       " (62956, \"Futurama: Bender's Game (2008)\"),\n",
       " (63082, 'Slumdog Millionaire (2008)'),\n",
       " (63113, 'Quantum of Solace (2008)'),\n",
       " (63131, 'Role Models (2008)')]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviesRDD.filter(lambda m: m[0] in [p[0] for p in predictedHighestRatedMovies]).collect()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "movie-recommendation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "name": "movie-recommendation-assignment",
  "notebookId": 973717700056962,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
